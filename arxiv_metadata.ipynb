{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f465b80fb554a83a4df83f4c585ae9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d9b92feb6f74ab4aa29b22315b99ffe",
              "IPY_MODEL_c82ee82757c142d0a583f2dd7038d06b",
              "IPY_MODEL_a4f6e64ac10d4684aed725f138f5c2d9"
            ],
            "layout": "IPY_MODEL_d11ed0912cc045679ec986552e1f1b56"
          }
        },
        "3d9b92feb6f74ab4aa29b22315b99ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c829c536dd6d4090b0f903e9140f8937",
            "placeholder": "​",
            "style": "IPY_MODEL_cd7fa0a9b18845c39d2711cf88c70c90",
            "value": "Downloading (…)32fc1/.gitattributes: 100%"
          }
        },
        "c82ee82757c142d0a583f2dd7038d06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857ec842134349d18141045a499e79b9",
            "max": 437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0569b0d7e17e4ce397fb5305894818ba",
            "value": 437
          }
        },
        "a4f6e64ac10d4684aed725f138f5c2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_307a23184c394c62bcf56be6699b4e1d",
            "placeholder": "​",
            "style": "IPY_MODEL_51203d46ac5d48e08f018ade2f6de86e",
            "value": " 437/437 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "d11ed0912cc045679ec986552e1f1b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c829c536dd6d4090b0f903e9140f8937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7fa0a9b18845c39d2711cf88c70c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "857ec842134349d18141045a499e79b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0569b0d7e17e4ce397fb5305894818ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "307a23184c394c62bcf56be6699b4e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51203d46ac5d48e08f018ade2f6de86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "250b5ac5ded74401b2343d0d716f37f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63e1f147a1f54240bd02bbd9ec0bb0f1",
              "IPY_MODEL_3ff6a89e91ae406abd6ef2e494fef5ee",
              "IPY_MODEL_5dcb5603855c4859a298f0d41377021b"
            ],
            "layout": "IPY_MODEL_97ba2e7f21bc4c299adf461e767c5f6a"
          }
        },
        "63e1f147a1f54240bd02bbd9ec0bb0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c98bdf79d54647b9b71b3cd2117783",
            "placeholder": "​",
            "style": "IPY_MODEL_b022b39d7d744c7eb304b4909b75c06f",
            "value": "Downloading (…)1e87732fc1/README.md: 100%"
          }
        },
        "3ff6a89e91ae406abd6ef2e494fef5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b07336c0e5e4ea59efb131e2a60c600",
            "max": 1135,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1101f621047548829e5d32e05a006b3a",
            "value": 1135
          }
        },
        "5dcb5603855c4859a298f0d41377021b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0cfe54aa3a149b4b6d12a3bf93bf21a",
            "placeholder": "​",
            "style": "IPY_MODEL_618e43fd0b844ab2b4c8a8a2e937fcca",
            "value": " 1.14k/1.14k [00:00&lt;00:00, 67.5kB/s]"
          }
        },
        "97ba2e7f21bc4c299adf461e767c5f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c98bdf79d54647b9b71b3cd2117783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b022b39d7d744c7eb304b4909b75c06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b07336c0e5e4ea59efb131e2a60c600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1101f621047548829e5d32e05a006b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0cfe54aa3a149b4b6d12a3bf93bf21a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618e43fd0b844ab2b4c8a8a2e937fcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7d8ec63fa044255bf53975835504ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dd656113fa84aaca612e05904db9d8f",
              "IPY_MODEL_da75b4d7ee974a568e0516ccf59f86a9",
              "IPY_MODEL_fb79ee8421a84e1082f690855f4897d5"
            ],
            "layout": "IPY_MODEL_a5b99520ffa84852a740867152f029d6"
          }
        },
        "4dd656113fa84aaca612e05904db9d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_625f2a8f6a0645d38cbddbc13f3141b4",
            "placeholder": "​",
            "style": "IPY_MODEL_75ba99716fbb4a9199cab93f11297d5c",
            "value": "Downloading (…)87732fc1/config.json: 100%"
          }
        },
        "da75b4d7ee974a568e0516ccf59f86a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f7494d3b25463e97778043ccc99ad1",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd911a72709149fe9c6b7cf74e4dc951",
            "value": 385
          }
        },
        "fb79ee8421a84e1082f690855f4897d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7df694f13274788824011df641f2782",
            "placeholder": "​",
            "style": "IPY_MODEL_35894ef1d0544353bee27b4e931eaa1b",
            "value": " 385/385 [00:00&lt;00:00, 25.7kB/s]"
          }
        },
        "a5b99520ffa84852a740867152f029d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "625f2a8f6a0645d38cbddbc13f3141b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ba99716fbb4a9199cab93f11297d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53f7494d3b25463e97778043ccc99ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd911a72709149fe9c6b7cf74e4dc951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7df694f13274788824011df641f2782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35894ef1d0544353bee27b4e931eaa1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c08d1477c1437e984064078e39db62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c39411941f514a67b36d02b7aa60046f",
              "IPY_MODEL_5b71497d9dc74d86a26a058477d5e71f",
              "IPY_MODEL_8d5b004363ce4516b9b963deef447c74"
            ],
            "layout": "IPY_MODEL_6c429b943aff434c9d309cf422a45d8a"
          }
        },
        "c39411941f514a67b36d02b7aa60046f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8330ccdd5ecf48c6ace97c4728fbe5a6",
            "placeholder": "​",
            "style": "IPY_MODEL_cc043c7dbac54d61b1e1b21d7619b797",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "5b71497d9dc74d86a26a058477d5e71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_763496ee8f5843658c2b3a7ba12fdbcb",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f0d5eb8bb20468da2a1c00e192b810e",
            "value": 442221694
          }
        },
        "8d5b004363ce4516b9b963deef447c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337fa46ddd634346a4348f399a52fb63",
            "placeholder": "​",
            "style": "IPY_MODEL_6926997936724f70aae96618dfa2e245",
            "value": " 442M/442M [00:01&lt;00:00, 333MB/s]"
          }
        },
        "6c429b943aff434c9d309cf422a45d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8330ccdd5ecf48c6ace97c4728fbe5a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc043c7dbac54d61b1e1b21d7619b797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "763496ee8f5843658c2b3a7ba12fdbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0d5eb8bb20468da2a1c00e192b810e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "337fa46ddd634346a4348f399a52fb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6926997936724f70aae96618dfa2e245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb8a21a6782d407185bb57ce543a4a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e610bd6d17c544199b05b665c0880328",
              "IPY_MODEL_7c4c10f842934b6cb2119ab115f9ce06",
              "IPY_MODEL_16a37082e1124bdf876a2a87d457ffaa"
            ],
            "layout": "IPY_MODEL_81a0d857f75a4295ae0c3c25130d402b"
          }
        },
        "e610bd6d17c544199b05b665c0880328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fcce2321589424aa2e7c05b5b4dd29a",
            "placeholder": "​",
            "style": "IPY_MODEL_e5e705ea29f74e68af6f354a6825f916",
            "value": "Downloading (…)1e87732fc1/vocab.txt: 100%"
          }
        },
        "7c4c10f842934b6cb2119ab115f9ce06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4bf00b55c74a96bb0b0446b1dbe7ef",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a396e48d30bf479292530506da21f000",
            "value": 227845
          }
        },
        "16a37082e1124bdf876a2a87d457ffaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e04e7349328461680cf23cd414ee7b3",
            "placeholder": "​",
            "style": "IPY_MODEL_8f248681e2d54d3ea5f0fc8e5d861dbf",
            "value": " 228k/228k [00:00&lt;00:00, 8.33MB/s]"
          }
        },
        "81a0d857f75a4295ae0c3c25130d402b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fcce2321589424aa2e7c05b5b4dd29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e705ea29f74e68af6f354a6825f916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be4bf00b55c74a96bb0b0446b1dbe7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a396e48d30bf479292530506da21f000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e04e7349328461680cf23cd414ee7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f248681e2d54d3ea5f0fc8e5d861dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5224baff5c64b6d8486c2f366591b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_551e5e5743444b43a36001733692fa1f",
              "IPY_MODEL_3cf140312f86430bb947a954462e7a15",
              "IPY_MODEL_6c66c0e80b674d4585c1545b9c69a218"
            ],
            "layout": "IPY_MODEL_8669246479ad4781a4b412077d045947"
          }
        },
        "551e5e5743444b43a36001733692fa1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfe1d8792d28405a9e025be81632ea11",
            "placeholder": "​",
            "style": "IPY_MODEL_47aaf3f82df4402d9e72d5c2bcf4746d",
            "value": "Downloading (…)7f4ef/.gitattributes: 100%"
          }
        },
        "3cf140312f86430bb947a954462e7a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4388846c5184bc483d64577437fd0d0",
            "max": 391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5602d8f1afc04f2280747e2f15dc0ca4",
            "value": 391
          }
        },
        "6c66c0e80b674d4585c1545b9c69a218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2dfd319c34e4dc49820edd01780aacc",
            "placeholder": "​",
            "style": "IPY_MODEL_6d00abdb875349d4839602bce75bfd84",
            "value": " 391/391 [00:00&lt;00:00, 21.7kB/s]"
          }
        },
        "8669246479ad4781a4b412077d045947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe1d8792d28405a9e025be81632ea11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47aaf3f82df4402d9e72d5c2bcf4746d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4388846c5184bc483d64577437fd0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5602d8f1afc04f2280747e2f15dc0ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2dfd319c34e4dc49820edd01780aacc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d00abdb875349d4839602bce75bfd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "293b62c75f7842549337d61858ff17cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98a26aaed8854f7eb105472c63e5bdee",
              "IPY_MODEL_70e0a735161b446f9d6477bcdcbe3669",
              "IPY_MODEL_187ec67fef4742ef88e1b341d7b5591b"
            ],
            "layout": "IPY_MODEL_ceeca64cdcd648b58076420564515c0f"
          }
        },
        "98a26aaed8854f7eb105472c63e5bdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c37c0a7095b4e77975a51d48cc2f298",
            "placeholder": "​",
            "style": "IPY_MODEL_12c5d0d95f6947b199ca4977d97f589f",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "70e0a735161b446f9d6477bcdcbe3669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9a1c4d9a594c26831bc62d6a5ae13a",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1ebed539ff84c6e9363d926ad912265",
            "value": 190
          }
        },
        "187ec67fef4742ef88e1b341d7b5591b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c051756264314cf0afc7e1e3540af69c",
            "placeholder": "​",
            "style": "IPY_MODEL_677eff99223a4bd4bc2475c6440c5fe9",
            "value": " 190/190 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "ceeca64cdcd648b58076420564515c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c37c0a7095b4e77975a51d48cc2f298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c5d0d95f6947b199ca4977d97f589f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f9a1c4d9a594c26831bc62d6a5ae13a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1ebed539ff84c6e9363d926ad912265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c051756264314cf0afc7e1e3540af69c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677eff99223a4bd4bc2475c6440c5fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16f2d81089e14f70b4c1ddc7dfe21c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848777ccc00241dbbf12fcca27eac285",
              "IPY_MODEL_beebf58715654153b5c83da0611f63e5",
              "IPY_MODEL_363cb14554bc4ebd8c13d70ffb5fab7a"
            ],
            "layout": "IPY_MODEL_9e5836b9e51a44f38f6f00ad060017ed"
          }
        },
        "848777ccc00241dbbf12fcca27eac285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c46047229cec4368bbc11864cf46a1ae",
            "placeholder": "​",
            "style": "IPY_MODEL_bc5c71a8f0c945e0a258bff965ca85cd",
            "value": "Downloading (…)f279f7f4ef/README.md: 100%"
          }
        },
        "beebf58715654153b5c83da0611f63e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64d1e47d456a40bcb727638a4a1bffc5",
            "max": 3741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c760cbad2a746bb932224dce9535645",
            "value": 3741
          }
        },
        "363cb14554bc4ebd8c13d70ffb5fab7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8022f4539cf84973ab0a5cc84280d306",
            "placeholder": "​",
            "style": "IPY_MODEL_ab8fa79f98d44651abed3494357fa7be",
            "value": " 3.74k/3.74k [00:00&lt;00:00, 280kB/s]"
          }
        },
        "9e5836b9e51a44f38f6f00ad060017ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46047229cec4368bbc11864cf46a1ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5c71a8f0c945e0a258bff965ca85cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64d1e47d456a40bcb727638a4a1bffc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c760cbad2a746bb932224dce9535645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8022f4539cf84973ab0a5cc84280d306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8fa79f98d44651abed3494357fa7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1ce47e1e08b4d2aa1a87a841939ba67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60614ac78d43485f88467dcadff7af2a",
              "IPY_MODEL_e45b6a7d08ef4e30be74c6496c778911",
              "IPY_MODEL_b153e7eb707f444f9aad79e1f8dc8e84"
            ],
            "layout": "IPY_MODEL_d9862dda7cbc4af0b9b01cfb5c5380df"
          }
        },
        "60614ac78d43485f88467dcadff7af2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563655b7b0144bca9725d7fb4ced1db4",
            "placeholder": "​",
            "style": "IPY_MODEL_8c1f7bbd187f4b12914fa774cdcf5fca",
            "value": "Downloading (…)79f7f4ef/config.json: 100%"
          }
        },
        "e45b6a7d08ef4e30be74c6496c778911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a744e462d0f485ea7b8c86ff756b865",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43dc784268964e3fa7453ada29cbc036",
            "value": 718
          }
        },
        "b153e7eb707f444f9aad79e1f8dc8e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084159886aee460f82e8efd389b05dc7",
            "placeholder": "​",
            "style": "IPY_MODEL_6749bec538334ae39df13a8b18e386a7",
            "value": " 718/718 [00:00&lt;00:00, 52.1kB/s]"
          }
        },
        "d9862dda7cbc4af0b9b01cfb5c5380df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563655b7b0144bca9725d7fb4ced1db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c1f7bbd187f4b12914fa774cdcf5fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a744e462d0f485ea7b8c86ff756b865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43dc784268964e3fa7453ada29cbc036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "084159886aee460f82e8efd389b05dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6749bec538334ae39df13a8b18e386a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cea800f22eb4d60a90b7df826fa3224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9932a15e29104c6ca1b5cbdc363a6e00",
              "IPY_MODEL_c43ce10a24bc42c0ad10e1d1a1cf7c28",
              "IPY_MODEL_6ca5a35b0bd144ffb87a7d018765a11a"
            ],
            "layout": "IPY_MODEL_ab09126edd534354a9622ecf10961e54"
          }
        },
        "9932a15e29104c6ca1b5cbdc363a6e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01ee437addb442d9e929a61865beaab",
            "placeholder": "​",
            "style": "IPY_MODEL_7d0d531338c9407f9f4e826ad8766899",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "c43ce10a24bc42c0ad10e1d1a1cf7c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca0e46653b59486b87c07a9b8975e417",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a0c6518a73149bd817f366d79c02d1e",
            "value": 122
          }
        },
        "6ca5a35b0bd144ffb87a7d018765a11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2afb6cf828e54f208e3c7810feac1848",
            "placeholder": "​",
            "style": "IPY_MODEL_0f94c38596d94a94aba3efe2ccee5f7f",
            "value": " 122/122 [00:00&lt;00:00, 10.1kB/s]"
          }
        },
        "ab09126edd534354a9622ecf10961e54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01ee437addb442d9e929a61865beaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0d531338c9407f9f4e826ad8766899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca0e46653b59486b87c07a9b8975e417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a0c6518a73149bd817f366d79c02d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2afb6cf828e54f208e3c7810feac1848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f94c38596d94a94aba3efe2ccee5f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "098cee77969d48738c48d86367efc733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cb0a6ceb5c1464d8d5448b974beb949",
              "IPY_MODEL_4becba18abb54403931808f73830ff4c",
              "IPY_MODEL_e77d6c4835d54a32ab87ed0c7e89a2f3"
            ],
            "layout": "IPY_MODEL_74aa8db2b0504d56b432e56bcdb7106b"
          }
        },
        "1cb0a6ceb5c1464d8d5448b974beb949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_342ec8e9ecab4ec39b8bfc74e6988dc4",
            "placeholder": "​",
            "style": "IPY_MODEL_6e1572168acb47b6a7c1862d3928c010",
            "value": "Downloading (…)279f7f4ef/merges.txt: 100%"
          }
        },
        "4becba18abb54403931808f73830ff4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7099650e1c4cc18926a4fb9b059d8b",
            "max": 456356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a923183535124b989cac6b9480f124ff",
            "value": 456356
          }
        },
        "e77d6c4835d54a32ab87ed0c7e89a2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9113d1b9438442a2a8d92093b26be9d1",
            "placeholder": "​",
            "style": "IPY_MODEL_c312285d209f4e74b36f203fb0443074",
            "value": " 456k/456k [00:00&lt;00:00, 1.95MB/s]"
          }
        },
        "74aa8db2b0504d56b432e56bcdb7106b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "342ec8e9ecab4ec39b8bfc74e6988dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1572168acb47b6a7c1862d3928c010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d7099650e1c4cc18926a4fb9b059d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a923183535124b989cac6b9480f124ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9113d1b9438442a2a8d92093b26be9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c312285d209f4e74b36f203fb0443074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff8db35691c04f468a4cff86458ddec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_186e389effcd4022981c0c839115faa8",
              "IPY_MODEL_228411d1e6aa4d668561f0016046f03f",
              "IPY_MODEL_b5608715424143f6a87929562b06e83b"
            ],
            "layout": "IPY_MODEL_b35db1cde7c34992bded3c3c98abbe58"
          }
        },
        "186e389effcd4022981c0c839115faa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3456a40940b346909ec869b3dd830977",
            "placeholder": "​",
            "style": "IPY_MODEL_0e4e197e8ef9494a966c66b100d0dd72",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "228411d1e6aa4d668561f0016046f03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0ddba370c3f4578a5895409d4783a52",
            "max": 328515953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42b4751307ae49cc851bb095fdf56bd7",
            "value": 328515953
          }
        },
        "b5608715424143f6a87929562b06e83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80af9a0e9a741e0aee5e69e67b4276b",
            "placeholder": "​",
            "style": "IPY_MODEL_425a18aa11174c5ca06676d5341667c2",
            "value": " 329M/329M [00:21&lt;00:00, 15.7MB/s]"
          }
        },
        "b35db1cde7c34992bded3c3c98abbe58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3456a40940b346909ec869b3dd830977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4e197e8ef9494a966c66b100d0dd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0ddba370c3f4578a5895409d4783a52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42b4751307ae49cc851bb095fdf56bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e80af9a0e9a741e0aee5e69e67b4276b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425a18aa11174c5ca06676d5341667c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "091194a01c034214800d26f20e34ac4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24ae5cb8d54a4a8da6806d6f3cd6ccfb",
              "IPY_MODEL_ec19b6703af04e4296129144ba01f127",
              "IPY_MODEL_6eeda853f11a45cdbe51bbc62dc62ca9"
            ],
            "layout": "IPY_MODEL_850001a55044493c93bff0a630be512a"
          }
        },
        "24ae5cb8d54a4a8da6806d6f3cd6ccfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4620d0656cf3430caec5fe4e0a05458f",
            "placeholder": "​",
            "style": "IPY_MODEL_07c91e5eddce43a58436a983299bd0ab",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "ec19b6703af04e4296129144ba01f127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce0f44c4ec314f65939ec5e8a7e9d5cd",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96f41520600b44fea94ca032e63eb621",
            "value": 53
          }
        },
        "6eeda853f11a45cdbe51bbc62dc62ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_638ae8a7e3104e248e1cf5651ea954cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c496b5abc5c947edb90b56cacadbda1a",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.20kB/s]"
          }
        },
        "850001a55044493c93bff0a630be512a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4620d0656cf3430caec5fe4e0a05458f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c91e5eddce43a58436a983299bd0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce0f44c4ec314f65939ec5e8a7e9d5cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f41520600b44fea94ca032e63eb621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "638ae8a7e3104e248e1cf5651ea954cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c496b5abc5c947edb90b56cacadbda1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "908a22b48e7c49a9a672d37c51cc3c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e9d688637ce4825bcfb8ac64cad4c02",
              "IPY_MODEL_ec83ceaf418d4517bac247250db7174f",
              "IPY_MODEL_fa5acd001f904f02b511cbc3cb1ef90b"
            ],
            "layout": "IPY_MODEL_c60fbf566c364adc91e23f9e8c7ac161"
          }
        },
        "0e9d688637ce4825bcfb8ac64cad4c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d82ff42d10134b67a16d2092c2f4f7b6",
            "placeholder": "​",
            "style": "IPY_MODEL_9e8713df8bb047d0817a01dcba4ae691",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "ec83ceaf418d4517bac247250db7174f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba060b8697644c308ef9fcceb5e4bdef",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2a4f08d9cb48cb96f115a5e08d7ed8",
            "value": 239
          }
        },
        "fa5acd001f904f02b511cbc3cb1ef90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086a65b99d2349eaa8f5c88e474a56c5",
            "placeholder": "​",
            "style": "IPY_MODEL_b57890d7688e4d139263a23c94d10877",
            "value": " 239/239 [00:00&lt;00:00, 16.6kB/s]"
          }
        },
        "c60fbf566c364adc91e23f9e8c7ac161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d82ff42d10134b67a16d2092c2f4f7b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e8713df8bb047d0817a01dcba4ae691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba060b8697644c308ef9fcceb5e4bdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2a4f08d9cb48cb96f115a5e08d7ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "086a65b99d2349eaa8f5c88e474a56c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b57890d7688e4d139263a23c94d10877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17afcfcb13394d8684c7b3ddb076aad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dea7d8dfc29c4ea39bd4ea975f70ba94",
              "IPY_MODEL_994ec02c0fc24d9487dd2999460f5261",
              "IPY_MODEL_36abbe2b72734e37a3a5e1ab0ae972cc"
            ],
            "layout": "IPY_MODEL_025e46d4da054053b2050a846ba2adc6"
          }
        },
        "dea7d8dfc29c4ea39bd4ea975f70ba94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0521f6c4ad54a1e983a3558758a0d18",
            "placeholder": "​",
            "style": "IPY_MODEL_496193f6454f435c8ea9198df3f8dcc4",
            "value": "Downloading (…)7f4ef/tokenizer.json: 100%"
          }
        },
        "994ec02c0fc24d9487dd2999460f5261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7a7678aef049c29289dfa38e4c3618",
            "max": 1355881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a8b944b7a324a538d1dfae9856c8bc4",
            "value": 1355881
          }
        },
        "36abbe2b72734e37a3a5e1ab0ae972cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_079166729c2f4cff907fb2cb877e478e",
            "placeholder": "​",
            "style": "IPY_MODEL_216851feeddb48fea60eee7223634d5e",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 32.3MB/s]"
          }
        },
        "025e46d4da054053b2050a846ba2adc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0521f6c4ad54a1e983a3558758a0d18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496193f6454f435c8ea9198df3f8dcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe7a7678aef049c29289dfa38e4c3618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8b944b7a324a538d1dfae9856c8bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "079166729c2f4cff907fb2cb877e478e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216851feeddb48fea60eee7223634d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d832d5c79acd49da8839fd3fa58cb932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecf52e83a4bc46a686de2eda42bf1656",
              "IPY_MODEL_11f4529b0ad746ad8aff35b1e37deca3",
              "IPY_MODEL_9127eb09015d4ba0a3cf9a5782f47f22"
            ],
            "layout": "IPY_MODEL_7e05d066983c4c34aea402a9873b6752"
          }
        },
        "ecf52e83a4bc46a686de2eda42bf1656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b42b97611687457592c526e1b8842264",
            "placeholder": "​",
            "style": "IPY_MODEL_6467e6ff057e437bac49d528c0fa42e0",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "11f4529b0ad746ad8aff35b1e37deca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2513d900518647659b339ef82f4ef34f",
            "max": 1352,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98d6ac92e834436180ffff1ca80d4b75",
            "value": 1352
          }
        },
        "9127eb09015d4ba0a3cf9a5782f47f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_317cd97a06564fbfbb151a1f6bd5dda8",
            "placeholder": "​",
            "style": "IPY_MODEL_291ffd9b54f34fcbbd01cc948c01630d",
            "value": " 1.35k/1.35k [00:00&lt;00:00, 71.1kB/s]"
          }
        },
        "7e05d066983c4c34aea402a9873b6752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42b97611687457592c526e1b8842264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6467e6ff057e437bac49d528c0fa42e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2513d900518647659b339ef82f4ef34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d6ac92e834436180ffff1ca80d4b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "317cd97a06564fbfbb151a1f6bd5dda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291ffd9b54f34fcbbd01cc948c01630d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e360a431d85b44b589e32c20d766e267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc991084ded94e838ca09e253e690f85",
              "IPY_MODEL_f13859412a5e458ab00e041a4e3c06bc",
              "IPY_MODEL_5ca59532fa7a4543ada8956f0547c97a"
            ],
            "layout": "IPY_MODEL_ceadabb0d11e457b981b6af391455d27"
          }
        },
        "bc991084ded94e838ca09e253e690f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ab79f676044a62a086311adc668516",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f69597749b41f59b821669cd5348d8",
            "value": "Downloading (…)279f7f4ef/vocab.json: 100%"
          }
        },
        "f13859412a5e458ab00e041a4e3c06bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80444d5488549749fae0671cf677efe",
            "max": 798293,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa30f34e18564370a7ded94a6c2dd79c",
            "value": 798293
          }
        },
        "5ca59532fa7a4543ada8956f0547c97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d78f5a40a9124721952e4e2171af67fc",
            "placeholder": "​",
            "style": "IPY_MODEL_05d3e1730d374033bfc2d3dbd2321d47",
            "value": " 798k/798k [00:00&lt;00:00, 27.6MB/s]"
          }
        },
        "ceadabb0d11e457b981b6af391455d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ab79f676044a62a086311adc668516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f69597749b41f59b821669cd5348d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e80444d5488549749fae0671cf677efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa30f34e18564370a7ded94a6c2dd79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d78f5a40a9124721952e4e2171af67fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d3e1730d374033bfc2d3dbd2321d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f5ade4ab474e479e8ccb8671e2ad1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d658db7c938443692337fa3942636e8",
              "IPY_MODEL_06f0a56e99e74b72b2e5c607eeefd9ca",
              "IPY_MODEL_b4eb1f631df347f99827221f5e083dc9"
            ],
            "layout": "IPY_MODEL_16327d47e0f847a589fb2ab321c3e7e3"
          }
        },
        "5d658db7c938443692337fa3942636e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea07cc8dc95049f295ad7f15c0f1db97",
            "placeholder": "​",
            "style": "IPY_MODEL_36ab39bbd58e416eba90b2962b3cb542",
            "value": "Downloading (…)9f7f4ef/modules.json: 100%"
          }
        },
        "06f0a56e99e74b72b2e5c607eeefd9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf9642ea33d349d7b38d2fa4787de17b",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ddab269a311491ca65eebad35e8f6d9",
            "value": 229
          }
        },
        "b4eb1f631df347f99827221f5e083dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9539e2ec63543c396667c433fa94323",
            "placeholder": "​",
            "style": "IPY_MODEL_1778732fb99f4f96a879021e8f38ee57",
            "value": " 229/229 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "16327d47e0f847a589fb2ab321c3e7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea07cc8dc95049f295ad7f15c0f1db97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ab39bbd58e416eba90b2962b3cb542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf9642ea33d349d7b38d2fa4787de17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ddab269a311491ca65eebad35e8f6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9539e2ec63543c396667c433fa94323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1778732fb99f4f96a879021e8f38ee57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade together\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4buMekESnFc",
        "outputId": "546ac52a-f4fc-4c4b-88bd-39e5a4e32fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting together\n",
            "  Downloading together-0.1.5-py3-none-any.whl (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from together) (2.31.0)\n",
            "Collecting sseclient-py>=1.7.0 (from together)\n",
            "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from together) (4.66.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from together) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->together) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->together) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->together) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->together) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer->together) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from typer->together) (4.7.1)\n",
            "Installing collected packages: sseclient-py, together\n",
            "Successfully installed sseclient-py-1.7.2 together-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "together.api_key = \"6d1aaac776a31d6e68ad91544b77398ced09ea0aa0d585dc42030fff1084a208\""
      ],
      "metadata": {
        "id": "524Fib-cSnR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see available models\n",
        "model_list = together.Models.list()\n",
        "\n",
        "print(f\"{len(model_list)} models available\")\n",
        "\n",
        "# print the first 10 models on the menu\n",
        "model_names = [model_dict['name'] for model_dict in model_list]\n",
        "model_names[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxNoEB5gSncR",
        "outputId": "754b57f7-df92-4b1d-98ec-530337f631d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64 models available\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EleutherAI/gpt-j-6b',\n",
              " 'EleutherAI/gpt-neox-20b',\n",
              " 'EleutherAI/pythia-12b-v0',\n",
              " 'EleutherAI/pythia-1b-v0',\n",
              " 'EleutherAI/pythia-2.8b-v0',\n",
              " 'EleutherAI/pythia-6.9b',\n",
              " 'HuggingFaceH4/starchat-alpha',\n",
              " 'NousResearch/Nous-Hermes-13b',\n",
              " 'NousResearch/Nous-Hermes-Llama2-13b',\n",
              " 'NumbersStation/nsql-6B']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSNIHVeiS-i1",
        "outputId": "d657fce2-b329-4003-e8fb-edae69c454f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.32.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=658762eb154230187e5326cb470280b39fe71e961be3ff902cec0d1f7de937fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "fz3IaHzEU4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
        "model.max_seq_length = 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "3f465b80fb554a83a4df83f4c585ae9a",
            "3d9b92feb6f74ab4aa29b22315b99ffe",
            "c82ee82757c142d0a583f2dd7038d06b",
            "a4f6e64ac10d4684aed725f138f5c2d9",
            "d11ed0912cc045679ec986552e1f1b56",
            "c829c536dd6d4090b0f903e9140f8937",
            "cd7fa0a9b18845c39d2711cf88c70c90",
            "857ec842134349d18141045a499e79b9",
            "0569b0d7e17e4ce397fb5305894818ba",
            "307a23184c394c62bcf56be6699b4e1d",
            "51203d46ac5d48e08f018ade2f6de86e",
            "250b5ac5ded74401b2343d0d716f37f2",
            "63e1f147a1f54240bd02bbd9ec0bb0f1",
            "3ff6a89e91ae406abd6ef2e494fef5ee",
            "5dcb5603855c4859a298f0d41377021b",
            "97ba2e7f21bc4c299adf461e767c5f6a",
            "86c98bdf79d54647b9b71b3cd2117783",
            "b022b39d7d744c7eb304b4909b75c06f",
            "4b07336c0e5e4ea59efb131e2a60c600",
            "1101f621047548829e5d32e05a006b3a",
            "e0cfe54aa3a149b4b6d12a3bf93bf21a",
            "618e43fd0b844ab2b4c8a8a2e937fcca",
            "b7d8ec63fa044255bf53975835504ea9",
            "4dd656113fa84aaca612e05904db9d8f",
            "da75b4d7ee974a568e0516ccf59f86a9",
            "fb79ee8421a84e1082f690855f4897d5",
            "a5b99520ffa84852a740867152f029d6",
            "625f2a8f6a0645d38cbddbc13f3141b4",
            "75ba99716fbb4a9199cab93f11297d5c",
            "53f7494d3b25463e97778043ccc99ad1",
            "bd911a72709149fe9c6b7cf74e4dc951",
            "c7df694f13274788824011df641f2782",
            "35894ef1d0544353bee27b4e931eaa1b",
            "c0c08d1477c1437e984064078e39db62",
            "c39411941f514a67b36d02b7aa60046f",
            "5b71497d9dc74d86a26a058477d5e71f",
            "8d5b004363ce4516b9b963deef447c74",
            "6c429b943aff434c9d309cf422a45d8a",
            "8330ccdd5ecf48c6ace97c4728fbe5a6",
            "cc043c7dbac54d61b1e1b21d7619b797",
            "763496ee8f5843658c2b3a7ba12fdbcb",
            "5f0d5eb8bb20468da2a1c00e192b810e",
            "337fa46ddd634346a4348f399a52fb63",
            "6926997936724f70aae96618dfa2e245",
            "fb8a21a6782d407185bb57ce543a4a0d",
            "e610bd6d17c544199b05b665c0880328",
            "7c4c10f842934b6cb2119ab115f9ce06",
            "16a37082e1124bdf876a2a87d457ffaa",
            "81a0d857f75a4295ae0c3c25130d402b",
            "0fcce2321589424aa2e7c05b5b4dd29a",
            "e5e705ea29f74e68af6f354a6825f916",
            "be4bf00b55c74a96bb0b0446b1dbe7ef",
            "a396e48d30bf479292530506da21f000",
            "6e04e7349328461680cf23cd414ee7b3",
            "8f248681e2d54d3ea5f0fc8e5d861dbf"
          ]
        },
        "id": "P26lJtCHVKaw",
        "outputId": "5497f058-c0b0-43ac-f4ed-a6f26350bfce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)32fc1/.gitattributes:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f465b80fb554a83a4df83f4c585ae9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)1e87732fc1/README.md:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "250b5ac5ded74401b2343d0d716f37f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)87732fc1/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d8ec63fa044255bf53975835504ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0c08d1477c1437e984064078e39db62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)1e87732fc1/vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb8a21a6782d407185bb57ce543a4a0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Two lists of sentences\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarits\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "a5224baff5c64b6d8486c2f366591b71",
            "551e5e5743444b43a36001733692fa1f",
            "3cf140312f86430bb947a954462e7a15",
            "6c66c0e80b674d4585c1545b9c69a218",
            "8669246479ad4781a4b412077d045947",
            "dfe1d8792d28405a9e025be81632ea11",
            "47aaf3f82df4402d9e72d5c2bcf4746d",
            "a4388846c5184bc483d64577437fd0d0",
            "5602d8f1afc04f2280747e2f15dc0ca4",
            "d2dfd319c34e4dc49820edd01780aacc",
            "6d00abdb875349d4839602bce75bfd84",
            "293b62c75f7842549337d61858ff17cc",
            "98a26aaed8854f7eb105472c63e5bdee",
            "70e0a735161b446f9d6477bcdcbe3669",
            "187ec67fef4742ef88e1b341d7b5591b",
            "ceeca64cdcd648b58076420564515c0f",
            "8c37c0a7095b4e77975a51d48cc2f298",
            "12c5d0d95f6947b199ca4977d97f589f",
            "3f9a1c4d9a594c26831bc62d6a5ae13a",
            "f1ebed539ff84c6e9363d926ad912265",
            "c051756264314cf0afc7e1e3540af69c",
            "677eff99223a4bd4bc2475c6440c5fe9",
            "16f2d81089e14f70b4c1ddc7dfe21c82",
            "848777ccc00241dbbf12fcca27eac285",
            "beebf58715654153b5c83da0611f63e5",
            "363cb14554bc4ebd8c13d70ffb5fab7a",
            "9e5836b9e51a44f38f6f00ad060017ed",
            "c46047229cec4368bbc11864cf46a1ae",
            "bc5c71a8f0c945e0a258bff965ca85cd",
            "64d1e47d456a40bcb727638a4a1bffc5",
            "1c760cbad2a746bb932224dce9535645",
            "8022f4539cf84973ab0a5cc84280d306",
            "ab8fa79f98d44651abed3494357fa7be",
            "d1ce47e1e08b4d2aa1a87a841939ba67",
            "60614ac78d43485f88467dcadff7af2a",
            "e45b6a7d08ef4e30be74c6496c778911",
            "b153e7eb707f444f9aad79e1f8dc8e84",
            "d9862dda7cbc4af0b9b01cfb5c5380df",
            "563655b7b0144bca9725d7fb4ced1db4",
            "8c1f7bbd187f4b12914fa774cdcf5fca",
            "2a744e462d0f485ea7b8c86ff756b865",
            "43dc784268964e3fa7453ada29cbc036",
            "084159886aee460f82e8efd389b05dc7",
            "6749bec538334ae39df13a8b18e386a7",
            "5cea800f22eb4d60a90b7df826fa3224",
            "9932a15e29104c6ca1b5cbdc363a6e00",
            "c43ce10a24bc42c0ad10e1d1a1cf7c28",
            "6ca5a35b0bd144ffb87a7d018765a11a",
            "ab09126edd534354a9622ecf10961e54",
            "f01ee437addb442d9e929a61865beaab",
            "7d0d531338c9407f9f4e826ad8766899",
            "ca0e46653b59486b87c07a9b8975e417",
            "4a0c6518a73149bd817f366d79c02d1e",
            "2afb6cf828e54f208e3c7810feac1848",
            "0f94c38596d94a94aba3efe2ccee5f7f",
            "098cee77969d48738c48d86367efc733",
            "1cb0a6ceb5c1464d8d5448b974beb949",
            "4becba18abb54403931808f73830ff4c",
            "e77d6c4835d54a32ab87ed0c7e89a2f3",
            "74aa8db2b0504d56b432e56bcdb7106b",
            "342ec8e9ecab4ec39b8bfc74e6988dc4",
            "6e1572168acb47b6a7c1862d3928c010",
            "2d7099650e1c4cc18926a4fb9b059d8b",
            "a923183535124b989cac6b9480f124ff",
            "9113d1b9438442a2a8d92093b26be9d1",
            "c312285d209f4e74b36f203fb0443074",
            "ff8db35691c04f468a4cff86458ddec4",
            "186e389effcd4022981c0c839115faa8",
            "228411d1e6aa4d668561f0016046f03f",
            "b5608715424143f6a87929562b06e83b",
            "b35db1cde7c34992bded3c3c98abbe58",
            "3456a40940b346909ec869b3dd830977",
            "0e4e197e8ef9494a966c66b100d0dd72",
            "e0ddba370c3f4578a5895409d4783a52",
            "42b4751307ae49cc851bb095fdf56bd7",
            "e80af9a0e9a741e0aee5e69e67b4276b",
            "425a18aa11174c5ca06676d5341667c2",
            "091194a01c034214800d26f20e34ac4e",
            "24ae5cb8d54a4a8da6806d6f3cd6ccfb",
            "ec19b6703af04e4296129144ba01f127",
            "6eeda853f11a45cdbe51bbc62dc62ca9",
            "850001a55044493c93bff0a630be512a",
            "4620d0656cf3430caec5fe4e0a05458f",
            "07c91e5eddce43a58436a983299bd0ab",
            "ce0f44c4ec314f65939ec5e8a7e9d5cd",
            "96f41520600b44fea94ca032e63eb621",
            "638ae8a7e3104e248e1cf5651ea954cc",
            "c496b5abc5c947edb90b56cacadbda1a",
            "908a22b48e7c49a9a672d37c51cc3c1c",
            "0e9d688637ce4825bcfb8ac64cad4c02",
            "ec83ceaf418d4517bac247250db7174f",
            "fa5acd001f904f02b511cbc3cb1ef90b",
            "c60fbf566c364adc91e23f9e8c7ac161",
            "d82ff42d10134b67a16d2092c2f4f7b6",
            "9e8713df8bb047d0817a01dcba4ae691",
            "ba060b8697644c308ef9fcceb5e4bdef",
            "4e2a4f08d9cb48cb96f115a5e08d7ed8",
            "086a65b99d2349eaa8f5c88e474a56c5",
            "b57890d7688e4d139263a23c94d10877",
            "17afcfcb13394d8684c7b3ddb076aad2",
            "dea7d8dfc29c4ea39bd4ea975f70ba94",
            "994ec02c0fc24d9487dd2999460f5261",
            "36abbe2b72734e37a3a5e1ab0ae972cc",
            "025e46d4da054053b2050a846ba2adc6",
            "b0521f6c4ad54a1e983a3558758a0d18",
            "496193f6454f435c8ea9198df3f8dcc4",
            "fe7a7678aef049c29289dfa38e4c3618",
            "6a8b944b7a324a538d1dfae9856c8bc4",
            "079166729c2f4cff907fb2cb877e478e",
            "216851feeddb48fea60eee7223634d5e",
            "d832d5c79acd49da8839fd3fa58cb932",
            "ecf52e83a4bc46a686de2eda42bf1656",
            "11f4529b0ad746ad8aff35b1e37deca3",
            "9127eb09015d4ba0a3cf9a5782f47f22",
            "7e05d066983c4c34aea402a9873b6752",
            "b42b97611687457592c526e1b8842264",
            "6467e6ff057e437bac49d528c0fa42e0",
            "2513d900518647659b339ef82f4ef34f",
            "98d6ac92e834436180ffff1ca80d4b75",
            "317cd97a06564fbfbb151a1f6bd5dda8",
            "291ffd9b54f34fcbbd01cc948c01630d",
            "e360a431d85b44b589e32c20d766e267",
            "bc991084ded94e838ca09e253e690f85",
            "f13859412a5e458ab00e041a4e3c06bc",
            "5ca59532fa7a4543ada8956f0547c97a",
            "ceadabb0d11e457b981b6af391455d27",
            "33ab79f676044a62a086311adc668516",
            "b5f69597749b41f59b821669cd5348d8",
            "e80444d5488549749fae0671cf677efe",
            "aa30f34e18564370a7ded94a6c2dd79c",
            "d78f5a40a9124721952e4e2171af67fc",
            "05d3e1730d374033bfc2d3dbd2321d47",
            "a1f5ade4ab474e479e8ccb8671e2ad1a",
            "5d658db7c938443692337fa3942636e8",
            "06f0a56e99e74b72b2e5c607eeefd9ca",
            "b4eb1f631df347f99827221f5e083dc9",
            "16327d47e0f847a589fb2ab321c3e7e3",
            "ea07cc8dc95049f295ad7f15c0f1db97",
            "36ab39bbd58e416eba90b2962b3cb542",
            "cf9642ea33d349d7b38d2fa4787de17b",
            "5ddab269a311491ca65eebad35e8f6d9",
            "b9539e2ec63543c396667c433fa94323",
            "1778732fb99f4f96a879021e8f38ee57"
          ]
        },
        "id": "lIPxBEdEVfpt",
        "outputId": "9ec473b4-46b7-4ac4-dba3-ad46a5767865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7f4ef/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5224baff5c64b6d8486c2f366591b71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "293b62c75f7842549337d61858ff17cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)f279f7f4ef/README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16f2d81089e14f70b4c1ddc7dfe21c82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)79f7f4ef/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1ce47e1e08b4d2aa1a87a841939ba67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cea800f22eb4d60a90b7df826fa3224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)279f7f4ef/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "098cee77969d48738c48d86367efc733"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff8db35691c04f468a4cff86458ddec4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "091194a01c034214800d26f20e34ac4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "908a22b48e7c49a9a672d37c51cc3c1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7f4ef/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17afcfcb13394d8684c7b3ddb076aad2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d832d5c79acd49da8839fd3fa58cb932"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)279f7f4ef/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e360a431d85b44b589e32c20d766e267"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)9f7f4ef/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1f5ade4ab474e479e8ccb8671e2ad1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.4579\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "YZPFGULUVjGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = r'/content/drive/MyDrive/arxiv/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "\"\"\" Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\"\"\"\n",
        "\n",
        "def get_metadata():\n",
        "    with open(data_file, 'r') as f:\n",
        "        for line in f:\n",
        "            yield line"
      ],
      "metadata": {
        "id": "S2U0pTO-V1s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = get_metadata()\n",
        "ids = []\n",
        "titles = []\n",
        "abstracts = []\n",
        "categories = []\n",
        "\n",
        "for paper in metadata:\n",
        "    metaDict = json.loads(paper)\n",
        "    try:\n",
        "        try:\n",
        "            year = int(metaDict['journal-ref'][-4:])    ### Example Format: \"Phys.Rev.D76:013009,2007\"\n",
        "        except:\n",
        "            year = int(metaDict['journal-ref'][-5:-1])    ### Example Format: \"Phys.Rev.D76:013009,(2007)\"\n",
        "        if(year == 2020):\n",
        "            ids.append(metaDict['id'])\n",
        "            titles.append(metaDict['title'])\n",
        "            abstracts.append(metaDict['abstract'])\n",
        "            categories.append(metaDict['categories'])\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "4Hpjxbd5WHe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'id' : ids,'title' : titles,'abstract' : abstracts, 'categories' : categories})\n",
        "\n",
        "\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88WJ3hGznC-K",
        "outputId": "d8734898-cd7c-460d-9e91-a9ebc342ec21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2WVju0ZLnDV1",
        "outputId": "c96655ef-648a-44de-a083-bd0252dd3a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                                              title  \\\n",
              "0  0712.1975         Reentrant spin glass transition in LuFe2O4   \n",
              "1  0804.3104  Teichm\\\"uller Structures and Dual Geometric Gi...   \n",
              "2  0810.3615  Exact results for the Wigner transform phase s...   \n",
              "3  0810.5491  Nonequilibrium phase transition in a spreading...   \n",
              "4  0902.3288  Origin and evolution of cosmic accelerators - ...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0    We have carried out a comprehensive investig...   \n",
              "1    The Gibbs measure theory for smooth potentia...   \n",
              "2    Closed form analytical expressions are obtai...   \n",
              "3    We consider a nonequilibrium process on a ti...   \n",
              "4    One of the most tantalizing questions in ast...   \n",
              "\n",
              "                          categories  \n",
              "0  cond-mat.str-el cond-mat.mtrl-sci  \n",
              "1                    math.DS math.CV  \n",
              "2                    physics.atom-ph  \n",
              "3                 cond-mat.stat-mech  \n",
              "4            astro-ph.CO astro-ph.HE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-decebdd1-6e3a-4fca-aabf-100710479550\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0712.1975</td>\n",
              "      <td>Reentrant spin glass transition in LuFe2O4</td>\n",
              "      <td>We have carried out a comprehensive investig...</td>\n",
              "      <td>cond-mat.str-el cond-mat.mtrl-sci</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0804.3104</td>\n",
              "      <td>Teichm\\\"uller Structures and Dual Geometric Gi...</td>\n",
              "      <td>The Gibbs measure theory for smooth potentia...</td>\n",
              "      <td>math.DS math.CV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0810.3615</td>\n",
              "      <td>Exact results for the Wigner transform phase s...</td>\n",
              "      <td>Closed form analytical expressions are obtai...</td>\n",
              "      <td>physics.atom-ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0810.5491</td>\n",
              "      <td>Nonequilibrium phase transition in a spreading...</td>\n",
              "      <td>We consider a nonequilibrium process on a ti...</td>\n",
              "      <td>cond-mat.stat-mech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0902.3288</td>\n",
              "      <td>Origin and evolution of cosmic accelerators - ...</td>\n",
              "      <td>One of the most tantalizing questions in ast...</td>\n",
              "      <td>astro-ph.CO astro-ph.HE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-decebdd1-6e3a-4fca-aabf-100710479550')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-decebdd1-6e3a-4fca-aabf-100710479550 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-decebdd1-6e3a-4fca-aabf-100710479550');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13b9399c-ff9e-4795-be1a-baa6b429e534\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13b9399c-ff9e-4795-be1a-baa6b429e534')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13b9399c-ff9e-4795-be1a-baa6b429e534 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_list= df['categories'].unique()\n",
        "print(*cat_list, sep = \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKPVbJknDgO",
        "outputId": "43c49b34-f86c-4c5b-ccbc-075bece20413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "quant-ph cond-mat.stat-mech hep-th math-ph math.MP\n",
            "physics.chem-ph physics.atom-ph\n",
            "astro-ph.EP\n",
            "math.FA math.OA math.SP\n",
            "physics.flu-dyn stat.AP\n",
            "cs.NI cs.CR\n",
            "physics.acc-ph\n",
            "stat.ME stat.CO\n",
            "math.OC math-ph math.AP math.MP\n",
            "stat.ML cs.CV cs.DS cs.LG math.ST stat.TH\n",
            "math-ph math.MP nlin.PS\n",
            "math-ph cond-mat.mes-hall math.MP\n",
            "math.LO math.CT\n",
            "math.NT math.AG\n",
            "math.AP gr-qc math-ph math.DG math.MP\n",
            "q-bio.BM\n",
            "cs.LG\n",
            "astro-ph.IM physics.ins-det\n",
            "cs.SY cs.CR\n",
            "math.DG math.CV\n",
            "math.GT math.NT\n",
            "q-fin.MF math.PR\n",
            "cs.DS q-bio.QM\n",
            "math.NA cs.CC cs.NA\n",
            "math-ph cond-mat.stat-mech math.MP math.PR math.ST stat.TH\n",
            "q-bio.NC physics.bio-ph\n",
            "physics.soc-ph cs.CY cs.SI\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el hep-th\n",
            "quant-ph nlin.AO nlin.CD\n",
            "math.CA math.DS\n",
            "cond-mat.stat-mech math-ph math.MP math.PR\n",
            "cond-mat.dis-nn cond-mat.stat-mech nlin.CD\n",
            "physics.hist-ph quant-ph\n",
            "gr-qc cond-mat.stat-mech hep-th math-ph math.MP\n",
            "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH\n",
            "cond-mat.str-el cond-mat.stat-mech\n",
            "math.OA math.FA\n",
            "hep-ph hep-ex nucl-ex\n",
            "nucl-th hep-lat hep-th\n",
            "astro-ph.IM\n",
            "math.NA cs.NA cs.SY eess.SY\n",
            "quant-ph cond-mat.other cs.IT math.IT\n",
            "hep-th hep-ph nucl-th\n",
            "quant-ph physics.hist-ph\n",
            "quant-ph math-ph math.FA math.MP math.OA\n",
            "math.CT math.RA\n",
            "cond-mat.supr-con physics.optics\n",
            "math-ph math.MP math.RT quant-ph\n",
            "physics.app-ph\n",
            "cs.NE\n",
            "cs.GR cs.SI\n",
            "math.ST math.PR stat.ME stat.TH\n",
            "cs.FL math.GR\n",
            "physics.ins-det cond-mat.mes-hall physics.app-ph physics.optics\n",
            "hep-th cond-mat.str-el\n",
            "nucl-th hep-ph nucl-ex\n",
            "physics.comp-ph\n",
            "math.MG\n",
            "physics.soc-ph nlin.AO\n",
            "cs.CE stat.CO stat.ME\n",
            "hep-ph hep-ex nucl-ex nucl-th\n",
            "cs.SE\n",
            "astro-ph.GA astro-ph.HE\n",
            "math.CO cs.DM\n",
            "math.AT cs.CG math.GT\n",
            "math.OC cs.NA math.NA\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el quant-ph\n",
            "stat.ML cs.AI\n",
            "math.QA hep-th math-ph math.MP\n",
            "math.SP math.CA\n",
            "astro-ph.GA astro-ph.CO astro-ph.IM\n",
            "cond-mat.quant-gas cond-mat.supr-con hep-lat nucl-th\n",
            "cs.DS cs.DM cs.NE math.PR\n",
            "cs.SY math.OC\n",
            "cs.IT cs.DC math.IT\n",
            "stat.ME math.ST stat.TH\n",
            "cs.AI\n",
            "cond-mat.str-el hep-th\n",
            "physics.flu-dyn nlin.CD physics.ao-ph\n",
            "cs.MS\n",
            "quant-ph cond-mat.str-el hep-th\n",
            "cond-mat.dis-nn nlin.AO physics.soc-ph\n",
            "stat.ME cs.LG stat.AP stat.ML\n",
            "cs.SI\n",
            "math.PR q-bio.PE\n",
            "physics.comp-ph cs.CE math.NA\n",
            "physics.hist-ph\n",
            "math.ST math.AC math.PR stat.TH\n",
            "cs.HC cs.AI cs.CY stat.ML\n",
            "astro-ph.IM astro-ph.CO hep-ex physics.ins-det\n",
            "q-fin.TR\n",
            "astro-ph.SR\n",
            "math-ph math.AT math.CT math.MP\n",
            "quant-ph cond-mat.str-el\n",
            "math.AG math.CV\n",
            "math.RT math.LO\n",
            "eess.SP\n",
            "cs.GT cs.MA cs.SY math.OC q-fin.EC\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech\n",
            "eess.IV cs.CV eess.SP\n",
            "cs.IT eess.SP math.IT\n",
            "cs.CG math.MG\n",
            "physics.bio-ph cond-mat.soft q-bio.CB\n",
            "cs.LG cs.AI cs.HC\n",
            "math.NT math.CV\n",
            "physics.chem-ph cond-mat.mtrl-sci physics.atm-clus physics.comp-ph quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall\n",
            "quant-ph hep-ex physics.atom-ph\n",
            "math.AP math.DS math.FA\n",
            "cond-mat.str-el cond-mat.other\n",
            "math-ph math.MP nlin.SI\n",
            "stat.ME astro-ph.IM physics.data-an stat.AP stat.CO\n",
            "cs.LG stat.ML\n",
            "stat.ML cond-mat.dis-nn cond-mat.stat-mech physics.data-an\n",
            "quant-ph cs.ET\n",
            "quant-ph cs.LG\n",
            "math.RT math.AC math.RA\n",
            "stat.ML cs.LG q-bio.NC\n",
            "math.SP math-ph math.MP\n",
            "stat.ME stat.AP stat.CO stat.ML\n",
            "math.CO cs.SI physics.data-an physics.soc-ph\n",
            "cond-mat.str-el quant-ph\n",
            "cs.MA cs.SY math.OC\n",
            "physics.bio-ph q-bio.CB\n",
            "physics.soc-ph cond-mat.stat-mech cs.GT econ.EM math-ph math.MP q-bio.PE\n",
            "math.DS math.AT math.DG\n",
            "math.GR cs.FL\n",
            "cs.SY cs.MA\n",
            "math.AT hep-th math-ph math.MP\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con\n",
            "quant-ph cond-mat.mes-hall physics.atom-ph physics.optics\n",
            "physics.flu-dyn physics.comp-ph\n",
            "math.ST stat.AP stat.TH\n",
            "astro-ph.HE\n",
            "math.ST q-bio.PE stat.CO stat.TH\n",
            "math.CO math.GT\n",
            "cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "cs.IR cs.SI\n",
            "cond-mat.soft cond-mat.mtrl-sci\n",
            "quant-ph cs.DS cs.LG stat.ML\n",
            "cs.GT math.CO math.PR\n",
            "math.GR math.CO math.PR\n",
            "q-bio.MN\n",
            "cs.DS cs.DM math.CO\n",
            "physics.geo-ph cs.CV\n",
            "cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "q-fin.CP math.NA\n",
            "quant-ph cond-mat.mes-hall gr-qc\n",
            "math.CO math.CA\n",
            "physics.soc-ph math.HO stat.AP\n",
            "cs.CE\n",
            "cs.CC cs.DM\n",
            "q-fin.RM cs.CE\n",
            "math.OA quant-ph\n",
            "math.PR math.LO\n",
            "math.CT math.KT\n",
            "q-bio.QM q-bio.GN q-bio.MN\n",
            "cs.GT econ.TH\n",
            "cs.LG q-bio.QM stat.ML\n",
            "gr-qc astro-ph.HE hep-ph\n",
            "stat.AP\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall\n",
            "physics.data-an cs.IT math.IT\n",
            "q-fin.ST stat.ML\n",
            "quant-ph cs.CC\n",
            "math.CO math.AC\n",
            "math.RT math.LO math.RA\n",
            "cs.CG cs.DS\n",
            "cs.NE cs.CV\n",
            "physics.hist-ph nucl-th\n",
            "quant-ph cs.IT math-ph math.FA math.IT math.MP\n",
            "astro-ph.HE nucl-th\n",
            "cs.IR cs.DB\n",
            "cs.DS cs.CC\n",
            "math.DG physics.class-ph\n",
            "cs.IR cs.AI cs.MM\n",
            "q-bio.PE math.RA math.ST stat.TH\n",
            "math.GT math.AG math.DS math.RT\n",
            "physics.bio-ph q-bio.PE\n",
            "cond-mat.stat-mech hep-th math-ph math.MP\n",
            "math.GT math.CO math.QA\n",
            "cs.DB\n",
            "math-ph hep-th math.AP math.DG math.MP\n",
            "quant-ph cond-mat.stat-mech physics.atom-ph\n",
            "quant-ph math.CO\n",
            "cond-mat.quant-gas cond-mat.mes-hall quant-ph\n",
            "math.GT math.AT math.RT\n",
            "hep-th cond-mat.str-el math-ph math.MP quant-ph\n",
            "physics.soc-ph cond-mat.stat-mech physics.app-ph\n",
            "math.GR math.CO\n",
            "math-ph math.FA math.MP\n",
            "quant-ph cond-mat.other physics.optics\n",
            "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
            "cs.CC\n",
            "quant-ph math.OA\n",
            "stat.ME stat.AP\n",
            "cs.CR cs.GT\n",
            "cs.LO cs.CC\n",
            "math.DG gr-qc\n",
            "math-ph math.MP math.PR\n",
            "physics.ins-det cond-mat.supr-con eess.IV eess.SP physics.app-ph\n",
            "math.AT math.CT\n",
            "hep-th cond-mat.stat-mech math-ph math.MP\n",
            "math.FA math.DS\n",
            "cs.SI cs.LG stat.ML\n",
            "hep-lat hep-ph\n",
            "cs.GT cs.NI\n",
            "physics.data-an hep-ex physics.comp-ph\n",
            "q-bio.OT\n",
            "hep-th quant-ph\n",
            "physics.optics physics.atom-ph quant-ph\n",
            "math.MG math.CA math.CV\n",
            "cs.NE cs.LG stat.ML\n",
            "hep-th math.DG\n",
            "cs.SI cs.IR\n",
            "cs.SD cs.AI cs.LG eess.AS\n",
            "quant-ph math.RT\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.supr-con\n",
            "math-ph math.AP math.MP\n",
            "math.AP math-ph math.MP math.PR\n",
            "math.MG math.CO\n",
            "math.PR math.CA\n",
            "physics.app-ph cond-mat.mtrl-sci physics.optics\n",
            "cond-mat.stat-mech math-ph math.MP physics.data-an\n",
            "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech physics.bio-ph\n",
            "physics.soc-ph cs.SI\n",
            "cs.CR cs.AI cs.NI\n",
            "math.RT math-ph math.MP\n",
            "astro-ph.CO astro-ph.GA\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.quant-gas\n",
            "astro-ph.CO astro-ph.GA hep-ph hep-th\n",
            "cond-mat.other cond-mat.mes-hall\n",
            "math.QA math.AG math.RA\n",
            "cond-mat.quant-gas physics.optics\n",
            "math.CO cs.DS\n",
            "gr-qc physics.flu-dyn\n",
            "math.AG math.DS\n",
            "cs.SY eess.SP\n",
            "cond-mat.str-el hep-th math-ph math.MP math.QA quant-ph\n",
            "cond-mat.str-el math-ph math.MP\n",
            "cs.SY cs.RO math.OC\n",
            "physics.med-ph physics.bio-ph physics.optics\n",
            "math.DS astro-ph.EP math-ph math.MP\n",
            "cond-mat.dis-nn cond-mat.soft\n",
            "math.AC math.NT\n",
            "q-fin.PR cs.DS math.PR\n",
            "q-fin.PM q-fin.MF\n",
            "physics.optics cond-mat.mes-hall physics.app-ph\n",
            "hep-ex physics.ins-det\n",
            "quant-ph cond-mat.mes-hall cs.CR cs.ET\n",
            "quant-ph cond-mat.dis-nn cs.AI cs.LG\n",
            "hep-lat quant-ph\n",
            "math.RA math.KT math.OA\n",
            "physics.ins-det\n",
            "physics.atm-clus\n",
            "hep-ph astro-ph.CO astro-ph.HE\n",
            "cs.DS cs.DM\n",
            "math-ph hep-th math.AG math.CO math.MP nlin.SI\n",
            "cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
            "math.CA math-ph math.MP math.SP\n",
            "quant-ph cond-mat.quant-gas\n",
            "physics.geo-ph\n",
            "cond-mat.str-el cond-mat.quant-gas\n",
            "cs.GT cond-mat.stat-mech physics.soc-ph\n",
            "cond-mat.mtrl-sci physics.comp-ph\n",
            "quant-ph cond-mat.str-el hep-lat hep-th\n",
            "econ.EM\n",
            "cond-mat.mes-hall quant-ph\n",
            "hep-th hep-ph\n",
            "hep-th cond-mat.stat-mech gr-qc\n",
            "cs.CL cs.CY\n",
            "physics.ed-ph physics.optics\n",
            "math.DS math-ph math.FA math.MP math.SP nlin.CD\n",
            "cond-mat.mtrl-sci physics.app-ph physics.chem-ph\n",
            "cond-mat.dis-nn physics.optics quant-ph\n",
            "math.CO math.AG math.AT\n",
            "math.GN\n",
            "nucl-th astro-ph.HE hep-ph\n",
            "cond-mat.str-el cond-mat.mes-hall math-ph math.MP\n",
            "nlin.AO cs.MA nlin.CD\n",
            "physics.space-ph\n",
            "physics.comp-ph math-ph math.MP math.NA\n",
            "math.AP math-ph math.MP math.SP\n",
            "math.AP math.DG math.FA\n",
            "cs.IT cs.DC cs.LG math.IT\n",
            "math.OC math.NA\n",
            "math.AT math.GR math.KT\n",
            "cs.ET cs.NE quant-ph\n",
            "cond-mat.str-el physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
            "cs.SY math-ph math.MP math.OC quant-ph\n",
            "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con physics.comp-ph\n",
            "q-fin.ST q-fin.EC\n",
            "cond-mat.supr-con quant-ph\n",
            "physics.flu-dyn astro-ph.CO gr-qc physics.comp-ph\n",
            "quant-ph cs.DS\n",
            "hep-th math-ph math.MP math.QA\n",
            "nlin.CD math.DS\n",
            "cs.CG math.AG\n",
            "hep-lat hep-ex hep-ph nucl-ex nucl-th\n",
            "physics.acc-ph physics.ins-det physics.med-ph\n",
            "stat.AP cs.AI cs.GT\n",
            "quant-ph hep-th\n",
            "cs.CE cs.NA math.NA physics.comp-ph physics.optics\n",
            "quant-ph cond-mat.quant-gas physics.atom-ph\n",
            "hep-th cond-mat.quant-gas cond-mat.stat-mech hep-ph nucl-th\n",
            "cs.CR cs.LG cs.SE\n",
            "math.DG hep-th math-ph math.AP math.CV math.MP\n",
            "quant-ph gr-qc physics.space-ph\n",
            "eess.IV cs.CE eess.SP math.OC\n",
            "math.DS math.OC nlin.CD\n",
            "hep-th cond-mat.stat-mech gr-qc hep-ph nucl-th\n",
            "hep-ph cs.LG hep-ex\n",
            "cond-mat.mes-hall hep-th\n",
            "quant-ph cs.LG physics.data-an\n",
            "math.RA math.CT math.GN\n",
            "quant-ph cs.DC cs.ET\n",
            "physics.atom-ph cond-mat.quant-gas quant-ph\n",
            "math.OC cs.LG\n",
            "math.NA math.DG\n",
            "physics.ao-ph\n",
            "econ.TH\n",
            "physics.data-an cs.CV cs.LG hep-ex\n",
            "stat.ML cs.LG econ.EM math.OC math.ST stat.TH\n",
            "math.PR cond-mat.stat-mech math-ph math.MP\n",
            "cond-mat.dis-nn cs.LG stat.ML\n",
            "quant-ph cond-mat.other cs.CC\n",
            "cs.MS cond-mat.str-el physics.comp-ph\n",
            "hep-th math.AG\n",
            "math.DS math.AT math.PR\n",
            "math-ph hep-th math.MP math.QA math.SG\n",
            "astro-ph.CO gr-qc hep-ph hep-th\n",
            "physics.ed-ph physics.pop-ph\n",
            "math.DG math.AG\n",
            "math.AP math-ph math.MP math.OC math.PR\n",
            "quant-ph cond-mat.quant-gas physics.atom-ph physics.optics\n",
            "stat.AP cs.CE\n",
            "cond-mat.str-el cond-mat.mes-hall math-ph math.MP quant-ph\n",
            "math.AP math.OC\n",
            "cs.IT math.IT math.PR\n",
            "math.AP cond-mat.mtrl-sci math-ph math.MP\n",
            "cs.IT cs.SY eess.SY math.IT\n",
            "quant-ph cond-mat.stat-mech hep-th\n",
            "physics.plasm-ph astro-ph.GA astro-ph.HE\n",
            "math.OC cs.GT cs.SY\n",
            "physics.app-ph physics.optics\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph\n",
            "math.QA math.RT\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.BM\n",
            "physics.ins-det physics.med-ph\n",
            "cs.SI cs.LG physics.soc-ph\n",
            "math.SG math.AG\n",
            "physics.plasm-ph astro-ph.CO astro-ph.HE physics.flu-dyn\n",
            "math.OA math.GR\n",
            "cs.RO cs.CV\n",
            "physics.comp-ph cond-mat.mtrl-sci cs.LG\n",
            "cond-mat.mes-hall physics.optics\n",
            "physics.optics physics.app-ph\n",
            "cond-mat.stat-mech quant-ph\n",
            "math.NA cs.LG\n",
            "math.ST math.PR q-fin.RM stat.TH\n",
            "math.OC math.ST q-bio.MN stat.TH\n",
            "math.GT math.QA\n",
            "q-bio.QM q-bio.NC\n",
            "math.DS cs.SY eess.SP nlin.CD\n",
            "math.CO cs.IT math.IT\n",
            "quant-ph cond-mat.stat-mech nlin.CD\n",
            "math.QA math.AG\n",
            "math.GT math.AT math.CO math.DG math.GR\n",
            "hep-ph hep-ex\n",
            "math.OA math.LO quant-ph\n",
            "cs.GT cs.CC cs.MA\n",
            "math.AG quant-ph\n",
            "math.AT math.RT\n",
            "math.PR cs.LO math.CT math.FA\n",
            "math.CT math.LO\n",
            "math.CA math.FA\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech hep-th\n",
            "econ.EM econ.GN q-fin.EC\n",
            "nlin.PS nlin.SI\n",
            "math-ph math.DS math.MP\n",
            "econ.EM stat.ME\n",
            "math.MG math-ph math.AP math.MP\n",
            "math.AC math.AG math.RT\n",
            "hep-th math.QA\n",
            "math.DG math.GT\n",
            "quant-ph cs.DS math.OC\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "cs.IT math.IT math.OC\n",
            "physics.soc-ph cs.CY cs.MA\n",
            "physics.ins-det physics.plasm-ph\n",
            "cs.CY cs.SI\n",
            "physics.chem-ph\n",
            "math-ph cond-mat.quant-gas cond-mat.str-el math.MP\n",
            "math.NT math.AG math.PR\n",
            "cs.LG cs.AI cs.IR stat.ML\n",
            "q-fin.ST\n",
            "quant-ph cs.SY\n",
            "math.OC cs.CV math.ST stat.TH\n",
            "econ.EM econ.GN q-fin.EC stat.ME\n",
            "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.str-el\n",
            "math.AG math.GT\n",
            "physics.bio-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall\n",
            "stat.CO cond-mat.stat-mech math.PR\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas\n",
            "physics.hist-ph hep-th physics.class-ph\n",
            "math.CO math.QA\n",
            "quant-ph cond-mat.mes-hall cond-mat.supr-con\n",
            "gr-qc astro-ph.CO hep-th\n",
            "cs.LG cs.CR stat.ML\n",
            "cond-mat.supr-con cond-mat.mes-hall\n",
            "math.GT math.CV\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el physics.atom-ph\n",
            "cond-mat.soft cond-mat.stat-mech q-bio.CB\n",
            "physics.class-ph cond-mat.mes-hall math-ph math.MP physics.optics quant-ph\n",
            "math.OC cs.LG cs.NA\n",
            "quant-ph cond-mat.mes-hall physics.optics\n",
            "quant-ph physics.acc-ph\n",
            "cs.NI\n",
            "math.AG math.RT math.SG\n",
            "math.ST stat.ML stat.TH\n",
            "cs.LG cs.NA math.NA stat.ML\n",
            "math.GN math.CT math.RA\n",
            "cs.DM math.CO\n",
            "hep-th astro-ph.HE gr-qc\n",
            "astro-ph.IM physics.hist-ph\n",
            "cs.CV physics.bio-ph physics.med-ph q-bio.QM\n",
            "cs.IR cs.CL\n",
            "q-fin.MF math.PR q-fin.PM\n",
            "quant-ph cs.IT math-ph math.IT math.MP\n",
            "cond-mat.mes-hall physics.class-ph physics.optics\n",
            "cs.LG cs.RO stat.ML\n",
            "quant-ph math-ph math.FA math.MP\n",
            "quant-ph cond-mat.mes-hall cond-mat.quant-gas cond-mat.stat-mech\n",
            "hep-th cond-mat.str-el hep-lat hep-ph quant-ph\n",
            "q-bio.PE cs.DB\n",
            "cs.SY cond-mat.stat-mech nlin.AO physics.soc-ph\n",
            "quant-ph cond-mat.supr-con\n",
            "hep-th cond-mat.str-el gr-qc\n",
            "physics.optics physics.bio-ph\n",
            "math.AP math.DS math.OC\n",
            "hep-th math-ph math.MP\n",
            "physics.comp-ph cond-mat.str-el\n",
            "q-bio.PE math.DS math.PR\n",
            "q-bio.QM\n",
            "nlin.SI math-ph math.AG math.MP\n",
            "hep-th math.RT\n",
            "cond-mat.str-el cond-mat.supr-con hep-th\n",
            "math.KT\n",
            "cond-mat.stat-mech cond-mat.other\n",
            "quant-ph cond-mat.other hep-th math-ph math.MP\n",
            "q-bio.SC cond-mat.stat-mech\n",
            "gr-qc hep-ex hep-th\n",
            "cond-mat.mes-hall gr-qc hep-th\n",
            "physics.app-ph quant-ph\n",
            "hep-ph nucl-ex nucl-th\n",
            "physics.bio-ph cond-mat.soft q-bio.BM\n",
            "stat.CO stat.ME\n",
            "cond-mat.mes-hall hep-ph\n",
            "quant-ph cond-mat.supr-con physics.acc-ph\n",
            "math.OC cs.SY\n",
            "physics.comp-ph cond-mat.mtrl-sci physics.data-an\n",
            "stat.AP econ.GN q-fin.EC\n",
            "cs.IT cs.DM math.IT\n",
            "quant-ph physics.app-ph\n",
            "quant-ph cond-mat.str-el hep-th math-ph math.MP\n",
            "math.FA math.SP\n",
            "cond-mat.quant-gas gr-qc hep-th\n",
            "math.AT stat.ML\n",
            "cs.LG cs.CE cs.NA math.AP math.NA stat.ML\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
            "quant-ph cond-mat.mes-hall physics.app-ph physics.ins-det\n",
            "quant-ph cond-mat.dis-nn physics.chem-ph physics.optics\n",
            "cs.SY cs.AI math.OC\n",
            "math.CT cs.LO\n",
            "cs.DC\n",
            "physics.atom-ph gr-qc quant-ph\n",
            "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
            "cond-mat.stat-mech math-ph math.MP\n",
            "hep-ph hep-th\n",
            "math-ph math.MP math.OA math.QA\n",
            "cond-mat.stat-mech cond-mat.dis-nn cs.LG hep-th\n",
            "physics.soc-ph q-fin.GN\n",
            "cond-mat.dis-nn cs.ET\n",
            "econ.GN q-fin.EC\n",
            "math.CT math.AG\n",
            "astro-ph.HE hep-ex hep-ph\n",
            "cs.CV cs.LG stat.ML\n",
            "quant-ph math.PR\n",
            "hep-th cond-mat.quant-gas cond-mat.str-el\n",
            "math.SG math.GT\n",
            "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI nlin.AO\n",
            "hep-ph hep-ex physics.comp-ph stat.ML\n",
            "nucl-ex astro-ph.HE nucl-th\n",
            "astro-ph.CO gr-qc hep-ph\n",
            "math.AP math-ph math.CA math.FA math.MP\n",
            "quant-ph physics.atom-ph physics.optics\n",
            "math.DG math.MG\n",
            "eess.IV\n",
            "math.GT math.AG math.DS\n",
            "cs.HC cs.AI\n",
            "cond-mat.mes-hall physics.atom-ph physics.optics quant-ph\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech\n",
            "cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
            "math.CV math.AG\n",
            "physics.data-an\n",
            "physics.gen-ph gr-qc hep-th\n",
            "cs.SY cs.MA math.OC\n",
            "cs.CL cs.AI cs.LG cs.NE\n",
            "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "physics.plasm-ph physics.acc-ph physics.app-ph\n",
            "cond-mat.str-el hep-th math-ph math.MP\n",
            "stat.AP cs.CY\n",
            "math.NT math.PR\n",
            "math.PR cs.LG cs.NA math.NA\n",
            "hep-lat cond-mat.stat-mech\n",
            "math.OC cs.AI cs.CC math.NA\n",
            "physics.optics cond-mat.mes-hall\n",
            "cs.GT cs.MA math.PR\n",
            "math.NT math.CA\n",
            "physics.optics physics.chem-ph\n",
            "physics.app-ph cs.ET physics.optics\n",
            "math.QA math.OA\n",
            "physics.comp-ph quant-ph\n",
            "astro-ph.CO gr-qc hep-th\n",
            "cond-mat.mes-hall cond-mat.quant-gas\n",
            "gr-qc math-ph math.AP math.DG math.MP\n",
            "hep-th gr-qc quant-ph\n",
            "stat.ML cs.LG math.OC\n",
            "cs.IT math.IT math.PR math.ST stat.TH\n",
            "cs.AR cs.AI cs.CV cs.LG\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el\n",
            "physics.ao-ph cs.CV cs.LG\n",
            "hep-th math.GR\n",
            "cond-mat.mes-hall cond-mat.str-el\n",
            "q-bio.TO physics.bio-ph q-bio.CB\n",
            "stat.ML cs.LG math.ST stat.ME stat.TH\n",
            "quant-ph cs.CR cs.IT math.IT\n",
            "stat.CO astro-ph.IM gr-qc stat.ME\n",
            "cond-mat.stat-mech cond-mat.quant-gas quant-ph\n",
            "math.AG cs.CC\n",
            "cond-mat.mtrl-sci physics.class-ph\n",
            "math.OC physics.comp-ph\n",
            "cs.ET\n",
            "math.OC cs.NA math.AP math.NA\n",
            "math-ph math.DS math.MP math.PR\n",
            "cond-mat.mtrl-sci physics.optics\n",
            "astro-ph.HE astro-ph.CO\n",
            "quant-ph cond-mat.stat-mech physics.chem-ph\n",
            "physics.optics physics.med-ph\n",
            "physics.data-an stat.ML\n",
            "hep-lat hep-ph nucl-ex nucl-th\n",
            "eess.SP nlin.CD\n",
            "hep-ph astro-ph.HE hep-th nucl-th\n",
            "stat.CO cs.NA math.NA stat.ME\n",
            "quant-ph cond-mat.mes-hall cs.AI cs.ET cs.LG\n",
            "physics.space-ph astro-ph.EP\n",
            "gr-qc cond-mat.quant-gas physics.flu-dyn\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el\n",
            "math-ph math.MP math.OA quant-ph\n",
            "math.RA math.CO\n",
            "q-fin.GN cs.GT cs.IT math.IT math.ST stat.TH\n",
            "gr-qc cond-mat.other hep-th\n",
            "quant-ph cond-mat.dis-nn cond-mat.str-el\n",
            "cs.DS math.CO\n",
            "gr-qc hep-ph hep-th\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn\n",
            "cs.GT cs.LG cs.SI stat.ML\n",
            "hep-ph astro-ph.CO hep-th\n",
            "hep-ph hep-ex nucl-ex nucl-th physics.data-an\n",
            "cs.LG math.CA\n",
            "cond-mat.stat-mech cond-mat.soft\n",
            "math.NA cs.NA stat.CO\n",
            "cs.GT math.OC\n",
            "q-bio.PE physics.bio-ph q-bio.CB\n",
            "physics.flu-dyn physics.ao-ph physics.geo-ph\n",
            "q-bio.QM physics.bio-ph\n",
            "math.OC cs.CE\n",
            "math.QA math-ph math.CT math.MP\n",
            "physics.soc-ph math.OC\n",
            "cond-mat.str-el hep-lat hep-th\n",
            "cond-mat.stat-mech cond-mat.soft math.ST physics.data-an stat.TH\n",
            "hep-th cond-mat.stat-mech\n",
            "gr-qc hep-th math-ph math.DG math.MP\n",
            "physics.comp-ph cs.NA math.NA physics.optics\n",
            "math-ph cond-mat.dis-nn math.MP\n",
            "cs.NA math.FA math.NA\n",
            "hep-th gr-qc hep-ph\n",
            "cs.CG cs.GR\n",
            "hep-th astro-ph.HE cond-mat.str-el gr-qc\n",
            "math.QA cond-mat.str-el hep-th\n",
            "quant-ph cond-mat.other cond-mat.quant-gas cond-mat.str-el\n",
            "math.AT math.KT\n",
            "q-bio.PE physics.bio-ph physics.soc-ph\n",
            "math.DS cs.IT math.IT\n",
            "math-ph hep-th math.CO math.MP math.RA\n",
            "cond-mat.mes-hall nlin.CG\n",
            "physics.chem-ph cond-mat.str-el\n",
            "math.AT cs.CG\n",
            "physics.app-ph cond-mat.supr-con\n",
            "quant-ph cond-mat.mes-hall physics.atom-ph\n",
            "hep-th cond-mat.supr-con\n",
            "cs.RO cs.CY\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el quant-ph\n",
            "nucl-th astro-ph.HE nucl-ex\n",
            "math.NA cs.NA math.AP math.OC\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph\n",
            "cs.CV cs.CR cs.LG cs.RO\n",
            "physics.ins-det hep-ex nucl-ex\n",
            "math.ST cs.IT math.IT stat.TH\n",
            "cs.CG cs.CV math.NA physics.med-ph q-bio.QM\n",
            "quant-ph cond-mat.mes-hall cond-mat.quant-gas\n",
            "math.GR math.RT\n",
            "quant-ph physics.atom-ph\n",
            "eess.SP cs.NI\n",
            "math.NA cs.NA physics.class-ph\n",
            "stat.ML cs.LG cs.SI stat.AP\n",
            "physics.ed-ph cond-mat.stat-mech\n",
            "math.RT math.CO\n",
            "math.PR cs.NA math.NA\n",
            "q-bio.CB q-bio.TO\n",
            "nlin.AO\n",
            "physics.soc-ph cond-mat.stat-mech\n",
            "eess.SP cs.HC cs.NE\n",
            "nlin.CD\n",
            "cond-mat.mes-hall physics.app-ph\n",
            "physics.soc-ph cs.SI nlin.AO\n",
            "hep-lat hep-th math-ph math.MP\n",
            "hep-th cond-mat.stat-mech quant-ph\n",
            "physics.chem-ph physics.comp-ph\n",
            "hep-th nlin.SI\n",
            "math.MG cs.CG\n",
            "gr-qc math.DG\n",
            "cs.IT cs.AI math.IT stat.ML\n",
            "astro-ph.CO astro-ph.HE hep-ph\n",
            "nlin.PS\n",
            "math.NA cs.NA math.PR\n",
            "cond-mat.mtrl-sci cond-mat.soft\n",
            "hep-lat hep-th quant-ph\n",
            "q-bio.QM cond-mat.stat-mech physics.bio-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph\n",
            "cs.DL cs.SI\n",
            "hep-ph hep-ex nucl-th\n",
            "cs.DC cs.LG q-bio.QM\n",
            "nlin.PS physics.bio-ph\n",
            "physics.chem-ph cond-mat.mtrl-sci\n",
            "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
            "hep-ph astro-ph.CO hep-ex nucl-ex physics.data-an\n",
            "math.GR math.GT\n",
            "math.OC math.DS math.SP\n",
            "cs.DS cs.DM math.OC\n",
            "cond-mat.soft cond-mat.stat-mech physics.flu-dyn\n",
            "quant-ph cond-mat.dis-nn\n",
            "quant-ph cond-mat.dis-nn cs.AI cs.DS\n",
            "physics.soc-ph gr-qc\n",
            "gr-qc astro-ph.CO astro-ph.GA\n",
            "hep-ph astro-ph.HE\n",
            "math.QA math.GR\n",
            "cs.LG cs.AI cs.MA stat.ML\n",
            "astro-ph.CO hep-ex hep-ph\n",
            "math-ph math.DS math.FA math.MP\n",
            "hep-th cond-mat.str-el math-ph math.AT math.GT math.MP\n",
            "hep-th cond-mat.str-el hep-lat hep-ph math.AT\n",
            "astro-ph.GA astro-ph.CO hep-ph\n",
            "cond-mat.mes-hall cond-mat.supr-con physics.optics\n",
            "astro-ph.SR physics.flu-dyn\n",
            "physics.soc-ph cond-mat.soft\n",
            "quant-ph cs.NA math.NA\n",
            "cond-mat.mtrl-sci cond-mat.other physics.optics\n",
            "cs.GR cs.CV\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph\n",
            "cs.DM\n",
            "stat.ML cs.LG stat.CO stat.ME\n",
            "cs.CV cs.CG q-bio.QM\n",
            "cs.SI cs.LG\n",
            "math.DS math.AP\n",
            "cs.MM\n",
            "math.DG math.DS\n",
            "cond-mat.mes-hall cs.LG\n",
            "gr-qc astro-ph.IM\n",
            "stat.ME econ.EM math.ST q-fin.RM q-fin.ST stat.TH\n",
            "math.OC cs.SY eess.SY q-fin.MF\n",
            "stat.ME econ.EM stat.AP\n",
            "math-ph math.AP math.MP quant-ph\n",
            "cs.GR\n",
            "cond-mat.mes-hall physics.app-ph quant-ph\n",
            "astro-ph.CO hep-th\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con\n",
            "cs.NE cs.AI cs.LG\n",
            "math.MG math.CA\n",
            "cond-mat.mtrl-sci physics.geo-ph\n",
            "math.NA math.OC\n",
            "stat.CO cs.LG stat.ML\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech math-ph math.MP\n",
            "cs.NE hep-lat physics.comp-ph\n",
            "hep-lat hep-ph hep-th\n",
            "physics.soc-ph math.CO math.OC\n",
            "physics.data-an hep-ph physics.comp-ph\n",
            "math-ph math.CA math.MP quant-ph\n",
            "hep-ex hep-ph\n",
            "physics.plasm-ph hep-ph hep-th\n",
            "math.GM\n",
            "cs.SI cs.CY\n",
            "cs.NA physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph\n",
            "astro-ph.SR astro-ph.HE physics.flu-dyn\n",
            "math.AG math.CO\n",
            "cond-mat.str-el hep-lat hep-th quant-ph\n",
            "math.DG math.AP\n",
            "math.FA math.AP math.CA\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con nlin.SI\n",
            "cs.AI cs.CE\n",
            "nucl-ex hep-ex nucl-th\n",
            "cond-mat.mes-hall cond-mat.str-el hep-th\n",
            "stat.ML cs.LG math.ST stat.TH\n",
            "astro-ph.SR astro-ph.IM\n",
            "stat.ME stat.ML\n",
            "cs.DC cs.CR\n",
            "hep-ph hep-lat\n",
            "q-bio.NC q-bio.QM\n",
            "cond-mat.mtrl-sci cond-mat.supr-con\n",
            "cond-mat.quant-gas nlin.PS\n",
            "math.QA cond-mat.str-el math-ph math.MP quant-ph\n",
            "stat.ML cs.LG eess.SP\n",
            "cond-mat.str-el cond-mat.mtrl-sci quant-ph\n",
            "math.QA math.GT\n",
            "math.GT math.SG\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el\n",
            "cs.GT cs.MA\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.optics\n",
            "gr-qc astro-ph.HE hep-th\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con\n",
            "cond-mat.quant-gas physics.atom-ph\n",
            "physics.plasm-ph nucl-th\n",
            "cs.LG cs.CV stat.ML\n",
            "physics.ins-det cond-mat.supr-con\n",
            "physics.bio-ph cond-mat.soft q-bio.SC\n",
            "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
            "math-ph math.CO math.MP math.PR nlin.SI\n",
            "math.FA math.OC\n",
            "cs.SD eess.AS\n",
            "cs.NA math.NA\n",
            "stat.AP cs.CY stat.ME stat.ML\n",
            "nlin.AO cond-mat.soft\n",
            "cs.CY cs.LG stat.ML\n",
            "stat.AP stat.ME\n",
            "cond-mat.quant-gas physics.atm-clus quant-ph\n",
            "math-ph cond-mat.quant-gas math.MP\n",
            "cs.LG cs.AI cs.RO stat.ML\n",
            "cond-mat.stat-mech cond-mat.dis-nn physics.optics\n",
            "quant-ph physics.atom-ph physics.data-an\n",
            "cs.LG stat.AP stat.ML\n",
            "cs.CV cs.LG\n",
            "cond-mat.mes-hall cond-mat.dis-nn\n",
            "hep-ex\n",
            "cs.MM cs.HC\n",
            "cs.DL cs.CY\n",
            "cond-mat.mes-hall cond-mat.supr-con\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el hep-th\n",
            "q-fin.TR math.OC q-fin.MF\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.str-el\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.mes-hall\n",
            "physics.app-ph cond-mat.mtrl-sci quant-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
            "cs.LG physics.chem-ph stat.ML\n",
            "cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
            "cs.IT math.IT q-bio.NC\n",
            "physics.atom-ph cond-mat.other\n",
            "cond-mat.stat-mech physics.comp-ph\n",
            "math.CA math.SP\n",
            "math.FA math.CA math.PR\n",
            "physics.bio-ph cond-mat.soft cond-mat.stat-mech\n",
            "cond-mat.str-el cond-mat.stat-mech math-ph math.MP\n",
            "physics.app-ph cond-mat.other cs.ET\n",
            "physics.optics physics.atom-ph physics.ins-det quant-ph\n",
            "physics.acc-ph hep-ex physics.ins-det\n",
            "math.QA math.RA\n",
            "math.ST math.NA math.OC stat.TH\n",
            "quant-ph cs.LO math.CO\n",
            "q-fin.CP\n",
            "physics.soc-ph cond-mat.dis-nn\n",
            "cs.LG physics.app-ph stat.ML\n",
            "cond-mat.dis-nn cond-mat.str-el cs.LG\n",
            "math.CV math.CO\n",
            "nucl-ex hep-ex\n",
            "quant-ph gr-qc physics.hist-ph\n",
            "cs.LO cs.CR cs.SY eess.SY\n",
            "quant-ph math-ph math.MP math.PR\n",
            "cs.SC\n",
            "quant-ph gr-qc math-ph math.MP\n",
            "physics.app-ph cond-mat.mes-hall\n",
            "math.AC math.CO\n",
            "nlin.AO math.DS nlin.PS physics.bio-ph\n",
            "cs.CC cs.FL math.DS\n",
            "physics.med-ph\n",
            "math.DG math.AP math.MG math.OC\n",
            "cond-mat.stat-mech nlin.CD quant-ph\n",
            "math-ph cond-mat.stat-mech math.MP math.PR\n",
            "physics.app-ph cond-mat.mtrl-sci physics.comp-ph\n",
            "quant-ph cond-mat.other\n",
            "stat.OT quant-ph\n",
            "physics.app-ph nlin.CD\n",
            "cond-mat.quant-gas nucl-th physics.atom-ph\n",
            "nucl-ex nucl-th\n",
            "cond-mat.mes-hall cond-mat.other quant-ph\n",
            "physics.comp-ph cs.LG\n",
            "astro-ph.SR astro-ph.GA astro-ph.HE\n",
            "cs.SI physics.soc-ph q-fin.GN\n",
            "math.CV math.PR math.RA\n",
            "physics.data-an nucl-ex\n",
            "hep-ph astro-ph.CO physics.atom-ph\n",
            "physics.geo-ph physics.comp-ph\n",
            "gr-qc astro-ph.HE astro-ph.IM hep-ph\n",
            "hep-ex cs.CV cs.LG physics.data-an\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech\n",
            "hep-ph cs.CV hep-ex\n",
            "cond-mat.soft physics.optics\n",
            "cs.GR cs.CG\n",
            "quant-ph cond-mat.quant-gas cs.LG\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech\n",
            "cond-mat.quant-gas cond-mat.stat-mech hep-lat quant-ph\n",
            "cs.RO cs.AI cs.CL cs.CV cs.LG\n",
            "cond-mat.stat-mech math-ph math.MP physics.class-ph\n",
            "gr-qc astro-ph.CO\n",
            "physics.flu-dyn math.AP math.DS\n",
            "cond-mat.mes-hall math-ph math.MP physics.ao-ph physics.flu-dyn\n",
            "math.ST cs.NA math.NA stat.ML stat.TH\n",
            "physics.geo-ph cs.LG eess.SP\n",
            "quant-ph physics.app-ph physics.optics\n",
            "quant-ph cs.GT cs.LG physics.comp-ph\n",
            "cs.LG cs.DS stat.ML\n",
            "quant-ph physics.chem-ph\n",
            "eess.SY cs.SY math.OC\n",
            "physics.atom-ph cond-mat.quant-gas physics.optics quant-ph\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el hep-lat math-ph math.MP\n",
            "math.AG math.GN\n",
            "physics.ins-det astro-ph.IM nucl-ex\n",
            "cs.AI cs.CL cs.CV\n",
            "hep-ph astro-ph.IM hep-ex quant-ph\n",
            "cond-mat.str-el physics.data-an\n",
            "cond-mat.dis-nn cond-mat.stat-mech hep-th\n",
            "nlin.AO cond-mat.dis-nn\n",
            "cs.NI cs.SI math.OC\n",
            "math.CO math.AG\n",
            "physics.atom-ph physics.ins-det physics.optics\n",
            "cond-mat.mes-hall physics.app-ph physics.ins-det\n",
            "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el hep-lat quant-ph\n",
            "cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
            "quant-ph math.CT\n",
            "math.OC cs.SY math.DS\n",
            "math.AP math-ph math.CA math.MP math.SP\n",
            "stat.ML cs.LG stat.CO\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.optics quant-ph\n",
            "eess.IV cs.CV\n",
            "math-ph hep-th math.CT math.DG math.MP\n",
            "cs.AI cs.CY cs.LG stat.ML\n",
            "math-ph math.AP math.MP math.RT physics.optics quant-ph\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.stat-mech quant-ph\n",
            "cs.CR cs.AR\n",
            "math.CO math.RT\n",
            "cs.CR cs.DC cs.GT\n",
            "cs.GT cs.CR\n",
            "eess.SP cs.NE physics.optics\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
            "cs.CG cs.CC\n",
            "astro-ph.IM astro-ph.SR\n",
            "gr-qc astro-ph.CO hep-ph\n",
            "math.CO math.HO\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el hep-th\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el\n",
            "physics.app-ph cond-mat.mes-hall physics.optics\n",
            "hep-ex cs.CV\n",
            "cs.LG cs.DC cs.PF\n",
            "cond-mat.str-el cond-mat.mtrl-sci physics.app-ph\n",
            "physics.optics cond-mat.dis-nn\n",
            "physics.plasm-ph physics.acc-ph physics.comp-ph\n",
            "cs.SI cs.CY physics.data-an\n",
            "quant-ph math-ph math.MP physics.optics\n",
            "math-ph cond-mat.dis-nn hep-th math.KT math.MP\n",
            "q-fin.MF q-fin.PM\n",
            "quant-ph cond-mat.other cs.DM hep-lat math-ph math.MP\n",
            "cs.IT math.FA math.IT math.PR\n",
            "physics.data-an hep-ex nucl-ex\n",
            "cs.FL cs.CC\n",
            "math-ph hep-th math.MP math.OA\n",
            "quant-ph cond-mat.mtrl-sci physics.optics\n",
            "math.CA math.CV\n",
            "stat.ML cs.LG stat.ME\n",
            "stat.AP nlin.CD\n",
            "physics.acc-ph physics.comp-ph\n",
            "physics.class-ph cond-mat.stat-mech\n",
            "math.OC math.DS physics.soc-ph\n",
            "physics.soc-ph cond-mat.stat-mech cs.IT math.IT physics.data-an\n",
            "cs.CV cs.AI cs.LG\n",
            "cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con\n",
            "physics.med-ph physics.comp-ph\n",
            "hep-ph hep-lat nucl-th\n",
            "physics.chem-ph physics.optics\n",
            "math.AP math.MG math.OC\n",
            "math.CV math.PR\n",
            "math.OC math.CA math.ST stat.TH\n",
            "math.CA math.CO\n",
            "cond-mat.mtrl-sci nlin.PS\n",
            "astro-ph.HE hep-ph nucl-th\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.comp-ph\n",
            "gr-qc physics.data-an\n",
            "nucl-ex physics.ins-det\n",
            "cs.LG cs.IT eess.SP math.IT stat.ML\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall quant-ph\n",
            "gr-qc hep-th math-ph math.MP quant-ph\n",
            "math.AP q-bio.CB\n",
            "math.AP math.CA\n",
            "astro-ph.CO gr-qc physics.optics\n",
            "math.PR cond-mat.stat-mech math-ph math.CV math.MP\n",
            "math.DG math.NA math.RA\n",
            "cond-mat.dis-nn cond-mat.supr-con\n",
            "q-fin.PM math.OC q-fin.RM\n",
            "cond-mat.str-el cond-mat.stat-mech hep-th\n",
            "cs.DL cs.CY physics.soc-ph\n",
            "cs.IT cond-mat.stat-mech math.IT stat.ML\n",
            "cs.LG cond-mat.str-el quant-ph stat.ML\n",
            "gr-qc math-ph math.MP\n",
            "astro-ph.IM astro-ph.GA\n",
            "math.AG math.AC math.RT\n",
            "cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el\n",
            "physics.ins-det gr-qc\n",
            "q-bio.PE cs.LG q-bio.QM stat.ML\n",
            "q-bio.PE q-bio.CB\n",
            "math.AP math.FA\n",
            "cond-mat.mtrl-sci physics.app-ph\n",
            "hep-th cond-mat.mes-hall hep-lat\n",
            "cond-mat.dis-nn cond-mat.stat-mech cs.LG\n",
            "cs.LG cs.IR stat.ML\n",
            "quant-ph physics.comp-ph\n",
            "hep-ph astro-ph.GA\n",
            "cond-mat.str-el math.CT math.QA\n",
            "cs.CG cs.DC cs.GR math.CV math.DG\n",
            "stat.ML cs.CV cs.LG\n",
            "math.CA math.CO math.CV\n",
            "astro-ph.HE astro-ph.SR\n",
            "nlin.CD cond-mat.stat-mech\n",
            "nucl-th cond-mat.dis-nn cs.LG\n",
            "math-ph cond-mat.quant-gas math.MP quant-ph\n",
            "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "stat.ML cs.IT cs.LG math.IT\n",
            "cond-mat.other cond-mat.quant-gas hep-ph hep-th physics.atom-ph\n",
            "math.CA math.MG\n",
            "stat.ML cs.CV cs.LG eess.IV\n",
            "physics.comp-ph physics.app-ph\n",
            "math.PR math.QA math.RT math.SG\n",
            "cs.CV cs.LG cs.RO\n",
            "physics.comp-ph cond-mat.mtrl-sci cs.CE\n",
            "cond-mat.stat-mech cond-mat.soft hep-th\n",
            "eess.SP physics.app-ph\n",
            "cond-mat.mes-hall physics.data-an quant-ph\n",
            "nlin.PS cond-mat.soft cond-mat.stat-mech\n",
            "cs.LG eess.SP stat.ML\n",
            "math.OC cs.SY eess.SY\n",
            "math.HO\n",
            "physics.ins-det hep-ex\n",
            "math.CA cs.LG\n",
            "cond-mat.soft physics.bio-ph\n",
            "math.OC math.DG math.NA\n",
            "math.FA math.OA\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas\n",
            "math.DS cond-mat.stat-mech math-ph math.MP math.PR\n",
            "cond-mat.str-el cond-mat.quant-gas quant-ph\n",
            "physics.comp-ph cond-mat.soft\n",
            "nucl-th astro-ph.HE\n",
            "stat.CO cs.DS math.CO stat.AP\n",
            "physics.bio-ph cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "cond-mat.stat-mech physics.bio-ph\n",
            "gr-qc astro-ph.GA\n",
            "cs.LG math.OC stat.ML\n",
            "stat.AP stat.CO\n",
            "hep-ph astro-ph.CO gr-qc\n",
            "cs.GR math.NA\n",
            "physics.class-ph cond-mat.quant-gas math-ph math.MP\n",
            "astro-ph.HE astro-ph.SR hep-ph\n",
            "cond-mat.stat-mech hep-th quant-ph\n",
            "hep-ph astro-ph.CO\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech\n",
            "physics.flu-dyn cs.NA math.NA\n",
            "physics.comp-ph cond-mat.mtrl-sci\n",
            "math.NT math-ph math.MP math.SG\n",
            "cs.SI cs.CY stat.AP\n",
            "cs.SI cs.LG stat.AP stat.ML\n",
            "eess.SP cs.IT math.IT\n",
            "math.SP cs.DM\n",
            "hep-th astro-ph.CO gr-qc\n",
            "quant-ph physics.atm-clus\n",
            "cs.CY cs.CR\n",
            "quant-ph cond-mat.mes-hall cs.CC\n",
            "cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
            "cs.LG cs.CV cs.NE physics.comp-ph quant-ph\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el\n",
            "physics.comp-ph cond-mat.mtrl-sci cs.NA math.NA\n",
            "quant-ph cond-mat.dis-nn nlin.CD\n",
            "q-bio.BM cs.LG\n",
            "nucl-ex astro-ph.IM\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el\n",
            "math-ph hep-th math.MP quant-ph\n",
            "physics.acc-ph cond-mat.supr-con\n",
            "astro-ph.HE astro-ph.CO astro-ph.IM gr-qc\n",
            "physics.optics nlin.AO nlin.PS\n",
            "cond-mat.soft cond-mat.dis-nn\n",
            "cs.DS cs.LO\n",
            "cond-mat.mes-hall cond-mat.soft\n",
            "cs.FL cs.CE\n",
            "cs.DS cs.IR\n",
            "math.AP math.CA math.FA\n",
            "physics.plasm-ph hep-ph\n",
            "cs.DB cs.DS\n",
            "cs.LG eess.IV stat.ML\n",
            "astro-ph.CO hep-ph nucl-th\n",
            "astro-ph.GA hep-ph\n",
            "math.FA cs.NA math.NA\n",
            "stat.ML cs.CV cs.LG stat.CO\n",
            "hep-ph astro-ph.HE gr-qc hep-th nucl-th\n",
            "cs.CV cs.AI cs.LG cs.RO\n",
            "math-ph hep-th math.MP physics.optics\n",
            "physics.atom-ph physics.optics\n",
            "math.PR math.AP\n",
            "physics.acc-ph physics.app-ph physics.ins-det physics.med-ph\n",
            "cond-mat.quant-gas cond-mat.stat-mech\n",
            "cs.CR cs.AI cs.CC cs.LG\n",
            "cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
            "cs.PF cs.DC\n",
            "stat.AP q-bio.GN q-bio.QM\n",
            "physics.soc-ph cs.SI physics.data-an\n",
            "physics.optics math-ph math.MP\n",
            "math.RT math.AT\n",
            "hep-th math-ph math.AT math.DG math.MP\n",
            "eess.IV physics.optics\n",
            "q-bio.TO q-bio.PE\n",
            "quant-ph cond-mat.stat-mech cs.IT hep-th math-ph math.IT math.MP\n",
            "quant-ph hep-lat hep-ph nucl-th\n",
            "hep-ex nucl-ex nucl-th\n",
            "quant-ph math.SG\n",
            "cond-mat.quant-gas cond-mat.mtrl-sci nucl-th physics.comp-ph\n",
            "quant-ph cs.LG stat.ML\n",
            "stat.AP cs.LG stat.ML\n",
            "astro-ph.CO hep-ph hep-th\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.soft cond-mat.str-el\n",
            "cs.AR\n",
            "cond-mat.stat-mech cond-mat.str-el\n",
            "q-bio.CB nlin.AO q-bio.PE\n",
            "hep-ph astro-ph.HE astro-ph.SR hep-ex\n",
            "cs.SE cs.AI\n",
            "physics.soc-ph q-bio.NC\n",
            "cs.IR\n",
            "hep-ph hep-ex hep-th quant-ph\n",
            "cond-mat.mes-hall cond-mat.stat-mech hep-th quant-ph\n",
            "math.DG math.AP math.OC\n",
            "econ.TH cs.LO math.CT\n",
            "math.AP math-ph math.MP\n",
            "q-fin.RM stat.ME\n",
            "cond-mat.dis-nn nlin.CD\n",
            "quant-ph cond-mat.mes-hall cond-mat.other math-ph math.MP\n",
            "cs.MM eess.IV\n",
            "cond-mat.str-el cond-mat.mtrl-sci physics.comp-ph\n",
            "q-bio.PE physics.bio-ph\n",
            "physics.soc-ph q-bio.PE\n",
            "physics.soc-ph q-fin.ST\n",
            "cond-mat.stat-mech math-ph math.MP quant-ph\n",
            "cond-mat.stat-mech cond-mat.dis-nn quant-ph\n",
            "cs.RO cs.LG cs.SY\n",
            "cond-mat.stat-mech cond-mat.dis-nn nlin.AO physics.data-an\n",
            "eess.SY cs.SY\n",
            "quant-ph cs.CC physics.comp-ph\n",
            "eess.IV cs.CV cs.GR\n",
            "physics.soc-ph stat.AP\n",
            "cs.SY cs.LG math.OC\n",
            "hep-ph astro-ph.HE nucl-th\n",
            "physics.optics nlin.CD\n",
            "cs.LG cs.HC stat.ML\n",
            "physics.data-an nlin.AO\n",
            "cond-mat.mes-hall cond-mat.supr-con quant-ph\n",
            "cs.DC cs.HC cs.SE\n",
            "cs.LO math.CT math.LO\n",
            "quant-ph cond-mat.other math-ph math.MP physics.comp-ph\n",
            "nlin.AO cond-mat.dis-nn math.DS q-bio.NC\n",
            "gr-qc astro-ph.CO quant-ph\n",
            "physics.app-ph cond-mat.str-el\n",
            "cs.DS q-bio.PE\n",
            "cs.SY quant-ph\n",
            "physics.bio-ph q-bio.SC\n",
            "cond-mat.stat-mech gr-qc hep-th math-ph math.MP\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el\n",
            "physics.flu-dyn nlin.CD\n",
            "cs.IR cs.AI\n",
            "cs.IR cs.LG\n",
            "cond-mat.mes-hall cond-mat.stat-mech\n",
            "quant-ph cs.CC cs.IT math.IT\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech math-ph math.MP\n",
            "quant-ph cond-mat.other math-ph math.MP\n",
            "hep-th cond-mat.str-el math-ph math.MP\n",
            "q-bio.QM math.AT\n",
            "cond-mat.quant-gas gr-qc\n",
            "cond-mat.str-el cond-mat.supr-con quant-ph\n",
            "cs.SY math.DS math.OC\n",
            "astro-ph.HE astro-ph.SR gr-qc\n",
            "cs.DM cs.DS\n",
            "math.OC econ.TH math.DS\n",
            "eess.AS\n",
            "cond-mat.stat-mech cond-mat.str-el hep-th\n",
            "cond-mat.stat-mech hep-th\n",
            "cs.SY cs.RO\n",
            "math.OC cs.MA\n",
            "cs.SD cs.SY eess.AS\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
            "quant-ph math.AT\n",
            "cs.CY cs.SI physics.soc-ph\n",
            "eess.IV cs.NA\n",
            "quant-ph cond-mat.mes-hall physics.chem-ph\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cs.CL cs.AI cs.HC cs.LG\n",
            "cond-mat.dis-nn physics.comp-ph\n",
            "cs.AI cs.LG cs.MA cs.NE cs.PL\n",
            "cs.GT math.PR physics.soc-ph\n",
            "physics.comp-ph cs.LG hep-lat hep-ph\n",
            "cond-mat.mes-hall cond-mat.quant-gas physics.optics\n",
            "math.AP cond-mat.mes-hall math-ph math.MP nlin.PS\n",
            "eess.SP cs.IT cs.NA math.IT math.NA math.OC\n",
            "hep-ph astro-ph.CO astro-ph.HE gr-qc\n",
            "cond-mat.mtrl-sci cs.CV eess.IV\n",
            "cs.MA cs.LG cs.SI math.DS stat.ML\n",
            "cond-mat.str-el hep-th math-ph math.MP math.QA\n",
            "gr-qc cond-mat.mes-hall hep-th\n",
            "cs.DC eess.SP\n",
            "cond-mat.quant-gas nucl-th\n",
            "nucl-th hep-ex hep-lat hep-ph nucl-ex\n",
            "cs.AI cs.NE cs.SE\n",
            "nucl-th cond-mat.quant-gas nucl-ex\n",
            "cs.LG cs.CC cs.DM physics.data-an stat.ML\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech cond-mat.supr-con\n",
            "nlin.CD physics.soc-ph\n",
            "cs.LG math.AT math.MG stat.ML\n",
            "cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP math.PR nlin.SI\n",
            "astro-ph.GA astro-ph.CO physics.plasm-ph\n",
            "physics.geo-ph nlin.CD\n",
            "physics.flu-dyn physics.ao-ph physics.data-an physics.geo-ph\n",
            "stat.ME stat.CO stat.ML\n",
            "physics.comp-ph physics.bio-ph\n",
            "physics.atom-ph nlin.CD\n",
            "cond-mat.str-el cond-mat.dis-nn quant-ph\n",
            "eess.IV physics.med-ph physics.optics\n",
            "physics.comp-ph physics.flu-dyn\n",
            "cs.FL math.NT\n",
            "cs.GR cs.CV cs.HC\n",
            "physics.flu-dyn cond-mat.soft physics.bio-ph\n",
            "q-bio.MN q-bio.SC\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech hep-th quant-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall physics.optics quant-ph\n",
            "hep-lat\n",
            "physics.bio-ph nlin.PS\n",
            "physics.optics physics.app-ph physics.atom-ph quant-ph\n",
            "math.PR math.AP math.SP\n",
            "cs.NI cs.ET\n",
            "cs.LG cs.DC stat.ML\n",
            "math.FA math-ph math.AP math.MP math.NA math.PR\n",
            "physics.soc-ph cond-mat.stat-mech math.PR\n",
            "cond-mat.stat-mech physics.bio-ph q-bio.MN\n",
            "cs.DS math.NT stat.CO\n",
            "cond-mat.quant-gas physics.optics quant-ph\n",
            "physics.comp-ph cond-mat.mes-hall\n",
            "astro-ph.EP physics.flu-dyn physics.geo-ph\n",
            "physics.bio-ph q-bio.TO\n",
            "physics.acc-ph physics.optics\n",
            "cs.LG cs.CY stat.ML\n",
            "stat.ME stat.AP stat.CO\n",
            "cs.SI stat.AP\n",
            "physics.ins-det physics.atom-ph\n",
            "quant-ph astro-ph.CO\n",
            "cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "math.OC cs.NA math.DS math.NA\n",
            "quant-ph nucl-th\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
            "physics.plasm-ph physics.comp-ph\n",
            "cond-mat.quant-gas physics.atom-ph physics.flu-dyn\n",
            "physics.comp-ph physics.plasm-ph\n",
            "physics.atom-ph hep-ex\n",
            "cond-mat.soft physics.app-ph\n",
            "hep-ph astro-ph.HE hep-ex\n",
            "hep-th cond-mat.str-el math-ph math.AT math.MP\n",
            "physics.atm-clus physics.atom-ph physics.optics\n",
            "math.CV math.DG\n",
            "physics.comp-ph cond-mat.mes-hall physics.ins-det\n",
            "physics.flu-dyn physics.geo-ph\n",
            "math.FA math.AP math.OC\n",
            "cond-mat.quant-gas nlin.PS physics.atom-ph\n",
            "astro-ph.IM physics.comp-ph physics.flu-dyn\n",
            "nlin.CG\n",
            "math.NA cs.LG cs.NA math.DS physics.comp-ph stat.ML\n",
            "cond-mat.dis-nn q-bio.NC\n",
            "q-bio.NC cond-mat.dis-nn physics.soc-ph\n",
            "gr-qc hep-ph\n",
            "eess.IV physics.app-ph physics.optics\n",
            "stat.ML cs.CV cs.LG math.FA\n",
            "math.NA cond-mat.soft cs.CE cs.CG cs.NA q-bio.QM\n",
            "math.AP math.NA\n",
            "cs.AI cs.CL cs.LG\n",
            "cond-mat.str-el cond-mat.dis-nn\n",
            "cond-mat.mes-hall cond-mat.str-el quant-ph\n",
            "cond-mat.quant-gas cond-mat.other cond-mat.str-el\n",
            "math.QA math.AT\n",
            "cs.CY cs.LG cs.NE\n",
            "cs.LG cond-mat.dis-nn stat.ML\n",
            "nlin.PS cond-mat.mes-hall physics.app-ph\n",
            "cs.DL cs.SE\n",
            "physics.soc-ph math.CO nlin.CG q-bio.PE\n",
            "cond-mat.soft physics.chem-ph physics.flu-dyn\n",
            "cond-mat.stat-mech nlin.AO\n",
            "math.CV math.AP math.DG\n",
            "physics.comp-ph cs.AI cs.LG hep-th\n",
            "cs.CV cs.AI cs.CR cs.LG\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "nlin.SI\n",
            "quant-ph math.NA\n",
            "physics.ins-det cs.LG hep-ex\n",
            "physics.ins-det cond-mat.mtrl-sci\n",
            "astro-ph.HE astro-ph.GA hep-ph\n",
            "cond-mat.quant-gas cond-mat.other\n",
            "cs.LG cs.DM cs.DS math.OC stat.ML\n",
            "cs.RO cs.MA cs.SY\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con quant-ph\n",
            "physics.flu-dyn physics.app-ph\n",
            "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG\n",
            "cond-mat.mtrl-sci cond-mat.str-el quant-ph\n",
            "physics.optics physics.ins-det\n",
            "math.AG math.CV math.DG\n",
            "quant-ph physics.ins-det physics.optics\n",
            "eess.SP cs.LG stat.ML\n",
            "q-bio.QM q-bio.MN\n",
            "cond-mat.stat-mech math.PR\n",
            "eess.SP cs.HC\n",
            "cond-mat.dis-nn cond-mat.mes-hall quant-ph\n",
            "q-bio.NC physics.data-an q-bio.QM\n",
            "cs.CG cs.LG math.AT stat.ML\n",
            "cs.CV cs.LG cs.NE\n",
            "cs.MA\n",
            "q-bio.CB\n",
            "nlin.PS math.DS\n",
            "cs.DB cs.DC cs.DS\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph physics.comp-ph\n",
            "hep-th nlin.CD quant-ph\n",
            "eess.SP physics.ins-det\n",
            "cond-mat.str-el hep-lat\n",
            "quant-ph cond-mat.mes-hall physics.app-ph\n",
            "nucl-th astro-ph.HE cond-mat.supr-con hep-th\n",
            "eess.IV cs.CV cs.LG\n",
            "cs.LG physics.chem-ph quant-ph stat.ML\n",
            "hep-ph astro-ph.CO cond-mat.mtrl-sci hep-ex\n",
            "eess.IV math.OC\n",
            "physics.comp-ph cond-mat.stat-mech\n",
            "stat.CO econ.EM stat.ME\n",
            "physics.app-ph eess.SP\n",
            "cond-mat.dis-nn quant-ph\n",
            "math.AP math-ph math.MP nlin.SI\n",
            "cond-mat.soft nlin.PS\n",
            "physics.soc-ph nlin.CG\n",
            "physics.optics physics.app-ph quant-ph\n",
            "cond-mat.mtrl-sci physics.app-ph physics.space-ph\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph q-bio.QM\n",
            "q-bio.NC cond-mat.dis-nn nlin.AO\n",
            "hep-th cond-mat.supr-con nucl-th\n",
            "hep-ph hep-ex hep-lat\n",
            "eess.SP physics.data-an physics.flu-dyn\n",
            "math-ph hep-th math.AT math.MP\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft cs.LG\n",
            "stat.ML cs.HC cs.LG\n",
            "physics.comp-ph cond-mat.mtrl-sci physics.flu-dyn\n",
            "physics.comp-ph astro-ph.CO astro-ph.IM cs.NA math.NA\n",
            "cs.LG cs.AI cs.MA cs.NE stat.ML\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci\n",
            "cs.LG cs.CV cs.NE\n",
            "quant-ph hep-ph hep-th math-ph math.MP physics.atom-ph\n",
            "quant-ph math-ph math.MP physics.app-ph physics.comp-ph physics.optics\n",
            "astro-ph.IM astro-ph.HE\n",
            "nucl-th astro-ph.HE cond-mat.quant-gas\n",
            "math.NA cs.LG cs.NA stat.ML\n",
            "gr-qc cond-mat.stat-mech hep-th quant-ph\n",
            "cond-mat.stat-mech cond-mat.str-el physics.comp-ph quant-ph\n",
            "cond-mat.dis-nn cond-mat.quant-gas\n",
            "quant-ph cond-mat.str-el physics.comp-ph\n",
            "cs.CV cs.SI\n",
            "cs.NE math.OC\n",
            "stat.ML cs.CL cs.IR cs.LG\n",
            "cs.GT econ.GN math.DS nlin.CD physics.soc-ph q-fin.EC\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.SC\n",
            "physics.optics eess.IV physics.bio-ph\n",
            "nlin.AO cond-mat.stat-mech\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el hep-th quant-ph\n",
            "eess.AS cs.SD eess.SP\n",
            "hep-ex hep-ph nucl-th\n",
            "hep-th cond-mat.other\n",
            "physics.atom-ph physics.optics quant-ph\n",
            "cs.RO math.AT\n",
            "cond-mat.soft physics.bio-ph physics.comp-ph physics.flu-dyn\n",
            "math.OC math.FA\n",
            "physics.optics physics.atom-ph physics.chem-ph quant-ph\n",
            "physics.optics cond-mat.mtrl-sci quant-ph\n",
            "math.QA math-ph math.MP\n",
            "math.DS nlin.CD physics.flu-dyn\n",
            "cond-mat.str-el cond-mat.stat-mech quant-ph\n",
            "quant-ph stat.ML\n",
            "cond-mat.supr-con physics.acc-ph\n",
            "math-ph cond-mat.str-el hep-th math.AT math.MP quant-ph\n",
            "q-bio.PE physics.soc-ph\n",
            "quant-ph astro-ph.CO gr-qc hep-th\n",
            "physics.optics physics.plasm-ph quant-ph\n",
            "cond-mat.quant-gas cond-mat.supr-con\n",
            "nlin.SI math-ph math.MP\n",
            "cs.SI stat.ME\n",
            "cond-mat.supr-con cond-mat.soft\n",
            "physics.comp-ph cond-mat.other\n",
            "physics.atom-ph cond-mat.mtrl-sci\n",
            "hep-ph astro-ph.CO astro-ph.HE hep-th\n",
            "quant-ph cond-mat.stat-mech gr-qc hep-th\n",
            "cs.RO cs.SY eess.SY\n",
            "hep-th gr-qc physics.bio-ph\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall math-ph math.MP\n",
            "math.NA cs.CE cs.NA\n",
            "q-bio.PE cond-mat.stat-mech\n",
            "q-fin.ST nlin.CD\n",
            "nlin.AO nlin.CD physics.bio-ph q-bio.NC\n",
            "physics.class-ph cond-mat.mes-hall\n",
            "cond-mat.mtrl-sci cond-mat.other\n",
            "astro-ph.SR astro-ph.HE\n",
            "physics.ins-det physics.geo-ph\n",
            "cond-mat.mtrl-sci physics.app-ph physics.optics\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.other\n",
            "math-ph math.FA math.MP math.OA\n",
            "cs.LG quant-ph stat.ML\n",
            "cond-mat.supr-con cond-mat.soft nlin.AO\n",
            "cond-mat.stat-mech physics.soc-ph\n",
            "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech\n",
            "eess.SP cs.CR cs.IT cs.LG math.IT stat.ML\n",
            "math.CO math.OA math.QA\n",
            "cond-mat.other quant-ph\n",
            "physics.data-an astro-ph.HE astro-ph.IM hep-ph stat.ME\n",
            "cs.CV cs.NA math.NA\n",
            "math.NA cond-mat.stat-mech cs.NA math.PR\n",
            "cs.DL cs.LG physics.hist-ph physics.soc-ph quant-ph\n",
            "math-ph cond-mat.dis-nn math.MP math.PR\n",
            "astro-ph.HE physics.comp-ph\n",
            "math.AP math.CV math.SG\n",
            "math.PR math.AT math.MG\n",
            "physics.atom-ph physics.atm-clus physics.optics quant-ph\n",
            "quant-ph cond-mat.str-el cs.DS physics.chem-ph\n",
            "hep-lat cond-mat.stat-mech hep-th\n",
            "q-bio.MN cs.LG\n",
            "hep-ph astro-ph.CO hep-ex\n",
            "cond-mat.mtrl-sci physics.chem-ph\n",
            "q-bio.NC cond-mat.dis-nn quant-ph\n",
            "quant-ph nlin.CD\n",
            "math.ST econ.EM stat.ME stat.TH\n",
            "cond-mat.str-el cond-mat.quant-gas math-ph math.MP\n",
            "physics.soc-ph cs.MA\n",
            "physics.optics cond-mat.mtrl-sci\n",
            "cond-mat.quant-gas cond-mat.soft cond-mat.stat-mech physics.flu-dyn\n",
            "cs.IT math.IT math.NT\n",
            "cond-mat.mes-hall cond-mat.other\n",
            "cond-mat.stat-mech physics.bio-ph q-bio.SC\n",
            "physics.flu-dyn math-ph math.MP physics.plasm-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci quant-ph\n",
            "physics.med-ph physics.bio-ph\n",
            "cs.LG math.DS nlin.CD stat.ML\n",
            "q-bio.MN physics.bio-ph\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech physics.chem-ph\n",
            "hep-lat hep-th\n",
            "cs.CR cs.LG stat.ML\n",
            "cs.GT cs.AI cs.LG\n",
            "physics.flu-dyn cond-mat.soft\n",
            "physics.comp-ph cs.LG physics.chem-ph\n",
            "quant-ph cond-mat.dis-nn cond-mat.str-el cs.LG\n",
            "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.atom-ph\n",
            "hep-th math-ph math.MP math.RT\n",
            "cs.DS quant-ph\n",
            "math.FA math.CO quant-ph\n",
            "nlin.AO cond-mat.dis-nn cond-mat.stat-mech math.DS\n",
            "physics.optics physics.acc-ph\n",
            "cond-mat.stat-mech cs.NA math.NA physics.soc-ph\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.str-el\n",
            "nucl-th hep-ex hep-ph\n",
            "cond-mat.stat-mech nlin.CD\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other cond-mat.str-el\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG\n",
            "cond-mat.dis-nn physics.data-an\n",
            "cond-mat.str-el cond-mat.quant-gas hep-th quant-ph\n",
            "astro-ph.EP astro-ph.IM physics.atom-ph\n",
            "physics.comp-ph cond-mat.mtrl-sci physics.chem-ph\n",
            "cond-mat.str-el cond-mat.stat-mech hep-th math-ph math.MP\n",
            "nlin.AO cs.NE q-bio.NC\n",
            "cond-mat.stat-mech cond-mat.mes-hall\n",
            "astro-ph.CO astro-ph.HE gr-qc hep-ex hep-ph\n",
            "hep-th gr-qc math-ph math.MP\n",
            "hep-ex hep-ph nucl-ex physics.data-an\n",
            "physics.flu-dyn cond-mat.dis-nn cond-mat.soft\n",
            "cs.CV cs.CL cs.HC cs.LG stat.ML\n",
            "physics.bio-ph physics.app-ph\n",
            "math.OC stat.AP\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
            "q-bio.BM cond-mat.stat-mech physics.bio-ph\n",
            "physics.comp-ph astro-ph.CO gr-qc\n",
            "math.CO math.GR\n",
            "cs.NE cs.LG\n",
            "hep-ph astro-ph.CO hep-ex nucl-ex nucl-th\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.supr-con\n",
            "hep-th cond-mat.mtrl-sci gr-qc\n",
            "q-bio.NC cond-mat.dis-nn\n",
            "physics.optics physics.app-ph physics.comp-ph\n",
            "cond-mat.mtrl-sci hep-ex\n",
            "cond-mat.stat-mech math.PR stat.OT\n",
            "quant-ph physics.chem-ph physics.comp-ph\n",
            "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech\n",
            "physics.med-ph cs.CE physics.comp-ph q-bio.QM\n",
            "physics.app-ph cs.ET\n",
            "astro-ph.GA astro-ph.SR\n",
            "cond-mat.dis-nn cs.IT math.IT\n",
            "cond-mat.other physics.flu-dyn\n",
            "math.FA math.CO math.MG\n",
            "cond-mat.quant-gas cond-mat.stat-mech hep-th quant-ph\n",
            "cs.RO eess.SP\n",
            "physics.plasm-ph physics.optics\n",
            "hep-ex physics.atom-ph\n",
            "math-ph math.FA math.MP quant-ph\n",
            "cs.CY cs.SI physics.data-an physics.soc-ph\n",
            "cond-mat.str-el cond-mat.stat-mech hep-th quant-ph\n",
            "cs.LG cs.IT math.IT stat.ML\n",
            "physics.plasm-ph astro-ph.SR\n",
            "cond-mat.stat-mech physics.class-ph\n",
            "eess.IV cs.LG stat.AP stat.ML\n",
            "cs.NI cs.SY eess.SY\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mes-hall quant-ph\n",
            "eess.SY cs.GT cs.SY math.OC\n",
            "astro-ph.IM gr-qc\n",
            "physics.space-ph astro-ph.SR physics.plasm-ph\n",
            "math.FA math.CA\n",
            "hep-ph astro-ph.CO hep-th math-ph math.MP\n",
            "hep-th astro-ph.CO gr-qc hep-ph\n",
            "eess.IV cs.LG stat.ML\n",
            "astro-ph.CO hep-ph physics.atom-ph\n",
            "math.KT math.AT math.MG\n",
            "physics.ao-ph physics.flu-dyn\n",
            "cs.DS cs.CV cs.LG\n",
            "cond-mat.supr-con cond-mat.mes-hall quant-ph\n",
            "cs.LG cs.CR cs.CV stat.ML\n",
            "physics.bio-ph physics.flu-dyn\n",
            "eess.IV cs.LG physics.med-ph\n",
            "physics.data-an cs.LG\n",
            "physics.optics cond-mat.quant-gas\n",
            "physics.optics cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
            "physics.atom-ph physics.chem-ph\n",
            "astro-ph.CO astro-ph.GA hep-ph physics.flu-dyn\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cs.CL cs.IR cs.LG\n",
            "quant-ph cond-mat.dis-nn cond-mat.mes-hall\n",
            "cs.CY cs.AI\n",
            "q-bio.QM q-bio.GN q-bio.MN q-bio.SC\n",
            "nucl-th astro-ph.HE cond-mat.quant-gas hep-lat quant-ph\n",
            "hep-th cond-mat.str-el nlin.PS physics.optics\n",
            "physics.atom-ph cond-mat.quant-gas\n",
            "physics.class-ph physics.app-ph physics.optics\n",
            "gr-qc astro-ph.CO hep-ph hep-th\n",
            "stat.ML cs.LG eess.SP physics.comp-ph\n",
            "astro-ph.IM astro-ph.GA astro-ph.HE gr-qc hep-th\n",
            "astro-ph.CO astro-ph.IM hep-ph\n",
            "cond-mat.mtrl-sci physics.comp-ph physics.optics quant-ph\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.geo-ph\n",
            "physics.app-ph physics.chem-ph\n",
            "cond-mat.quant-gas math-ph math.MP quant-ph\n",
            "physics.optics cond-mat.mes-hall physics.comp-ph\n",
            "cs.DS cs.CC cs.CG cs.IR\n",
            "math.CT math.OA\n",
            "physics.atm-clus physics.atom-ph\n",
            "q-fin.ST cond-mat.stat-mech physics.soc-ph\n",
            "math.CT math.GN\n",
            "eess.IV cs.CR cs.MM\n",
            "eess.IV cs.MM\n",
            "physics.optics cond-mat.mtrl-sci physics.app-ph\n",
            "q-bio.PE cond-mat.stat-mech nlin.AO physics.soc-ph\n",
            "q-bio.QM cs.LG\n",
            "physics.flu-dyn astro-ph.SR physics.plasm-ph\n",
            "quant-ph nlin.SI\n",
            "math.RA math.RT\n",
            "cond-mat.str-el physics.optics\n",
            "cs.LG cs.AI cs.MA\n",
            "hep-th math-ph math.AT math.MP\n",
            "cond-mat.stat-mech cond-mat.supr-con\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.class-ph physics.flu-dyn\n",
            "physics.comp-ph cs.LG physics.data-an stat.ML\n",
            "quant-ph cs.NI\n",
            "astro-ph.CO astro-ph.IM hep-ex physics.ins-det\n",
            "physics.chem-ph cond-mat.other physics.optics quant-ph\n",
            "physics.app-ph physics.bio-ph physics.med-ph quant-ph\n",
            "quant-ph cond-mat.dis-nn physics.app-ph physics.data-an\n",
            "astro-ph.GA hep-ph stat.ML\n",
            "q-fin.MF cs.LG q-fin.CP stat.ML\n",
            "astro-ph.CO astro-ph.GA gr-qc hep-ph\n",
            "quant-ph cond-mat.str-el math-ph math.MP math.QA\n",
            "cond-mat.stat-mech cond-mat.mes-hall quant-ph\n",
            "physics.space-ph physics.plasm-ph\n",
            "cs.LG cs.CG cs.DS quant-ph stat.ML\n",
            "cs.CY cs.CL\n",
            "cond-mat.mes-hall physics.class-ph\n",
            "cs.CV stat.ML\n",
            "math.CA math.AP\n",
            "physics.space-ph astro-ph.EP physics.ao-ph\n",
            "cond-mat.stat-mech astro-ph.CO\n",
            "nucl-th hep-ph physics.atom-ph\n",
            "math.NA astro-ph.IM cs.NA physics.comp-ph\n",
            "q-bio.QM math.PR q-bio.SC\n",
            "math-ph math.MP physics.data-an stat.AP\n",
            "math-ph cond-mat.stat-mech math.MP physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el physics.atom-ph quant-ph\n",
            "q-bio.QM eess.IV\n",
            "cond-mat.mtrl-sci quant-ph\n",
            "cond-mat.str-el physics.ins-det quant-ph\n",
            "q-fin.MF q-fin.GN\n",
            "hep-ph hep-th nucl-th\n",
            "physics.optics physics.comp-ph\n",
            "quant-ph cs.CC math.CO\n",
            "cs.SI math.OC physics.soc-ph\n",
            "math-ph cond-mat.str-el hep-th math.MP nlin.SI\n",
            "cs.ET cond-mat.mtrl-sci\n",
            "cond-mat.dis-nn cond-mat.str-el quant-ph\n",
            "cond-mat.str-el math-ph math.MP quant-ph\n",
            "hep-ph hep-ex stat.ML\n",
            "hep-th cond-mat.mes-hall cond-mat.quant-gas hep-ph\n",
            "physics.class-ph cs.ET\n",
            "cs.NI cs.LG\n",
            "physics.flu-dyn math.DS nlin.CD physics.ao-ph\n",
            "quant-ph cond-mat.other math-ph math.MP nlin.PS\n",
            "quant-ph cond-mat.stat-mech physics.hist-ph\n",
            "math.AG math.CO math.GT math.MG math.OC\n",
            "q-bio.NC cs.HC eess.SP\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas quant-ph\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.PE q-bio.QM\n",
            "math-ph hep-th math.MP math.RT\n",
            "physics.app-ph physics.bio-ph physics.chem-ph physics.med-ph\n",
            "physics.flu-dyn nlin.CD physics.app-ph physics.geo-ph\n",
            "eess.SP cs.LG\n",
            "cs.AI cs.NE math.OC\n",
            "hep-th cond-mat.str-el gr-qc quant-ph\n",
            "quant-ph cond-mat.mes-hall cond-mat.str-el\n",
            "math.CA math.FA math.OA\n",
            "cs.ET cond-mat.dis-nn cond-mat.mes-hall\n",
            "cond-mat.mtrl-sci hep-lat math-ph math.MP\n",
            "physics.atom-ph hep-ex nucl-ex\n",
            "astro-ph.HE physics.plasm-ph\n",
            "physics.class-ph physics.comp-ph physics.optics\n",
            "physics.optics cond-mat.mes-hall cond-mat.soft\n",
            "cond-mat.supr-con cond-mat.mtrl-sci\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other cond-mat.supr-con quant-ph\n",
            "quant-ph cond-mat.dis-nn cond-mat.mes-hall cond-mat.quant-gas\n",
            "nlin.PS cond-mat.mtrl-sci math-ph math.MP\n",
            "cond-mat.dis-nn cond-mat.str-el\n",
            "cs.MS cs.DS cs.SE physics.comp-ph\n",
            "eess.IV cs.CV cs.LG stat.ML\n",
            "physics.flu-dyn physics.ao-ph\n",
            "math.GT math.GR\n",
            "nlin.PS physics.bio-ph q-bio.CB\n",
            "stat.ML cond-mat.str-el cs.LG quant-ph\n",
            "physics.comp-ph physics.atom-ph physics.chem-ph\n",
            "cond-mat.stat-mech cond-mat.str-el hep-th quant-ph\n",
            "nucl-th hep-th\n",
            "cs.DB cs.CR\n",
            "math-ph math.MP math.SP\n",
            "cs.HC cs.CV\n",
            "quant-ph cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
            "q-fin.GN physics.soc-ph q-fin.RM\n",
            "physics.flu-dyn cond-mat.str-el hep-th\n",
            "physics.soc-ph cond-mat.stat-mech cs.SI math.DS nlin.AO\n",
            "nlin.CD math.DS physics.comp-ph\n",
            "physics.comp-ph astro-ph.CO astro-ph.IM\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.str-el\n",
            "cs.NI cs.CR cs.DC\n",
            "cond-mat.stat-mech physics.geo-ph\n",
            "math.GR math.AT math.CO math.GT\n",
            "quant-ph cs.SY eess.SY\n",
            "stat.CO cs.LG math.OC\n",
            "q-bio.MN cond-mat.stat-mech physics.bio-ph\n",
            "nlin.AO cond-mat.dis-nn nlin.CD\n",
            "math.PR cond-mat.stat-mech\n",
            "cond-mat.str-el math-ph math.MP math.OA math.QA\n",
            "astro-ph.CO astro-ph.GA gr-qc hep-th\n",
            "cond-mat.quant-gas cond-mat.supr-con nucl-th\n",
            "math.QA cond-mat.str-el hep-th math.GT\n",
            "cond-mat.quant-gas nlin.PS physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el physics.optics quant-ph\n",
            "hep-ph nucl-th physics.atom-ph\n",
            "cs.SI physics.soc-ph stat.ML\n",
            "cond-mat.str-el cond-mat.quant-gas cond-mat.supr-con physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall physics.chem-ph quant-ph\n",
            "math.AP math-ph math.DG math.FA math.MP\n",
            "quant-ph cs.DS cs.LG\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech nlin.AO\n",
            "nucl-th astro-ph.HE hep-ph hep-th\n",
            "nlin.AO nlin.CD\n",
            "cond-mat.quant-gas nlin.PS quant-ph\n",
            "physics.app-ph cond-mat.mes-hall quant-ph\n",
            "quant-ph gr-qc physics.ins-det\n",
            "physics.soc-ph physics.data-an\n",
            "physics.bio-ph q-bio.MN\n",
            "nucl-th astro-ph.HE gr-qc\n",
            "hep-th cond-mat.mes-hall gr-qc quant-ph\n",
            "physics.app-ph cond-mat.mtrl-sci physics.ins-det\n",
            "cs.CR cs.LG cs.NI stat.OT\n",
            "math.AP gr-qc\n",
            "cs.HC cs.CY cs.LG\n",
            "cs.DL nucl-ex nucl-th physics.comp-ph physics.data-an\n",
            "hep-th hep-lat hep-ph\n",
            "physics.plasm-ph nlin.CD\n",
            "physics.med-ph physics.app-ph\n",
            "cs.CV cs.RO\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.comp-ph\n",
            "hep-lat cond-mat.str-el hep-th quant-ph\n",
            "physics.flu-dyn astro-ph.SR nlin.CD physics.ao-ph\n",
            "gr-qc astro-ph.IM hep-ex hep-ph physics.atom-ph\n",
            "physics.optics nlin.PS physics.app-ph\n",
            "math.NA cs.CC cs.CV cs.NA\n",
            "astro-ph.HE astro-ph.CO gr-qc\n",
            "hep-lat cond-mat.stat-mech hep-ph nucl-th\n",
            "cs.LG math.CO stat.ML\n",
            "cond-mat.str-el cond-mat.dis-nn hep-th\n",
            "hep-th astro-ph.GA\n",
            "astro-ph.GA gr-qc\n",
            "cs.SI eess.SP\n",
            "physics.atom-ph physics.atm-clus quant-ph\n",
            "math.NA cs.NA math.CV\n",
            "physics.atom-ph cond-mat.dis-nn\n",
            "math.DS cs.NA math.NA\n",
            "q-bio.TO cond-mat.stat-mech\n",
            "physics.ins-det physics.optics\n",
            "math.OC cs.LG cs.SY eess.SY\n",
            "cond-mat.other physics.data-an\n",
            "nucl-th astro-ph.HE astro-ph.SR hep-ph\n",
            "math-ph gr-qc hep-th math.MP\n",
            "cond-mat.quant-gas nlin.CD\n",
            "quant-ph eess.IV physics.optics\n",
            "quant-ph cond-mat.other gr-qc\n",
            "nucl-th hep-ex nucl-ex\n",
            "nucl-th hep-ex hep-ph nucl-ex\n",
            "quant-ph hep-ph\n",
            "math.NA cs.NA physics.comp-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.ins-det quant-ph\n",
            "physics.soc-ph cs.CY\n",
            "physics.app-ph cond-mat.mes-hall cs.ET\n",
            "cs.SD cs.LG eess.AS\n",
            "cond-mat.str-el hep-th math-ph math.MP quant-ph\n",
            "cond-mat.soft physics.geo-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el\n",
            "physics.soc-ph econ.GN q-fin.EC\n",
            "cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con\n",
            "eess.SP cs.CC\n",
            "eess.SP cs.LG cs.NE\n",
            "quant-ph cs.NA math-ph math.MP math.NA\n",
            "astro-ph.HE astro-ph.IM\n",
            "nlin.PS physics.flu-dyn\n",
            "astro-ph.IM astro-ph.SR gr-qc nucl-th\n",
            "quant-ph hep-lat hep-ph nucl-th physics.atom-ph\n",
            "astro-ph.HE gr-qc hep-ph hep-th nucl-th\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.mes-hall\n",
            "astro-ph.IM physics.atom-ph\n",
            "math-ph cond-mat.stat-mech math.MP math.PR math.SP\n",
            "physics.app-ph physics.optics quant-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall physics.app-ph physics.optics quant-ph\n",
            "cs.HC cs.CR cs.CY\n",
            "nucl-ex hep-ex hep-ph\n",
            "physics.comp-ph cs.LG cs.SY eess.SY math.DS stat.ML\n",
            "cs.MM cs.IR\n",
            "physics.comp-ph math.DS stat.ML\n",
            "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI stat.ML\n",
            "physics.flu-dyn physics.comp-ph physics.plasm-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.stat-mech\n",
            "cs.SI math.CO physics.data-an physics.soc-ph stat.ML\n",
            "math.CO math-ph math.MP\n",
            "hep-lat hep-ph nucl-th\n",
            "astro-ph.CO astro-ph.IM\n",
            "cond-mat.quant-gas nucl-th physics.atm-clus\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.str-el hep-th quant-ph\n",
            "cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech\n",
            "cs.CV cs.LG eess.IV stat.ML\n",
            "nucl-th astro-ph.HE astro-ph.SR\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
            "quant-ph cs.LG hep-ph\n",
            "math.DS math-ph math.MP nlin.SI physics.class-ph\n",
            "cs.SI physics.data-an physics.soc-ph\n",
            "cond-mat.supr-con physics.app-ph physics.comp-ph\n",
            "math-ph math.AP math.FA math.MP math.PR\n",
            "nucl-th astro-ph.HE gr-qc nucl-ex\n",
            "gr-qc astro-ph.GA hep-th\n",
            "cond-mat.supr-con hep-th quant-ph\n",
            "physics.ed-ph astro-ph.CO\n",
            "cond-mat.mes-hall physics.comp-ph\n",
            "math.RA math.MG\n",
            "cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
            "math.OC cs.CC quant-ph\n",
            "nlin.PS math.DS physics.optics\n",
            "cond-mat.dis-nn math-ph math.MP\n",
            "cond-mat.str-el cond-mat.quant-gas hep-th\n",
            "cond-mat.soft cond-mat.mes-hall\n",
            "nucl-th hep-lat hep-ph nucl-ex\n",
            "astro-ph.IM physics.ao-ph physics.space-ph\n",
            "physics.optics physics.app-ph physics.ins-det\n",
            "cond-mat.stat-mech physics.optics quant-ph\n",
            "math.AG math.CO math.RT\n",
            "cs.LG cs.GT math.OC stat.ML\n",
            "stat.AP stat.CO stat.ME\n",
            "eess.IV cs.LG\n",
            "cond-mat.quant-gas physics.atom-ph physics.comp-ph\n",
            "hep-ph physics.plasm-ph\n",
            "cs.CC math.CO\n",
            "gr-qc astro-ph.CO astro-ph.HE\n",
            "cond-mat.stat-mech physics.flu-dyn\n",
            "math.AG cs.SC\n",
            "hep-ph hep-th quant-ph\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft\n",
            "q-bio.NC cs.AI cs.LG\n",
            "math.CV math.FA\n",
            "physics.optics physics.ins-det quant-ph\n",
            "cond-mat.supr-con astro-ph.HE nucl-th\n",
            "physics.flu-dyn cond-mat.stat-mech nlin.CD\n",
            "nucl-th hep-ph nucl-ex quant-ph\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.str-el quant-ph\n",
            "cs.AI cs.LG\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.app-ph\n",
            "q-bio.SC cond-mat.soft physics.bio-ph\n",
            "eess.IV physics.app-ph physics.med-ph\n",
            "physics.atom-ph physics.app-ph\n",
            "math.ST cs.IT math.IT stat.OT stat.TH\n",
            "eess.IV cs.CV q-bio.QM\n",
            "cs.CR cs.LG\n",
            "hep-ph physics.plasm-ph quant-ph\n",
            "cond-mat.stat-mech gr-qc hep-th quant-ph\n",
            "cs.IR cs.CL cs.LG\n",
            "math.ST cs.LO math.CT math.PR stat.TH\n",
            "cs.NE q-bio.PE\n",
            "astro-ph.IM astro-ph.GA astro-ph.HE gr-qc\n",
            "hep-th hep-ph nucl-th quant-ph\n",
            "cond-mat.stat-mech cond-mat.dis-nn cs.SI physics.soc-ph q-bio.PE\n",
            "physics.ins-det physics.app-ph\n",
            "hep-th math-ph math.CO math.MP\n",
            "physics.plasm-ph cond-mat.soft\n",
            "cond-mat.stat-mech nlin.SI\n",
            "math.NA cs.NA physics.flu-dyn\n",
            "physics.atom-ph cond-mat.quant-gas physics.chem-ph quant-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas physics.comp-ph\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech nlin.CD physics.comp-ph\n",
            "physics.plasm-ph cond-mat.dis-nn physics.chem-ph\n",
            "cs.CG math.CO\n",
            "physics.class-ph gr-qc hep-th\n",
            "nucl-th cond-mat.stat-mech\n",
            "physics.ins-det nucl-ex\n",
            "physics.ins-det physics.ed-ph physics.optics\n",
            "math.OC math.AP math.FA\n",
            "eess.SY cs.SY eess.SP physics.ins-det\n",
            "cond-mat.str-el cond-mat.mes-hall hep-th\n",
            "cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "stat.ML cs.CR cs.DC cs.LG eess.SP\n",
            "cond-mat.mes-hall physics.app-ph physics.optics quant-ph\n",
            "physics.plasm-ph physics.flu-dyn\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "astro-ph.CO cond-mat.stat-mech\n",
            "cond-mat.soft cond-mat.other physics.class-ph physics.geo-ph\n",
            "astro-ph.IM astro-ph.HE hep-ex physics.ins-det\n",
            "astro-ph.HE astro-ph.CO hep-ph\n",
            "quant-ph cond-mat.stat-mech math.DS nlin.CD\n",
            "physics.chem-ph cond-mat.str-el nucl-th physics.atom-ph physics.comp-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
            "math.DS math-ph math.AP math.MP\n",
            "math.PR math-ph math.CA math.MP\n",
            "physics.flu-dyn cs.LG cs.SY eess.SY math.OC\n",
            "nlin.PS cond-mat.stat-mech\n",
            "cs.DM math.CO math.PR\n",
            "cond-mat.quant-gas cond-mat.supr-con physics.atom-ph\n",
            "q-bio.BM cond-mat.soft\n",
            "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con hep-th\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech\n",
            "hep-th math.NT\n",
            "physics.comp-ph cond-mat.dis-nn quant-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.atm-clus\n",
            "physics.chem-ph cond-mat.str-el physics.comp-ph\n",
            "quant-ph math-ph math.MP math.SP\n",
            "q-fin.MF econ.TH math.OC q-fin.PM\n",
            "hep-ex hep-ph nucl-ex\n",
            "physics.chem-ph physics.atm-clus\n",
            "physics.flu-dyn eess.IV physics.med-ph\n",
            "cs.LG cs.ET math.AG quant-ph stat.ML\n",
            "math.NA cs.NA math.DS math.OC\n",
            "astro-ph.HE gr-qc hep-ph nucl-th\n",
            "hep-ph astro-ph.HE gr-qc hep-lat nucl-th\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "hep-lat hep-ph nucl-th quant-ph\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph\n",
            "cond-mat.mes-hall cond-mat.str-el physics.optics quant-ph\n",
            "nlin.PS cond-mat.other\n",
            "hep-lat hep-ex hep-ph nucl-th\n",
            "cond-mat.other cond-mat.supr-con\n",
            "cs.CE physics.flu-dyn\n",
            "cond-mat.str-el hep-th physics.comp-ph\n",
            "astro-ph.CO astro-ph.SR\n",
            "quant-ph cond-mat.other cs.DS\n",
            "quant-ph cs.DC\n",
            "quant-ph cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech math-ph math.MP\n",
            "nlin.AO physics.class-ph\n",
            "hep-ph hep-ex hep-lat hep-th\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
            "quant-ph cond-mat.quant-gas math-ph math.MP\n",
            "math.GT math.CO\n",
            "math.RA math-ph math.MP\n",
            "physics.flu-dyn math.AP\n",
            "cond-mat.quant-gas physics.atom-ph physics.chem-ph\n",
            "cs.CV cs.CL\n",
            "physics.soc-ph cs.DM\n",
            "cs.NE q-bio.QM\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.stat-mech\n",
            "cs.SI cs.CR physics.soc-ph\n",
            "physics.comp-ph cond-mat.stat-mech physics.chem-ph\n",
            "nlin.PS cond-mat.soft\n",
            "hep-lat nucl-th\n",
            "cond-mat.quant-gas cond-mat.str-el physics.atom-ph\n",
            "physics.optics physics.acc-ph quant-ph\n",
            "physics.optics cond-mat.other\n",
            "cs.CR cs.CL cs.LG stat.ML\n",
            "cs.CL cs.AI cs.LG\n",
            "quant-ph cs.CR physics.optics\n",
            "quant-ph cond-mat.dis-nn cond-mat.quant-gas\n",
            "cond-mat.dis-nn physics.bio-ph q-bio.BM q-bio.QM\n",
            "cond-mat.str-el cond-mat.quant-gas math-ph math.MP quant-ph\n",
            "q-fin.PR q-fin.MF\n",
            "astro-ph.HE astro-ph.IM astro-ph.SR\n",
            "math.CT math.GN math.LO math.RA\n",
            "nlin.CD cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
            "quant-ph cs.IT hep-th math-ph math.IT math.MP\n",
            "nlin.CD physics.optics\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.QM\n",
            "cond-mat.quant-gas physics.atom-ph physics.optics quant-ph\n",
            "math.OC cs.CV\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
            "cs.DC cs.LG\n",
            "physics.plasm-ph physics.space-ph\n",
            "eess.SP cs.LG physics.optics quant-ph\n",
            "hep-ex physics.geo-ph\n",
            "quant-ph physics.ins-det\n",
            "quant-ph cond-mat.other physics.app-ph\n",
            "physics.chem-ph quant-ph\n",
            "physics.chem-ph cs.LG physics.comp-ph\n",
            "physics.optics eess.SP\n",
            "cond-mat.str-el physics.chem-ph physics.comp-ph\n",
            "nlin.CD quant-ph\n",
            "cond-mat.supr-con physics.comp-ph\n",
            "quant-ph cond-mat.stat-mech cs.IT math.IT\n",
            "q-bio.NC cs.ET\n",
            "cond-mat.mtrl-sci cond-mat.supr-con physics.comp-ph\n",
            "cs.RO cs.HC\n",
            "hep-ph hep-ex physics.data-an\n",
            "astro-ph.IM gr-qc physics.optics\n",
            "quant-ph cond-mat.mes-hall nlin.CD\n",
            "physics.flu-dyn astro-ph.SR physics.ao-ph physics.geo-ph\n",
            "eess.SY cs.SD cs.SY eess.AS eess.SP\n",
            "math-ph math.MP math.RT\n",
            "cond-mat.mes-hall cond-mat.other physics.app-ph\n",
            "math.NA cs.NA econ.GN math.OC q-fin.EC\n",
            "eess.SP cs.SI nlin.CD\n",
            "cs.LG cs.NE stat.ML\n",
            "cond-mat.stat-mech math-ph math.MP nlin.SI\n",
            "cond-mat.stat-mech cond-mat.dis-nn physics.comp-ph\n",
            "math.AT math.GR math.GT\n",
            "cond-mat.mes-hall physics.app-ph physics.optics\n",
            "math-ph cs.NA math.CA math.MP math.NA\n",
            "astro-ph.HE gr-qc nucl-th\n",
            "math.NA cs.CE cs.NA math.AP\n",
            "cond-mat.soft cond-mat.supr-con\n",
            "quant-ph cond-mat.mes-hall physics.atom-ph physics.chem-ph\n",
            "math.DG math-ph math.GT math.MP math.QA\n",
            "hep-ph gr-qc\n",
            "stat.ML cs.LG physics.data-an\n",
            "cs.LG cs.CR cs.MA stat.ML\n",
            "cs.NI cs.MA\n",
            "cond-mat.mes-hall cond-mat.stat-mech cond-mat.supr-con\n",
            "math.NA astro-ph.CO astro-ph.EP astro-ph.GA cs.NA physics.comp-ph\n",
            "cs.LG cs.AI\n",
            "cond-mat.stat-mech cond-mat.str-el physics.comp-ph\n",
            "physics.optics cond-mat.mes-hall cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall physics.med-ph quant-ph\n",
            "physics.chem-ph physics.comp-ph quant-ph\n",
            "physics.flu-dyn math-ph math.AP math.MP\n",
            "math.AP physics.comp-ph\n",
            "nlin.PS quant-ph\n",
            "math.FA math.CV\n",
            "q-bio.BM cs.LG stat.ML\n",
            "cond-mat.quant-gas hep-ph quant-ph\n",
            "cs.LG cs.NA math.NA\n",
            "q-bio.QM cs.CV cs.LG eess.IV stat.ML\n",
            "cond-mat.mes-hall cond-mat.stat-mech hep-th\n",
            "cs.LG q-fin.CP stat.ML\n",
            "gr-qc astro-ph.CO math-ph math.MP\n",
            "cs.LG cs.DC math.OC stat.ML\n",
            "cs.LG math.DS physics.data-an stat.ML\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con\n",
            "cs.RO cs.CV eess.IV\n",
            "cond-mat.mes-hall astro-ph.IM cond-mat.mtrl-sci cond-mat.supr-con quant-ph\n",
            "quant-ph cond-mat.quant-gas physics.optics\n",
            "math-ph cond-mat.mes-hall math.CO math.MP math.PR math.SP\n",
            "cs.RO cs.NA math.NA\n",
            "cs.GR cs.HC\n",
            "astro-ph.HE astro-ph.GA gr-qc\n",
            "nlin.AO math.DS math.GN q-bio.PE\n",
            "cs.CL cs.LG\n",
            "cond-mat.supr-con cond-mat.str-el hep-th\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
            "math.ST stat.AP stat.ME stat.ML stat.TH\n",
            "physics.acc-ph physics.data-an\n",
            "gr-qc astro-ph.IM stat.ML\n",
            "astro-ph.SR astro-ph.GA\n",
            "nlin.CD cond-mat.stat-mech math.CO math.OC\n",
            "physics.bio-ph nlin.AO\n",
            "hep-ph cond-mat.quant-gas hep-lat\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.soft\n",
            "quant-ph cond-mat.quant-gas nlin.CD\n",
            "physics.comp-ph cs.NA math.NA\n",
            "hep-th math-ph math.DG math.MP\n",
            "nucl-th cond-mat.mes-hall\n",
            "physics.ins-det cond-mat.mtrl-sci quant-ph\n",
            "physics.chem-ph cond-mat.soft\n",
            "cond-mat.soft physics.comp-ph\n",
            "math-ph math.CA math.CV math.MP\n",
            "cond-mat.mtrl-sci math.AG\n",
            "math.ST q-fin.ST stat.TH\n",
            "cond-mat.stat-mech cs.IT math.IT nlin.AO q-bio.SC\n",
            "cs.MA cs.NE cs.RO nlin.AO q-bio.NC\n",
            "cond-mat.soft physics.chem-ph\n",
            "q-bio.NC cs.CL quant-ph\n",
            "hep-ph astro-ph.CO astro-ph.HE astro-ph.SR\n",
            "math.ST stat.OT stat.TH\n",
            "eess.IV cs.NA math.NA\n",
            "eess.SY cs.SY eess.SP\n",
            "math-ph cond-mat.stat-mech math.MP\n",
            "cond-mat.quant-gas nlin.SI quant-ph\n",
            "cond-mat.str-el cond-mat.quant-gas hep-lat hep-th\n",
            "cond-mat.stat-mech hep-th nlin.CD quant-ph\n",
            "cond-mat.str-el physics.chem-ph quant-ph\n",
            "nucl-th cond-mat.str-el hep-ph\n",
            "stat.ML cs.LG cs.SI physics.data-an q-bio.MN\n",
            "cond-mat.stat-mech physics.atom-ph\n",
            "cs.RO cs.DS\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci hep-th\n",
            "cond-mat.soft physics.class-ph physics.geo-ph\n",
            "math.GR math.RA math.RT\n",
            "physics.plasm-ph astro-ph.SR cond-mat.mes-hall physics.flu-dyn\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas\n",
            "math-ph gr-qc math.MP\n",
            "hep-th astro-ph.CO hep-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn\n",
            "quant-ph nlin.AO physics.data-an\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft physics.bio-ph physics.soc-ph\n",
            "cs.MA cs.SY eess.SY\n",
            "gr-qc physics.comp-ph\n",
            "physics.comp-ph cond-mat.stat-mech physics.flu-dyn\n",
            "cond-mat.supr-con cond-mat.stat-mech\n",
            "stat.ME q-bio.QM stat.AP\n",
            "cond-mat.str-el physics.chem-ph\n",
            "math-ph cond-mat.other math.MP\n",
            "cs.CV cs.RO math.OC\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.quant-gas physics.atom-ph\n",
            "cond-mat.stat-mech physics.atom-ph quant-ph\n",
            "math-ph cs.IT math.FA math.IT math.MP math.SP quant-ph\n",
            "physics.soc-ph cond-mat.stat-mech nlin.AO\n",
            "gr-qc hep-th physics.class-ph\n",
            "astro-ph.EP astro-ph.SR\n",
            "physics.soc-ph q-bio.PE q-bio.QM\n",
            "astro-ph.HE gr-qc hep-th\n",
            "physics.soc-ph cond-mat.stat-mech cs.SI\n",
            "cond-mat.str-el gr-qc hep-ph\n",
            "physics.optics cond-mat.mes-hall physics.ins-det quant-ph\n",
            "eess.SP physics.optics\n",
            "cond-mat.quant-gas cond-mat.mes-hall physics.atom-ph quant-ph\n",
            "physics.flu-dyn physics.app-ph physics.plasm-ph\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.chem-ph q-bio.SC\n",
            "hep-ph hep-ex physics.atom-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn quant-ph\n",
            "cond-mat.dis-nn cond-mat.mes-hall\n",
            "quant-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph\n",
            "math-ph hep-th math.DG math.MP\n",
            "math.SG math-ph math.DG math.MP math.RT\n",
            "physics.class-ph nlin.CD\n",
            "math.CO math.RA\n",
            "math.PR q-bio.QM q-bio.SC\n",
            "math.NA cs.NA cs.RO\n",
            "physics.app-ph cond-mat.mes-hall physics.plasm-ph\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.supr-con quant-ph\n",
            "gr-qc astro-ph.CO hep-th physics.data-an\n",
            "cs.RO cs.LG cs.MA cs.NE\n",
            "cs.DC cs.LG hep-ex\n",
            "cond-mat.mtrl-sci hep-ph\n",
            "math.DG hep-th math.AG\n",
            "cond-mat.dis-nn cond-mat.stat-mech physics.soc-ph\n",
            "cs.RO cs.AI cs.LG\n",
            "astro-ph.SR astro-ph.HE astro-ph.IM hep-ex\n",
            "physics.chem-ph physics.atom-ph physics.optics\n",
            "math.DS math.AG math.SG\n",
            "math-ph math.MP math.OA math.SP quant-ph\n",
            "cs.LG cs.CV cs.NE math.OC\n",
            "q-bio.MN cond-mat.stat-mech\n",
            "hep-ph astro-ph.CO astro-ph.EP\n",
            "quant-ph physics.app-ph physics.optics stat.AP\n",
            "math.OC math.DS\n",
            "eess.IV cs.CV cs.LG q-bio.QM q-bio.TO\n",
            "astro-ph.IM astro-ph.GA cs.CV\n",
            "physics.atom-ph cond-mat.quant-gas physics.ins-det\n",
            "physics.bio-ph eess.IV\n",
            "cond-mat.str-el cond-mat.other quant-ph\n",
            "physics.comp-ph physics.atom-ph\n",
            "cond-mat.str-el physics.atom-ph\n",
            "nucl-th cond-mat.mes-hall hep-ph\n",
            "quant-ph cond-mat.mtrl-sci\n",
            "cs.NE cs.LG physics.chem-ph physics.comp-ph\n",
            "hep-ph astro-ph.EP hep-ex\n",
            "cond-mat.dis-nn cs.LG quant-ph\n",
            "physics.optics astro-ph.IM\n",
            "physics.comp-ph cond-mat.mes-hall physics.optics\n",
            "cs.RO cs.LG\n",
            "cs.PL cs.CG\n",
            "math-ph math.CV math.MP math.PR\n",
            "hep-th cond-mat.mes-hall cond-mat.str-el nucl-th\n",
            "q-bio.GN stat.AP stat.ME\n",
            "eess.IV eess.SP\n",
            "math.SP\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el hep-lat hep-th\n",
            "physics.app-ph physics.plasm-ph\n",
            "physics.atom-ph cond-mat.dis-nn quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.optics quant-ph\n",
            "hep-ph hep-lat hep-th nucl-th\n",
            "cond-mat.str-el cond-mat.mtrl-sci physics.chem-ph\n",
            "cond-mat.soft physics.chem-ph physics.optics\n",
            "math-ph hep-th math.GR math.MP math.OA\n",
            "hep-ph astro-ph.HE gr-qc hep-th\n",
            "gr-qc math.NT\n",
            "physics.med-ph eess.IV\n",
            "stat.CO stat.AP\n",
            "astro-ph.CO astro-ph.GA hep-ph\n",
            "physics.geo-ph nlin.PS\n",
            "nlin.AO nlin.PS\n",
            "cond-mat.supr-con physics.app-ph\n",
            "physics.app-ph cond-mat.dis-nn cond-mat.mtrl-sci physics.atom-ph physics.optics\n",
            "econ.TH nlin.AO\n",
            "cs.LO cs.LG cs.PL\n",
            "cond-mat.dis-nn cond-mat.mes-hall physics.optics\n",
            "hep-ph hep-ex hep-lat nucl-th\n",
            "astro-ph.CO astro-ph.IM gr-qc\n",
            "physics.atm-clus physics.chem-ph\n",
            "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.str-el quant-ph\n",
            "astro-ph.CO astro-ph.HE astro-ph.IM hep-ph physics.data-an\n",
            "cond-mat.stat-mech cond-mat.dis-nn hep-th quant-ph\n",
            "cond-mat.stat-mech cs.LG physics.comp-ph stat.ML\n",
            "gr-qc hep-th math.DG\n",
            "physics.bio-ph cond-mat.stat-mech\n",
            "physics.comp-ph cond-mat.str-el physics.chem-ph\n",
            "math.GR math.NT math.RT\n",
            "eess.IV cs.CV physics.med-ph\n",
            "physics.flu-dyn cond-mat.other\n",
            "cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT\n",
            "physics.plasm-ph hep-ph nlin.PS\n",
            "nlin.PS physics.class-ph\n",
            "hep-ph astro-ph.CO gr-qc hep-th physics.atom-ph\n",
            "physics.ao-ph cond-mat.stat-mech nlin.CD physics.flu-dyn physics.geo-ph\n",
            "math.OC cs.SY eess.SY physics.space-ph\n",
            "math.NA cs.NA math.AP math.FA\n",
            "gr-qc astro-ph.GA astro-ph.HE\n",
            "eess.SP cs.LG eess.IV stat.ML\n",
            "math.SP physics.chem-ph\n",
            "nlin.AO q-bio.PE\n",
            "cond-mat.soft cond-mat.stat-mech physics.comp-ph physics.flu-dyn\n",
            "hep-th gr-qc math-ph math.MP quant-ph\n",
            "cond-mat.soft cond-mat.other\n",
            "physics.soc-ph nlin.AO physics.data-an\n",
            "math.GR math.DS\n",
            "hep-ph astro-ph.CO astro-ph.SR hep-ex\n",
            "hep-th cond-mat.stat-mech math.GT math.QA math.RT\n",
            "cond-mat.str-el cond-mat.soft cond-mat.stat-mech hep-th\n",
            "astro-ph.HE astro-ph.GA\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el\n",
            "physics.soc-ph cond-mat.stat-mech cs.SI math.CO nlin.AO\n",
            "cs.NE cs.LG physics.geo-ph\n",
            "cond-mat.mes-hall physics.plasm-ph\n",
            "cs.IT math-ph math.IT math.MP math.ST stat.TH\n",
            "nlin.PS physics.optics\n",
            "cond-mat.quant-gas hep-lat nucl-th\n",
            "q-bio.PE q-bio.MN\n",
            "cond-mat.stat-mech hep-lat quant-ph\n",
            "math-ph math.MP physics.class-ph\n",
            "math.PR math.ST stat.CO stat.TH\n",
            "math.AT math.AC\n",
            "hep-ph astro-ph.HE gr-qc\n",
            "quant-ph cond-mat.stat-mech math-ph math.MP nlin.CD\n",
            "cond-mat.stat-mech astro-ph.GA\n",
            "physics.flu-dyn physics.bio-ph\n",
            "cs.NI cs.AI cs.LG\n",
            "cs.CC cs.CG cs.DM math.CO physics.soc-ph\n",
            "math-ph hep-th math.CA math.MP\n",
            "hep-ph astro-ph.HE hep-lat nucl-th\n",
            "cs.SI physics.data-an stat.AP\n",
            "physics.flu-dyn math.OC stat.ML\n",
            "cs.MS cs.DC cs.PF\n",
            "hep-ph cond-mat.mtrl-sci hep-ex\n",
            "stat.CO cs.NA math.NA\n",
            "quant-ph physics.atom-ph physics.chem-ph physics.optics\n",
            "quant-ph cond-mat.quant-gas physics.flu-dyn\n",
            "cond-mat.str-el cond-mat.quant-gas physics.atom-ph\n",
            "cond-mat.stat-mech cond-mat.str-el math-ph math.MP quant-ph\n",
            "stat.AP cs.LG econ.EM\n",
            "cond-mat.soft physics.bio-ph physics.comp-ph\n",
            "hep-th cond-mat.str-el math-ph math.MP nlin.SI\n",
            "physics.space-ph astro-ph.EP astro-ph.SR physics.geo-ph physics.plasm-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.chem-ph\n",
            "gr-qc cond-mat.stat-mech math-ph math.MP\n",
            "cond-mat.mes-hall physics.comp-ph quant-ph\n",
            "physics.soc-ph physics.geo-ph\n",
            "cs.RO cs.LG stat.ML\n",
            "physics.hist-ph physics.space-ph\n",
            "hep-ph hep-ex hep-th\n",
            "cond-mat.mes-hall cond-mat.str-el math-ph math.MP physics.optics quant-ph\n",
            "quant-ph hep-ph hep-th\n",
            "math.NT math.AT math.CT math.GR math.KT\n",
            "cs.CL cs.AI\n",
            "hep-ph hep-ex hep-th nucl-th\n",
            "math.OC cs.LG stat.ML\n",
            "cs.PF cs.SY eess.SY physics.space-ph\n",
            "cs.IT cs.LG math.IT\n",
            "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.soft cond-mat.str-el physics.ins-det\n",
            "math.AP math-ph math.DG math.MP math.SP\n",
            "stat.ML cs.LG math.DS\n",
            "math-ph cond-mat.quant-gas cond-mat.stat-mech math.MP\n",
            "physics.comp-ph cond-mat.dis-nn physics.chem-ph\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
            "cs.DS cs.SI physics.soc-ph\n",
            "cond-mat.soft physics.class-ph\n",
            "physics.chem-ph cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cs.IT math-ph math.IT math.MP quant-ph\n",
            "physics.bio-ph physics.optics\n",
            "quant-ph math-ph math.MP physics.comp-ph\n",
            "nlin.CD astro-ph.HE\n",
            "hep-th math-ph math.MP nlin.PS\n",
            "nucl-th physics.comp-ph\n",
            "quant-ph stat.AP\n",
            "astro-ph.GA gr-qc hep-ph physics.hist-ph\n",
            "cs.GR cs.AI cs.HC\n",
            "cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "physics.soc-ph cond-mat.stat-mech cs.GT\n",
            "cs.LG math.ST stat.ML stat.TH\n",
            "cond-mat.soft physics.bio-ph physics.chem-ph\n",
            "nlin.CD physics.data-an\n",
            "cs.CV cs.GR\n",
            "cs.CY eess.SP\n",
            "hep-ph astro-ph.CO gr-qc hep-th\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.supr-con hep-th quant-ph\n",
            "physics.atom-ph physics.plasm-ph\n",
            "math-ph cond-mat.stat-mech math.MP quant-ph\n",
            "hep-ph astro-ph.SR nucl-th\n",
            "cond-mat.supr-con physics.ins-det\n",
            "hep-th cond-mat.soft cond-mat.str-el\n",
            "math.NT math.AG math.KT\n",
            "cs.LG cs.NE cs.SE stat.ML\n",
            "physics.optics eess.IV\n",
            "astro-ph.GA cond-mat.stat-mech gr-qc\n",
            "physics.bio-ph cond-mat.dis-nn\n",
            "cs.CG cond-mat.mtrl-sci\n",
            "math.OC cs.DC cs.LG eess.SP stat.ML\n",
            "cs.HC cs.LG\n",
            "hep-th cond-mat.stat-mech math-ph math.MP math.QA\n",
            "stat.ML cs.LG cs.NA math.MG math.NA\n",
            "math.ST stat.CO stat.TH\n",
            "physics.comp-ph cs.CE cs.NA math.NA\n",
            "physics.app-ph cond-mat.soft physics.flu-dyn\n",
            "astro-ph.SR astro-ph.GA physics.flu-dyn\n",
            "cond-mat.stat-mech cs.CC physics.soc-ph\n",
            "cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
            "cs.DS cs.DB cs.IR cs.LG\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci\n",
            "astro-ph.SR astro-ph.EP astro-ph.GA\n",
            "hep-ph astro-ph.CO astro-ph.GA\n",
            "cond-mat.stat-mech hep-ph\n",
            "physics.ins-det physics.data-an\n",
            "cond-mat.quant-gas cond-mat.mtrl-sci\n",
            "cond-mat.stat-mech cs.GT physics.data-an\n",
            "cs.HC\n",
            "physics.data-an physics.soc-ph stat.CO\n",
            "cs.CV math.OC\n",
            "math-ph cond-mat.stat-mech math.CO math.MP\n",
            "q-bio.PE math.DS physics.soc-ph\n",
            "nlin.PS math-ph math.MP physics.optics\n",
            "math.CA math.PR\n",
            "math.NA cs.LG cs.NA cs.NE stat.ML\n",
            "cond-mat.soft cond-mat.mtrl-sci nlin.PS physics.flu-dyn\n",
            "cond-mat.mes-hall physics.atom-ph\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech cs.CG\n",
            "math-ph cond-mat.stat-mech hep-lat math.MP\n",
            "nlin.AO quant-ph\n",
            "math.OC math.DG math.DS\n",
            "physics.class-ph cond-mat.mes-hall math-ph math.MP\n",
            "math.RA math.GR math.GT math.QA math.RT\n",
            "cond-mat.stat-mech cond-mat.str-el hep-lat\n",
            "math-ph cond-mat.dis-nn hep-th math.MP nlin.CD quant-ph\n",
            "physics.app-ph cond-mat.mtrl-sci physics.comp-ph physics.data-an physics.ins-det\n",
            "astro-ph.IM hep-ph quant-ph\n",
            "physics.chem-ph cond-mat.stat-mech\n",
            "hep-ph physics.atom-ph\n",
            "quant-ph cond-mat.quant-gas hep-lat nucl-th\n",
            "hep-lat astro-ph.CO hep-ph hep-th\n",
            "astro-ph.GA astro-ph.IM\n",
            "cond-mat.mtrl-sci nlin.PS physics.comp-ph\n",
            "cond-mat.stat-mech physics.chem-ph\n",
            "physics.flu-dyn math-ph math.MP nlin.PS nlin.SI\n",
            "cs.IT eess.SP math.IT math.OC\n",
            "gr-qc hep-th nlin.CD quant-ph\n",
            "quant-ph cond-mat.str-el hep-lat\n",
            "physics.ins-det cond-mat.mes-hall\n",
            "astro-ph.IM physics.class-ph\n",
            "math.OC cs.LG eess.SP\n",
            "physics.comp-ph physics.data-an\n",
            "quant-ph cond-mat.other cond-mat.quant-gas\n",
            "q-bio.BM cond-mat.soft physics.bio-ph\n",
            "physics.acc-ph physics.plasm-ph\n",
            "physics.acc-ph nlin.SI\n",
            "hep-ex astro-ph.CO physics.ins-det\n",
            "astro-ph.GA hep-ex\n",
            "physics.geo-ph physics.ao-ph physics.space-ph\n",
            "astro-ph.SR astro-ph.EP astro-ph.IM\n",
            "eess.IV cs.CV cs.LG eess.SP physics.med-ph\n",
            "q-fin.ST cs.AI cs.CE cs.LG\n",
            "astro-ph.IM astro-ph.EP physics.ins-det\n",
            "cs.LG cs.AI cs.HC stat.ML\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP\n",
            "physics.app-ph cond-mat.mes-hall physics.flu-dyn\n",
            "cs.NE cs.CV cs.LG q-bio.NC\n",
            "math.OC cs.LG cs.NA math.NA stat.ML\n",
            "cond-mat.stat-mech cond-mat.quant-gas\n",
            "hep-th cond-mat.dis-nn math-ph math.MP quant-ph\n",
            "cond-mat.str-el cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech\n",
            "quant-ph math-ph math.AG math.MP\n",
            "hep-lat cond-mat.str-el hep-th math.DG\n",
            "cs.RO cs.AI cs.GT\n",
            "cond-mat.supr-con cond-mat.str-el physics.optics\n",
            "astro-ph.SR physics.ao-ph physics.flu-dyn\n",
            "hep-th nlin.PS\n",
            "cs.DC q-bio.QM\n",
            "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML\n",
            "eess.SP cs.IT cs.LG math.IT\n",
            "cs.CV eess.IV stat.AP\n",
            "cond-mat.quant-gas hep-th quant-ph\n",
            "cond-mat.other physics.optics\n",
            "cond-mat.mes-hall nlin.CD\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el hep-ph\n",
            "astro-ph.CO nlin.PS physics.plasm-ph\n",
            "cond-mat.str-el physics.comp-ph quant-ph\n",
            "astro-ph.SR physics.plasm-ph\n",
            "gr-qc astro-ph.HE physics.comp-ph\n",
            "astro-ph.CO astro-ph.GA gr-qc hep-ph hep-th\n",
            "stat.CO cs.NE\n",
            "nlin.CD physics.class-ph\n",
            "cond-mat.soft physics.comp-ph stat.AP\n",
            "cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP\n",
            "cond-mat.soft physics.bio-ph physics.flu-dyn\n",
            "math.SP math-ph math.MP quant-ph\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con quant-ph\n",
            "hep-ph cond-mat.mtrl-sci\n",
            "cond-mat.supr-con cond-mat.stat-mech cond-mat.str-el\n",
            "eess.SY cs.MA cs.SY\n",
            "hep-th cond-mat.stat-mech gr-qc quant-ph\n",
            "hep-lat physics.comp-ph\n",
            "nucl-th cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
            "physics.gen-ph hep-ph\n",
            "cs.DS math.ST physics.data-an stat.ML stat.TH\n",
            "quant-ph cond-mat.stat-mech stat.ML\n",
            "astro-ph.EP astro-ph.GA\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cond-mat.mtrl-sci cs.LG physics.comp-ph\n",
            "hep-ph nucl-ex\n",
            "math.NA cs.LG cs.NA\n",
            "hep-lat hep-ex hep-ph\n",
            "eess.AS cs.LG cs.SD\n",
            "cs.CR cs.MM\n",
            "quant-ph cond-mat.other physics.atm-clus physics.hist-ph physics.optics\n",
            "stat.ME physics.soc-ph q-bio.PE stat.CO\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
            "quant-ph cond-mat.other physics.atom-ph\n",
            "quant-ph physics.plasm-ph\n",
            "astro-ph.HE hep-ex hep-ph nucl-th\n",
            "hep-th hep-ph math-ph math.MP\n",
            "quant-ph hep-ex\n",
            "cs.LG cs.AI quant-ph stat.ML\n",
            "q-bio.GN\n",
            "physics.plasm-ph math-ph math.MP\n",
            "physics.geo-ph physics.ao-ph physics.bio-ph\n",
            "math.PR q-bio.QM\n",
            "quant-ph cond-mat.mes-hall gr-qc hep-th math-ph math.MP\n",
            "physics.data-an nlin.CD\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.mtrl-sci physics.class-ph\n",
            "physics.ed-ph physics.ins-det\n",
            "nucl-th nucl-ex stat.ML\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.soft physics.class-ph\n",
            "quant-ph cond-mat.mes-hall nlin.AO\n",
            "eess.IV cond-mat.mtrl-sci\n",
            "cond-mat.soft cs.LG physics.chem-ph\n",
            "cond-mat.stat-mech math-ph math.MP nlin.CD quant-ph\n",
            "math.CO math.AT\n",
            "astro-ph.HE astro-ph.IM hep-ex\n",
            "cond-mat.stat-mech hep-th nlin.SI\n",
            "quant-ph cond-mat.str-el physics.chem-ph\n",
            "stat.CO cs.SY eess.SY\n",
            "math-ph cond-mat.soft math.MP\n",
            "hep-ex hep-ph physics.ins-det\n",
            "nucl-th hep-ph hep-th nucl-ex\n",
            "cond-mat.stat-mech physics.class-ph quant-ph\n",
            "cond-mat.str-el hep-lat math-ph math.MP\n",
            "cond-mat.stat-mech cs.LG stat.ML\n",
            "cs.CR cs.DC cs.LG\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.MN\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el hep-lat hep-ph\n",
            "cs.DM math.ST stat.TH\n",
            "physics.atom-ph cond-mat.quant-gas physics.chem-ph\n",
            "hep-th cond-mat.str-el math.AT\n",
            "cond-mat.stat-mech physics.bio-ph q-bio.BM\n",
            "q-bio.TO physics.bio-ph\n",
            "cond-mat.quant-gas physics.atm-clus physics.atom-ph physics.optics quant-ph\n",
            "cs.RO cs.AI cs.CV cs.LG\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.flu-dyn physics.plasm-ph\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech hep-th math-ph math.MP\n",
            "nucl-ex hep-ph\n",
            "math.SP math.AP\n",
            "cs.LG cs.CV cs.IT math.IT stat.ML\n",
            "cond-mat.str-el hep-ph hep-th\n",
            "hep-th gr-qc physics.flu-dyn\n",
            "cond-mat.str-el cond-mat.other cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall cond-mat.str-el\n",
            "quant-ph cond-mat.quant-gas physics.atom-ph physics.chem-ph\n",
            "cond-mat.stat-mech cs.SI physics.soc-ph stat.ML\n",
            "cond-mat.quant-gas nucl-th quant-ph\n",
            "cond-mat.stat-mech physics.gen-ph\n",
            "quant-ph cond-mat.mtrl-sci cond-mat.str-el\n",
            "physics.app-ph cond-mat.other cs.NA math.NA physics.comp-ph\n",
            "gr-qc nlin.CD\n",
            "astro-ph.CO physics.atom-ph\n",
            "astro-ph.IM eess.IV stat.ME stat.ML\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el\n",
            "astro-ph.GA astro-ph.CO astro-ph.SR\n",
            "math.KT math.AT\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.flu-dyn q-bio.TO\n",
            "physics.ins-det hep-ex hep-ph nucl-ex\n",
            "physics.chem-ph cond-mat.stat-mech cs.CE math.PR\n",
            "hep-ph cond-mat.str-el hep-th nucl-th\n",
            "hep-ph hep-ex hep-lat nucl-ex nucl-th\n",
            "q-bio.QM cs.LG stat.ML\n",
            "physics.plasm-ph cond-mat.mes-hall physics.flu-dyn\n",
            "cs.LG eess.IV physics.med-ph stat.ML\n",
            "cs.DC cs.DB cs.LG stat.ML\n",
            "stat.ML cs.AI cs.LG cs.MA physics.data-an\n",
            "cond-mat.str-el hep-th math.QA quant-ph\n",
            "math.DS cs.CG cs.RO math.AG\n",
            "cond-mat.supr-con cond-mat.other physics.flu-dyn\n",
            "astro-ph.CO astro-ph.GA astro-ph.HE gr-qc\n",
            "physics.soc-ph cond-mat.stat-mech cs.MA\n",
            "gr-qc hep-th nucl-th physics.flu-dyn\n",
            "cs.FL math.CO\n",
            "cond-mat.stat-mech cond-mat.mes-hall math-ph math.MP\n",
            "cs.LG cs.DS math.OC math.PR stat.ML\n",
            "astro-ph.HE hep-th\n",
            "q-bio.PE cs.SI math.CO\n",
            "cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
            "cond-mat.str-el physics.app-ph\n",
            "math.OC cs.MS\n",
            "hep-ph cond-mat.supr-con hep-th math-ph math.MP quant-ph\n",
            "cs.DL physics.soc-ph\n",
            "physics.ed-ph quant-ph\n",
            "q-bio.PE cond-mat.stat-mech physics.bio-ph\n",
            "cs.CL cs.IR\n",
            "physics.space-ph astro-ph.EP astro-ph.SR\n",
            "q-bio.CB cond-mat.stat-mech math.PR\n",
            "cs.CE cs.LG stat.ML\n",
            "cs.IT cs.SI math.IT physics.data-an q-bio.NC\n",
            "nlin.PS cond-mat.quant-gas\n",
            "physics.comp-ph cs.DC cs.PF\n",
            "physics.app-ph physics.data-an\n",
            "gr-qc astro-ph.IM physics.optics\n",
            "cs.CC cs.CR\n",
            "cs.CR cs.DC cs.NI cs.SI cs.SY eess.SY\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft cond-mat.supr-con\n",
            "math.DS physics.comp-ph physics.flu-dyn\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph physics.comp-ph\n",
            "hep-th cond-mat.str-el quant-ph\n",
            "physics.optics cs.SY eess.SY physics.class-ph quant-ph\n",
            "cond-mat.mtrl-sci physics.ins-det\n",
            "hep-th astro-ph.HE gr-qc math-ph math.MP\n",
            "physics.optics cond-mat.mes-hall physics.class-ph\n",
            "physics.atom-ph gr-qc\n",
            "math.CA math.OC\n",
            "cs.LG eess.SP math.ST stat.ML stat.TH\n",
            "cond-mat.soft physics.plasm-ph\n",
            "math-ph cond-mat.stat-mech math.CO math.DS math.MP\n",
            "cond-mat.quant-gas cond-mat.supr-con quant-ph\n",
            "cs.CE cs.DC cs.PF q-bio.GN\n",
            "physics.data-an physics.med-ph q-bio.QM\n",
            "physics.app-ph physics.comp-ph\n",
            "eess.IV cs.CV cs.LG physics.med-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.other\n",
            "quant-ph cond-mat.stat-mech physics.bio-ph\n",
            "hep-ph hep-th math.CO\n",
            "hep-ph cond-mat.quant-gas cond-mat.stat-mech\n",
            "physics.chem-ph cond-mat.mes-hall physics.comp-ph\n",
            "cs.PL cs.LO\n",
            "physics.hist-ph physics.class-ph\n",
            "physics.comp-ph cond-mat.stat-mech quant-ph\n",
            "cond-mat.supr-con cond-mat.mtrl-sci physics.app-ph\n",
            "cond-mat.mes-hall cond-mat.other gr-qc hep-th\n",
            "nucl-ex nlin.AO physics.atom-ph physics.hist-ph physics.pop-ph\n",
            "nlin.AO math.OC physics.soc-ph\n",
            "math-ph math.MP physics.chem-ph\n",
            "math.SP math.FA\n",
            "quant-ph physics.bio-ph physics.chem-ph\n",
            "gr-qc physics.optics quant-ph\n",
            "physics.ins-det astro-ph.IM hep-ph\n",
            "cs.DL\n",
            "nucl-th physics.comp-ph physics.data-an\n",
            "cond-mat.soft astro-ph.HE cond-mat.mtrl-sci nucl-th\n",
            "physics.optics cond-mat.mes-hall cond-mat.quant-gas nlin.PS\n",
            "cs.GT physics.soc-ph\n",
            "math.OC math.DS nlin.CD\n",
            "cond-mat.stat-mech nlin.AO q-bio.NC\n",
            "gr-qc astro-ph.GA astro-ph.HE hep-th\n",
            "cs.GT cs.CC cs.DS\n",
            "cs.LO cs.CR cs.FL\n",
            "hep-ph gr-qc hep-ex\n",
            "nucl-th hep-lat hep-ph\n",
            "astro-ph.CO astro-ph.GA gr-qc\n",
            "astro-ph.CO astro-ph.IM physics.ins-det\n",
            "cond-mat.str-el cond-mat.stat-mech math-ph math.MP quant-ph\n",
            "cs.CR cs.CV cs.LG\n",
            "cs.NI cs.DC\n",
            "quant-ph physics.app-ph physics.atom-ph physics.optics\n",
            "physics.app-ph physics.flu-dyn\n",
            "quant-ph gr-qc physics.optics\n",
            "gr-qc astro-ph.SR\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.optics\n",
            "astro-ph.EP astro-ph.IM\n",
            "q-bio.BM cs.LG eess.IV stat.ML\n",
            "nlin.CD math.DS physics.chem-ph\n",
            "cs.SD cs.GR cs.MM eess.AS\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.stat-mech\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el nucl-th\n",
            "quant-ph hep-lat stat.ML\n",
            "hep-th math.DG math.SG\n",
            "nucl-th astro-ph.SR\n",
            "physics.bio-ph cond-mat.soft q-bio.TO\n",
            "physics.atom-ph physics.app-ph quant-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci hep-th\n",
            "cs.CV cs.CR cs.LG\n",
            "quant-ph cs.CR math.FA math.PR\n",
            "physics.chem-ph cond-mat.other\n",
            "gr-qc astro-ph.HE nucl-th\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci\n",
            "cs.PL cs.DC\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci\n",
            "nucl-th astro-ph.CO\n",
            "nucl-th cond-mat.supr-con hep-ph\n",
            "hep-lat cond-mat.mes-hall cs.LG hep-th\n",
            "stat.ML cond-mat.dis-nn cs.LG cs.NE q-bio.NC\n",
            "astro-ph.CO hep-ex hep-ph nucl-ex physics.ins-det\n",
            "cond-mat.dis-nn math.DS nlin.AO nlin.CD nlin.PS\n",
            "physics.optics physics.atom-ph\n",
            "math.AP math-ph math.MP physics.flu-dyn\n",
            "physics.plasm-ph astro-ph.SR physics.space-ph\n",
            "quant-ph cs.CC cs.CR\n",
            "eess.IV cs.AI cs.CV cs.LG\n",
            "cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "cond-mat.supr-con cond-mat.quant-gas cond-mat.str-el\n",
            "cs.DC cs.LG math.OC\n",
            "physics.app-ph physics.ins-det physics.optics\n",
            "cs.DS cs.DB\n",
            "hep-lat math-ph math.MP\n",
            "math.ST q-fin.MF stat.AP stat.TH\n",
            "hep-ph hep-ex quant-ph\n",
            "hep-th math.GT math.QA\n",
            "physics.optics physics.data-an\n",
            "cond-mat.stat-mech hep-th math-ph math.MP nlin.PS\n",
            "astro-ph.IM cs.LG\n",
            "cond-mat.soft physics.bio-ph q-bio.BM\n",
            "cond-mat.soft physics.bio-ph q-bio.CB\n",
            "astro-ph.EP physics.ao-ph physics.geo-ph\n",
            "cs.SD cs.LG eess.AS stat.ML\n",
            "q-fin.ST cs.CE econ.EM\n",
            "physics.comp-ph cond-mat.soft cond-mat.stat-mech physics.bio-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn nlin.CD quant-ph\n",
            "cs.LG cs.AI cs.LO stat.ML\n",
            "cs.CV cs.LG eess.IV\n",
            "gr-qc astro-ph.IM cs.LG physics.data-an physics.ins-det\n",
            "physics.ins-det physics.class-ph physics.flu-dyn\n",
            "hep-ph hep-ex physics.data-an stat.ML\n",
            "cond-mat.str-el cond-mat.mes-hall quant-ph\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci nlin.CD nucl-th\n",
            "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech\n",
            "cs.DS cs.DC cs.LG\n",
            "physics.ao-ph nlin.PS physics.flu-dyn physics.geo-ph\n",
            "cs.CY physics.soc-ph\n",
            "cs.CR cs.CY\n",
            "cond-mat.stat-mech cond-mat.quant-gas hep-th quant-ph\n",
            "cond-mat.mes-hall cond-mat.stat-mech physics.optics quant-ph\n",
            "stat.ME eess.SP math.DS stat.AP stat.CO\n",
            "math.HO math.AC\n",
            "physics.atom-ph astro-ph.HE astro-ph.IM astro-ph.SR physics.plasm-ph\n",
            "astro-ph.CO astro-ph.GA physics.comp-ph\n",
            "math.DS nlin.AO\n",
            "cond-mat.stat-mech cs.LG q-bio.QM\n",
            "astro-ph.SR astro-ph.HE hep-ph\n",
            "eess.IV cs.CV stat.AP\n",
            "cs.CV cs.LG cs.SD eess.AS eess.IV\n",
            "cond-mat.stat-mech cond-mat.str-el hep-lat hep-th\n",
            "physics.chem-ph cond-mat.soft physics.app-ph physics.comp-ph\n",
            "hep-ex physics.data-an\n",
            "cs.LG cs.SE\n",
            "astro-ph.GA cs.CV\n",
            "cs.LG cs.PL stat.ML\n",
            "q-bio.MN q-bio.BM\n",
            "cs.LG cs.CR cs.CV\n",
            "hep-th astro-ph.CO gr-qc quant-ph\n",
            "hep-th math-ph math.MP nlin.SI\n",
            "cond-mat.dis-nn cond-mat.mes-hall cond-mat.quant-gas\n",
            "physics.app-ph physics.class-ph\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.other\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.other physics.atom-ph physics.chem-ph\n",
            "cond-mat.soft math.CO\n",
            "cs.CL cs.AI cs.IR\n",
            "astro-ph.CO astro-ph.GA astro-ph.IM\n",
            "math.AG math-ph math.DG math.MP\n",
            "cond-mat.dis-nn hep-lat quant-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.stat-mech hep-th\n",
            "quant-ph hep-lat\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.str-el quant-ph\n",
            "cs.ET cond-mat.mes-hall physics.app-ph\n",
            "quant-ph nlin.CG\n",
            "cs.CL cs.CV cs.LG\n",
            "cs.IT cs.CR math.IT\n",
            "hep-th cond-mat.str-el hep-lat\n",
            "cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph\n",
            "physics.optics physics.atm-clus physics.atom-ph\n",
            "physics.flu-dyn math.OC\n",
            "cond-mat.soft physics.ao-ph physics.flu-dyn physics.geo-ph\n",
            "stat.AP cs.AI math.OC\n",
            "cond-mat.stat-mech hep-lat\n",
            "gr-qc astro-ph.HE hep-ph hep-th\n",
            "nlin.PS physics.ao-ph physics.flu-dyn\n",
            "gr-qc physics.hist-ph\n",
            "nlin.SI nlin.PS physics.flu-dyn\n",
            "cs.LG cs.SD eess.AS stat.ML\n",
            "cs.CV cs.GR cs.LG eess.IV\n",
            "astro-ph.GA astro-ph.CO gr-qc\n",
            "cs.SI cs.CY physics.soc-ph\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech physics.class-ph\n",
            "cs.CC quant-ph\n",
            "physics.chem-ph cs.LG\n",
            "cond-mat.mtrl-sci astro-ph.IM gr-qc physics.ins-det\n",
            "physics.optics cond-mat.str-el\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph q-bio.OT\n",
            "quant-ph q-fin.RM\n",
            "nucl-th cond-mat.str-el\n",
            "quant-ph physics.acc-ph physics.optics\n",
            "eess.SP cs.IT math.IT nlin.SI\n",
            "physics.flu-dyn cond-mat.stat-mech math-ph math.MP nlin.CD\n",
            "cs.SC cs.LG\n",
            "cond-mat.dis-nn stat.ML\n",
            "math-ph cond-mat.mes-hall math.MP physics.optics quant-ph\n",
            "astro-ph.EP astro-ph.IM astro-ph.SR\n",
            "astro-ph.EP physics.space-ph\n",
            "gr-qc hep-lat hep-th quant-ph\n",
            "physics.flu-dyn cs.CE\n",
            "physics.plasm-ph physics.chem-ph\n",
            "cs.IT cs.DM math.CO math.IT\n",
            "physics.flu-dyn cond-mat.soft physics.geo-ph\n",
            "nlin.SI nlin.PS\n",
            "physics.atom-ph physics.chem-ph quant-ph\n",
            "physics.geo-ph physics.ao-ph physics.data-an\n",
            "physics.bio-ph astro-ph.SR cond-mat.stat-mech\n",
            "physics.soc-ph cs.GT\n",
            "hep-ph cs.LG\n",
            "math-ph math.CA math.MP\n",
            "math.FA quant-ph\n",
            "physics.geo-ph physics.app-ph\n",
            "cs.IR cs.AI cs.CL\n",
            "math.ST math.FA stat.ML stat.TH\n",
            "cond-mat.mes-hall hep-th quant-ph\n",
            "cs.MA nlin.AO\n",
            "cond-mat.supr-con cond-mat.other\n",
            "cs.NI cs.IT eess.SP math.IT\n",
            "cond-mat.mes-hall cond-mat.quant-gas hep-th\n",
            "astro-ph.EP astro-ph.HE hep-ph nucl-th\n",
            "cond-mat.soft q-bio.TO\n",
            "hep-ph astro-ph.CO astro-ph.HE gr-qc hep-th\n",
            "astro-ph.EP physics.ao-ph\n",
            "astro-ph.CO hep-ph nucl-ex\n",
            "cs.LG cs.CV cs.NE stat.ML\n",
            "cs.HC cs.AI cs.LG\n",
            "physics.med-ph physics.app-ph physics.bio-ph\n",
            "physics.bio-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
            "cond-mat.mes-hall math-ph math.MP\n",
            "hep-ph hep-ex physics.optics\n",
            "hep-th cond-mat.soft math-ph math.MP\n",
            "physics.flu-dyn eess.IV\n",
            "cond-mat.supr-con cond-mat.mtrl-sci quant-ph\n",
            "cs.CV cs.CY\n",
            "math.ST cond-mat.dis-nn cs.LG eess.SP stat.ML stat.TH\n",
            "q-bio.CB cond-mat.stat-mech physics.bio-ph\n",
            "cs.DB cs.AI\n",
            "stat.ML cond-mat.dis-nn cs.LG math.PR\n",
            "q-bio.QM eess.SP\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.str-el\n",
            "cs.AI cs.MA\n",
            "physics.atom-ph cond-mat.mes-hall\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.comp-ph\n",
            "physics.app-ph physics.class-ph physics.optics\n",
            "physics.pop-ph astro-ph.EP astro-ph.IM astro-ph.SR\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.flu-dyn\n",
            "cond-mat.stat-mech cond-mat.mes-hall nlin.PS quant-ph\n",
            "math.CT math.PR\n",
            "q-fin.TR cs.CE cs.MA\n",
            "hep-ph astro-ph.HE hep-th\n",
            "physics.chem-ph cond-mat.mes-hall quant-ph\n",
            "physics.comp-ph cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "physics.ins-det nucl-ex nucl-th\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.other physics.atm-clus physics.atom-ph\n",
            "quant-ph cs.AI cs.LG\n",
            "astro-ph.EP astro-ph.GA astro-ph.SR\n",
            "physics.optics nlin.AO quant-ph\n",
            "eess.IV physics.ins-det\n",
            "quant-ph hep-lat hep-ph hep-th nucl-th\n",
            "physics.atom-ph hep-ph nucl-th\n",
            "physics.data-an physics.ao-ph stat.ML\n",
            "cond-mat.stat-mech q-bio.PE\n",
            "nlin.SI nlin.PS physics.optics\n",
            "cond-mat.dis-nn cs.LG\n",
            "physics.app-ph cond-mat.mes-hall physics.acc-ph physics.plasm-ph\n",
            "hep-ph hep-ex physics.comp-ph\n",
            "cond-mat.soft cond-mat.other physics.class-ph\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft stat.ML\n",
            "physics.ins-det astro-ph.IM\n",
            "quant-ph cond-mat.dis-nn physics.comp-ph\n",
            "q-bio.SC\n",
            "nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.bio-ph physics.soc-ph\n",
            "hep-ph hep-ex nucl-th physics.comp-ph\n",
            "nlin.PS cond-mat.quant-gas physics.optics\n",
            "cs.AI cs.IR\n",
            "cond-mat.stat-mech cs.CC\n",
            "physics.atom-ph hep-ph\n",
            "stat.CO cs.CG cs.DC cs.DS cs.SE\n",
            "physics.atom-ph physics.ins-det quant-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech math.OC quant-ph\n",
            "physics.med-ph q-bio.TO\n",
            "physics.acc-ph physics.ins-det\n",
            "cs.HC cs.LG stat.AP stat.ML\n",
            "nucl-th cond-mat.stat-mech hep-lat nucl-ex\n",
            "hep-th math-ph math.MP math.QA math.RT\n",
            "physics.med-ph eess.IV physics.app-ph q-bio.QM\n",
            "astro-ph.HE hep-ex\n",
            "eess.IV physics.bio-ph physics.optics\n",
            "cs.IT eess.SP math.IT physics.app-ph\n",
            "physics.comp-ph physics.optics quant-ph\n",
            "eess.IV cond-mat.dis-nn physics.app-ph physics.optics\n",
            "cond-mat.mtrl-sci cond-mat.other nucl-ex\n",
            "nlin.SI cond-mat.supr-con\n",
            "cs.MA cs.AI cs.RO\n",
            "hep-th cond-mat.mtrl-sci\n",
            "cond-mat.quant-gas nlin.PS physics.optics\n",
            "cs.LG cond-mat.mtrl-sci\n",
            "nucl-ex hep-ex hep-ph nucl-th\n",
            "cond-mat.other gr-qc hep-ph\n",
            "hep-th cond-mat.mes-hall hep-ph\n",
            "nucl-th quant-ph\n",
            "cond-mat.stat-mech physics.comp-ph physics.data-an\n",
            "stat.ML astro-ph.CO cs.LG stat.CO stat.ME\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el physics.comp-ph\n",
            "cs.LG stat.CO stat.ML\n",
            "cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "cond-mat.quant-gas physics.atm-clus\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci cond-mat.soft\n",
            "nlin.AO cs.LG cs.NE stat.ML\n",
            "cond-mat.stat-mech nlin.PS physics.optics\n",
            "cond-mat.quant-gas cond-mat.stat-mech nlin.CD\n",
            "hep-th math-ph math.MP quant-ph\n",
            "q-bio.NC nlin.AO\n",
            "cond-mat.soft hep-th\n",
            "cs.LG cs.NI stat.ML\n",
            "math.DS cs.GT econ.TH\n",
            "cond-mat.stat-mech nlin.SI quant-ph\n",
            "physics.ed-ph cs.SI physics.soc-ph\n",
            "astro-ph.HE astro-ph.IM nucl-ex\n",
            "cond-mat.mes-hall cond-mat.quant-gas math-ph math.MP\n",
            "nucl-th hep-lat\n",
            "physics.optics cond-mat.quant-gas cs.LG nlin.PS\n",
            "eess.SP cs.LG stat.AP stat.ML\n",
            "quant-ph cs.LG physics.comp-ph\n",
            "quant-ph math-ph math.MP math.OA\n",
            "q-bio.QM cs.LG eess.IV\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.other nlin.CD quant-ph\n",
            "astro-ph.CO astro-ph.GA physics.flu-dyn\n",
            "hep-ph cond-mat.quant-gas\n",
            "cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con hep-th\n",
            "astro-ph.IM astro-ph.HE gr-qc\n",
            "cs.IT cs.ET math.IT\n",
            "nucl-th hep-ex hep-lat hep-ph\n",
            "cs.NI eess.SP\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el math-ph math.MP quant-ph\n",
            "hep-th cond-mat.mes-hall gr-qc hep-ph\n",
            "physics.ins-det physics.acc-ph\n",
            "nlin.CD nlin.SI\n",
            "physics.optics cond-mat.mtrl-sci physics.app-ph physics.ins-det\n",
            "cs.ET cs.NE physics.optics\n",
            "physics.ao-ph cond-mat.soft physics.chem-ph\n",
            "physics.optics physics.comp-ph physics.plasm-ph\n",
            "cs.CY cs.CL cs.DL cs.LG\n",
            "cond-mat.stat-mech cs.LG cs.NE\n",
            "physics.comp-ph cond-mat.str-el physics.atm-clus physics.chem-ph quant-ph\n",
            "astro-ph.GA physics.chem-ph\n",
            "astro-ph.SR astro-ph.HE nucl-ex nucl-th\n",
            "astro-ph.SR astro-ph.HE physics.atom-ph\n",
            "cond-mat.mes-hall physics.ins-det\n",
            "cond-mat.supr-con cond-mat.mes-hall physics.app-ph\n",
            "nlin.CD physics.comp-ph quant-ph\n",
            "math.NA cond-mat.mtrl-sci cs.NA\n",
            "hep-th hep-lat\n",
            "math-ph math.CV math.MP\n",
            "physics.class-ph cs.CR\n",
            "physics.atom-ph physics.comp-ph quant-ph\n",
            "math.DG hep-th\n",
            "q-fin.TR cs.LG stat.ML\n",
            "hep-ph astro-ph.CO astro-ph.HE hep-ex\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el hep-lat quant-ph\n",
            "hep-th cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con hep-ph\n",
            "physics.comp-ph cond-mat.dis-nn cond-mat.mtrl-sci\n",
            "cond-mat.quant-gas hep-th\n",
            "cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech physics.flu-dyn physics.optics\n",
            "physics.atom-ph physics.comp-ph physics.optics\n",
            "q-fin.RM stat.AP\n",
            "cs.CR quant-ph\n",
            "physics.chem-ph physics.bio-ph\n",
            "hep-ph physics.comp-ph\n",
            "cond-mat.mes-hall nlin.PS\n",
            "physics.ins-det physics.acc-ph physics.atom-ph physics.geo-ph\n",
            "cond-mat.supr-con cond-mat.quant-gas quant-ph\n",
            "cond-mat.stat-mech nlin.CG\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.supr-con\n",
            "math.DS math.FA\n",
            "hep-th cond-mat.dis-nn cond-mat.str-el\n",
            "cs.NE cs.NA math.NA\n",
            "cs.LO cs.PL\n",
            "cond-mat.dis-nn cond-mat.str-el physics.comp-ph\n",
            "quant-ph cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con physics.ins-det\n",
            "cs.RO cs.LG cs.SY eess.SY\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph physics.comp-ph\n",
            "nlin.PS cond-mat.soft physics.flu-dyn\n",
            "q-bio.QM cond-mat.stat-mech physics.data-an\n",
            "astro-ph.EP astro-ph.CO astro-ph.GA astro-ph.SR\n",
            "physics.atom-ph hep-ph nucl-th physics.chem-ph\n",
            "q-bio.GN cond-mat.stat-mech q-bio.QM\n",
            "cond-mat.mtrl-sci physics.ins-det physics.optics\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.app-ph physics.flu-dyn\n",
            "hep-th hep-lat physics.comp-ph\n",
            "physics.chem-ph cond-mat.mes-hall\n",
            "cs.HC cs.LG eess.SP\n",
            "cond-mat.stat-mech cond-mat.dis-nn hep-th nlin.CD\n",
            "physics.comp-ph astro-ph.IM physics.flu-dyn\n",
            "cs.DL cs.CY cs.LG\n",
            "cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP math.PR\n",
            "cond-mat.mes-hall cond-mat.dis-nn physics.optics\n",
            "cond-mat.stat-mech math-ph math.MP physics.optics\n",
            "astro-ph.GA physics.comp-ph physics.space-ph\n",
            "physics.flu-dyn cond-mat.soft cond-mat.stat-mech\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el hep-lat\n",
            "physics.class-ph physics.app-ph\n",
            "cond-mat.stat-mech physics.comp-ph physics.plasm-ph\n",
            "cond-mat.mes-hall hep-ph physics.atom-ph quant-ph\n",
            "cs.CR cs.CY cs.NI\n",
            "nucl-ex astro-ph.SR\n",
            "math-ph math.DG math.MP\n",
            "quant-ph cond-mat.stat-mech hep-th nlin.CD\n",
            "physics.comp-ph physics.chem-ph\n",
            "cond-mat.quant-gas cond-mat.other quant-ph\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech math-ph math.MP physics.atom-ph\n",
            "quant-ph physics.med-ph q-bio.QM\n",
            "nlin.AO cs.LG physics.bio-ph\n",
            "cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
            "cond-mat.supr-con hep-th math-ph math.MP\n",
            "hep-ph cs.LG physics.comp-ph\n",
            "hep-th cond-mat.stat-mech hep-lat\n",
            "nucl-th cond-mat.quant-gas\n",
            "cond-mat.other cond-mat.quant-gas\n",
            "physics.plasm-ph astro-ph.HE gr-qc\n",
            "physics.comp-ph physics.atm-clus physics.chem-ph\n",
            "hep-ph cond-mat.mes-hall cond-mat.str-el\n",
            "cond-mat.str-el cond-mat.stat-mech cond-mat.supr-con\n",
            "math.MG math.DG math.DS\n",
            "math.GN math.CT\n",
            "math.AP nlin.CD physics.flu-dyn\n",
            "q-bio.SC q-bio.NC\n",
            "cs.GR q-bio.NC\n",
            "math.NA cs.NA math.OC stat.ML\n",
            "econ.TH math.GN\n",
            "gr-qc astro-ph.CO hep-th math-ph math.MP\n",
            "cs.NI cs.DM math.CO\n",
            "cond-mat.soft math.DG\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.supr-con nlin.PS\n",
            "cs.CR cs.IT math.IT\n",
            "astro-ph.EP physics.comp-ph\n",
            "eess.SP cs.DC cs.LG\n",
            "hep-th astro-ph.GA gr-qc hep-ph\n",
            "physics.optics physics.class-ph\n",
            "hep-th cond-mat.stat-mech hep-ph nucl-th\n",
            "cond-mat.mes-hall hep-ph nucl-th\n",
            "hep-th cond-mat.str-el hep-ph math.AT\n",
            "hep-th cond-mat.supr-con hep-ph math.AT nucl-th\n",
            "math.HO cs.MM\n",
            "cond-mat.quant-gas cond-mat.stat-mech hep-lat hep-ph quant-ph\n",
            "quant-ph cond-mat.dis-nn cond-mat.str-el cs.CV\n",
            "hep-lat gr-qc\n",
            "hep-th nucl-th\n",
            "cond-mat.dis-nn cond-mat.mes-hall physics.optics quant-ph\n",
            "cs.CV cs.MM\n",
            "quant-ph physics.hist-ph physics.pop-ph\n",
            "nlin.PS q-bio.PE\n",
            "math.NA cs.NA q-bio.QM\n",
            "physics.comp-ph cond-mat.dis-nn\n",
            "math.CO cs.SC\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft\n",
            "hep-ph hep-lat quant-ph\n",
            "cs.CL cs.LG stat.ML\n",
            "eess.AS cs.SD\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.other\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci cs.IT math-ph math.IT math.MP\n",
            "cond-mat.dis-nn cond-mat.mes-hall nlin.CD physics.optics\n",
            "hep-ex physics.acc-ph physics.ins-det\n",
            "cs.SI cs.AI cs.CL cs.LG\n",
            "math-ph cond-mat.quant-gas math.CA math.MP physics.class-ph\n",
            "gr-qc math-ph math.DG math.MP\n",
            "cond-mat.soft nlin.AO\n",
            "math-ph math.DS math.MP physics.flu-dyn\n",
            "cond-mat.mtrl-sci cond-mat.str-el physics.app-ph\n",
            "cond-mat.stat-mech nlin.AO physics.bio-ph\n",
            "cond-mat.mes-hall cs.ET cs.SY eess.SY physics.comp-ph quant-ph\n",
            "eess.SP cs.CV\n",
            "hep-th hep-lat hep-ph nucl-th\n",
            "math.DG gr-qc math-ph math.MP physics.hist-ph\n",
            "stat.ML cs.LG physics.ao-ph\n",
            "cs.MS cond-mat.soft physics.chem-ph physics.comp-ph\n",
            "eess.SP eess.AS\n",
            "physics.data-an nucl-ex physics.ins-det\n",
            "cond-mat.mes-hall cond-mat.str-el physics.comp-ph\n",
            "cs.IR cs.LG stat.ML\n",
            "cs.SI cond-mat.soft cs.CG math.AT physics.soc-ph\n",
            "physics.atom-ph nucl-th\n",
            "cs.AI cs.LG cs.RO\n",
            "hep-th gr-qc hep-lat\n",
            "q-bio.PE q-bio.QM\n",
            "quant-ph cond-mat.stat-mech cs.IT math.IT nlin.CD\n",
            "physics.app-ph astro-ph.IM\n",
            "quant-ph cond-mat.mtrl-sci physics.atom-ph\n",
            "cond-mat.stat-mech cond-mat.quant-gas cond-mat.str-el physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall cond-mat.supr-con physics.ins-det quant-ph\n",
            "cond-mat.mtrl-sci physics.app-ph physics.ins-det physics.optics\n",
            "physics.bio-ph physics.ins-det q-bio.BM\n",
            "physics.plasm-ph astro-ph.HE\n",
            "cond-mat.mes-hall gr-qc quant-ph\n",
            "cond-mat.dis-nn cond-mat.mes-hall physics.data-an\n",
            "physics.bio-ph cond-mat.mes-hall q-bio.CB quant-ph\n",
            "astro-ph.SR astro-ph.GA astro-ph.IM\n",
            "cond-mat.dis-nn cond-mat.mes-hall cond-mat.soft physics.comp-ph physics.optics\n",
            "cond-mat.mtrl-sci physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall nlin.AO physics.optics\n",
            "q-bio.QM cond-mat.mes-hall physics.bio-ph quant-ph\n",
            "q-bio.NC nlin.AO physics.bio-ph\n",
            "physics.optics cond-mat.mes-hall cond-mat.mtrl-sci physics.atom-ph\n",
            "physics.space-ph astro-ph.SR\n",
            "cs.IR cs.DL\n",
            "hep-th cond-mat.dis-nn cond-mat.stat-mech hep-lat hep-ph\n",
            "gr-qc astro-ph.IM physics.data-an\n",
            "quant-ph physics.app-ph physics.atom-ph\n",
            "cond-mat.quant-gas gr-qc hep-th physics.flu-dyn\n",
            "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML\n",
            "quant-ph cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
            "astro-ph.HE astro-ph.GA astro-ph.SR\n",
            "quant-ph physics.atom-ph stat.ML\n",
            "physics.med-ph physics.atom-ph physics.ins-det\n",
            "physics.bio-ph cond-mat.soft physics.flu-dyn\n",
            "physics.geo-ph cs.NA math.NA\n",
            "astro-ph.GA astro-ph.HE astro-ph.SR\n",
            "q-bio.PE nlin.AO\n",
            "cond-mat.mes-hall physics.chem-ph physics.optics\n",
            "hep-ph astro-ph.HE nucl-ex nucl-th\n",
            "quant-ph cond-mat.other physics.chem-ph\n",
            "nlin.AO physics.soc-ph\n",
            "q-bio.CB physics.bio-ph\n",
            "math.GR math.CV math.DG\n",
            "cond-mat.str-el cond-mat.stat-mech physics.comp-ph\n",
            "cond-mat.stat-mech cond-mat.mes-hall math-ph math.MP math.PR\n",
            "physics.bio-ph q-bio.BM\n",
            "cs.LG physics.flu-dyn\n",
            "physics.optics cond-mat.mes-hall hep-th\n",
            "physics.ins-det cond-mat.soft physics.optics\n",
            "physics.app-ph cond-mat.other physics.chem-ph physics.flu-dyn\n",
            "cs.DS cs.DM cs.IT math.IT math.PR stat.CO\n",
            "astro-ph.SR physics.space-ph\n",
            "physics.atom-ph cond-mat.mes-hall physics.optics\n",
            "physics.soc-ph cs.LG\n",
            "cs.LG cs.AI cs.GT cs.MA stat.ML\n",
            "eess.SY cs.NI cs.SI cs.SY\n",
            "cs.LO cs.AI cs.CL math.GN math.LO\n",
            "math.CT math.AG math.RT\n",
            "physics.atom-ph nlin.PS\n",
            "cond-mat.str-el cond-mat.supr-con physics.chem-ph\n",
            "hep-th cond-mat.other math-ph math.MP\n",
            "astro-ph.GA astro-ph.CO astro-ph.HE physics.plasm-ph\n",
            "astro-ph.SR physics.plasm-ph physics.space-ph\n",
            "cs.CR cs.DC\n",
            "physics.flu-dyn physics.optics\n",
            "physics.acc-ph physics.ins-det physics.optics\n",
            "physics.data-an hep-ph\n",
            "cs.SI cs.CY nlin.AO\n",
            "hep-lat cond-mat.stat-mech quant-ph\n",
            "hep-th gr-qc nucl-th\n",
            "q-bio.QM stat.AP stat.ML\n",
            "q-bio.QM cs.CV eess.IV\n",
            "cond-mat.mes-hall physics.atm-clus physics.flu-dyn quant-ph\n",
            "physics.optics physics.atom-ph physics.ins-det\n",
            "cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph\n",
            "cond-mat.mes-hall cond-mat.quant-gas physics.optics quant-ph\n",
            "cond-mat.mes-hall cond-mat.str-el physics.chem-ph\n",
            "math.HO math-ph math.MP math.PR\n",
            "cs.PL cs.GT cs.IT cs.LO math.IT\n",
            "cond-mat.quant-gas nlin.CD physics.atom-ph\n",
            "nucl-th gr-qc\n",
            "physics.soc-ph cs.SI q-fin.TR\n",
            "quant-ph cs.DM math-ph math.CA math.MP\n",
            "hep-ph physics.ins-det\n",
            "physics.soc-ph stat.CO\n",
            "physics.plasm-ph physics.comp-ph physics.data-an\n",
            "physics.med-ph physics.optics\n",
            "math.OC math.CO q-bio.QM\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other\n",
            "physics.plasm-ph hep-th\n",
            "physics.comp-ph physics.ed-ph\n",
            "math-ph cond-mat.str-el math.MP\n",
            "physics.soc-ph cond-mat.dis-nn nlin.AO q-bio.MN\n",
            "quant-ph physics.class-ph\n",
            "hep-ex astro-ph.HE\n",
            "eess.IV cs.CV cs.LG physics.med-ph q-bio.QM\n",
            "math.GT gr-qc math-ph math.MP math.QA\n",
            "physics.gen-ph quant-ph\n",
            "physics.atom-ph physics.chem-ph physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.mtrl-sci quant-ph\n",
            "nlin.AO physics.comp-ph\n",
            "cs.CY cs.HC\n",
            "hep-ph astro-ph.CO hep-ex hep-th\n",
            "eess.SP physics.data-an physics.optics\n",
            "cs.RO cs.CV math.OC\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.quant-gas hep-th\n",
            "nlin.PS physics.plasm-ph\n",
            "cs.GT cs.IR\n",
            "physics.optics nlin.CD nlin.PS\n",
            "physics.flu-dyn astro-ph.SR\n",
            "physics.acc-ph cond-mat.other hep-th\n",
            "cs.NE cs.NA math.NA physics.comp-ph quant-ph\n",
            "cs.LG cs.RO cs.SY eess.SY stat.ML\n",
            "physics.soc-ph cond-mat.stat-mech cs.CY cs.SI\n",
            "gr-qc physics.class-ph\n",
            "nucl-th physics.optics\n",
            "physics.optics nlin.PS physics.comp-ph\n",
            "physics.chem-ph physics.atom-ph quant-ph\n",
            "quant-ph math.NT\n",
            "cs.CL cs.AI cs.IR cs.LG\n",
            "eess.SP cs.GT cs.SY eess.SY\n",
            "cs.MS cs.CE\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft\n",
            "astro-ph.SR astro-ph.EP astro-ph.GA astro-ph.IM\n",
            "cs.LG cs.CL stat.ML\n",
            "hep-ex quant-ph\n",
            "cond-mat.str-el cond-mat.stat-mech nlin.PS\n",
            "nlin.AO cs.LG physics.comp-ph stat.ML\n",
            "physics.data-an cond-mat.mtrl-sci cond-mat.str-el\n",
            "math.GT quant-ph\n",
            "physics.optics cs.ET\n",
            "cond-mat.soft math.DS nlin.PS\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el\n",
            "eess.SP q-bio.QM\n",
            "math.NA cs.LG cs.NA stat.CO\n",
            "quant-ph nucl-ex physics.ins-det\n",
            "nlin.AO cond-mat.soft cond-mat.stat-mech\n",
            "cond-mat.mes-hall cond-mat.other nlin.AO\n",
            "physics.comp-ph physics.class-ph\n",
            "eess.SY cs.SY physics.app-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.quant-gas\n",
            "quant-ph cs.SY eess.SY stat.ML\n",
            "physics.plasm-ph physics.acc-ph physics.optics\n",
            "math.CV math.DS\n",
            "gr-qc physics.geo-ph\n",
            "quant-ph physics.atom-ph physics.chem-ph\n",
            "math.DG math-ph math.MP math.SG\n",
            "quant-ph gr-qc physics.atom-ph\n",
            "astro-ph.SR astro-ph.GA physics.space-ph\n",
            "physics.atom-ph cond-mat.supr-con quant-ph\n",
            "math.PR math.ST stat.TH\n",
            "physics.med-ph cs.LG cs.NE stat.ML\n",
            "cs.LG q-bio.QM stat.AP stat.ML\n",
            "physics.soc-ph cs.DL\n",
            "physics.soc-ph physics.ao-ph\n",
            "physics.app-ph physics.acc-ph physics.plasm-ph\n",
            "math-ph cond-mat.str-el math.MP quant-ph\n",
            "physics.atom-ph physics.med-ph\n",
            "cs.CV cs.HC\n",
            "physics.flu-dyn cs.CE cs.LG\n",
            "cs.CY cs.SI q-bio.QM\n",
            "astro-ph.IM astro-ph.EP astro-ph.SR\n",
            "cs.MA stat.AP\n",
            "eess.SY cs.LG cs.SY\n",
            "math.HO math.CV\n",
            "quant-ph cs.DC physics.comp-ph\n",
            "physics.comp-ph cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "cs.CV cs.HC cs.LG eess.IV\n",
            "physics.ins-det cs.CV physics.app-ph\n",
            "physics.chem-ph cond-mat.soft cond-mat.stat-mech\n",
            "cond-mat.quant-gas cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cs.IT cs.NI cs.PF eess.SP math.IT math.OC\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el quant-ph\n",
            "physics.app-ph physics.ins-det\n",
            "math-ph math.CA math.MP math.SG nlin.SI physics.class-ph\n",
            "astro-ph.HE gr-qc hep-ph hep-th\n",
            "cs.LG cs.AI cs.HC cs.IR\n",
            "cs.SE cs.CL cs.SI\n",
            "quant-ph cond-mat.str-el math-ph math.MP\n",
            "cond-mat.stat-mech cond-mat.quant-gas math-ph math.MP\n",
            "eess.IV cs.CV cs.LG physics.med-ph stat.ML\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.chem-ph\n",
            "physics.soc-ph cs.LG cs.SI stat.ML\n",
            "physics.atom-ph astro-ph.HE\n",
            "cs.SC cs.CC cs.DS\n",
            "hep-ex nucl-ex physics.ins-det\n",
            "quant-ph cond-mat.str-el cond-mat.supr-con\n",
            "eess.IV physics.med-ph\n",
            "math.DS cs.CG cs.RO\n",
            "cs.HC stat.OT\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.quant-gas quant-ph\n",
            "cond-mat.stat-mech physics.bio-ph physics.data-an\n",
            "cond-mat.other cond-mat.stat-mech\n",
            "math.DG math-ph math.MP nlin.SI\n",
            "physics.chem-ph nlin.CD\n",
            "eess.AS cs.CL cs.LG cs.NE cs.SD\n",
            "math.DS math-ph math.MP physics.class-ph\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
            "cs.LG cs.DB cs.DC stat.ML\n",
            "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech\n",
            "physics.app-ph cond-mat.soft\n",
            "gr-qc astro-ph.CO hep-lat hep-th\n",
            "astro-ph.SR astro-ph.HE physics.space-ph\n",
            "hep-ph astro-ph.SR gr-qc\n",
            "physics.soc-ph math.AP nlin.PS q-bio.PE\n",
            "quant-ph cond-mat.other cs.LG physics.comp-ph\n",
            "cs.NI stat.AP\n",
            "math.DG math.RA\n",
            "cond-mat.mes-hall math-ph math.MP quant-ph\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci physics.comp-ph\n",
            "cond-mat.stat-mech cond-mat.soft physics.chem-ph\n",
            "cond-mat.stat-mech cond-mat.str-el nlin.SI\n",
            "physics.flu-dyn cond-mat.stat-mech\n",
            "astro-ph.IM physics.optics\n",
            "econ.GN cs.CY q-fin.EC stat.AP\n",
            "cs.CL cs.LG eess.AS\n",
            "math-ph hep-th math.MP math.SG nlin.SI physics.flu-dyn\n",
            "cs.LG cs.CV cs.IR stat.ML\n",
            "cond-mat.soft cond-mat.dis-nn physics.flu-dyn\n",
            "cond-mat.mes-hall cond-mat.str-el physics.atom-ph physics.chem-ph\n",
            "q-fin.ST cs.LG q-fin.PM stat.ML\n",
            "cond-mat.soft cond-mat.mes-hall cond-mat.mtrl-sci physics.flu-dyn\n",
            "physics.atm-clus cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "gr-qc physics.optics\n",
            "physics.atom-ph nucl-th quant-ph\n",
            "physics.comp-ph cond-mat.dis-nn physics.data-an quant-ph\n",
            "cs.HC cs.LG cs.RO\n",
            "cond-mat.other nlin.CD quant-ph\n",
            "cs.CR cs.IT cs.SY eess.SP eess.SY math.IT\n",
            "physics.comp-ph astro-ph.IM cs.LG hep-ex\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.optics\n",
            "nucl-th hep-ex\n",
            "nlin.CG math.DS\n",
            "cs.LG cond-mat.stat-mech physics.comp-ph stat.ML\n",
            "math.NA astro-ph.CO cs.NA physics.comp-ph\n",
            "eess.AS cs.CL cs.LG cs.SD\n",
            "astro-ph.CO astro-ph.GA astro-ph.IM gr-qc hep-th\n",
            "gr-qc physics.atom-ph quant-ph\n",
            "cond-mat.dis-nn cond-mat.str-el physics.comp-ph quant-ph\n",
            "cs.HC cs.CY cs.RO\n",
            "cond-mat.stat-mech cond-mat.soft nlin.CD physics.bio-ph\n",
            "astro-ph.IM astro-ph.EP astro-ph.HE physics.space-ph\n",
            "astro-ph.EP astro-ph.SR q-bio.PE\n",
            "cs.SC q-bio.MN\n",
            "physics.data-an cs.CV eess.IV hep-ex\n",
            "quant-ph cond-mat.quant-gas nlin.SI\n",
            "astro-ph.GA astro-ph.IM astro-ph.SR\n",
            "hep-ph cond-mat.mes-hall hep-th nucl-th\n",
            "cond-mat.dis-nn cs.LG eess.SP\n",
            "physics.app-ph cond-mat.supr-con quant-ph\n",
            "cs.LG cs.SI stat.ML\n",
            "physics.flu-dyn cond-mat.mtrl-sci cond-mat.soft\n",
            "cond-mat.soft physics.bio-ph q-bio.TO\n",
            "cond-mat.quant-gas math-ph math.MP nlin.PS\n",
            "cs.LG cs.DM stat.ML\n",
            "astro-ph.GA astro-ph.HE gr-qc\n",
            "cs.NI cs.SI\n",
            "hep-lat cond-mat.quant-gas hep-ph\n",
            "stat.CO astro-ph.IM\n",
            "physics.comp-ph cond-mat.mtrl-sci physics.app-ph physics.chem-ph\n",
            "cs.DL cs.IR\n",
            "physics.comp-ph cond-mat.dis-nn cond-mat.stat-mech\n",
            "math.NT physics.class-ph\n",
            "physics.ao-ph physics.data-an physics.geo-ph\n",
            "math-ph math.MP physics.hist-ph\n",
            "cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech\n",
            "cond-mat.soft cond-mat.stat-mech physics.comp-ph\n",
            "cond-mat.str-el cond-mat.dis-nn cs.LG physics.comp-ph quant-ph\n",
            "cs.LG hep-ex physics.data-an stat.ML\n",
            "physics.optics nlin.CD physics.atm-clus quant-ph\n",
            "cs.SI cs.DM\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.other quant-ph\n",
            "hep-th hep-ph nucl-th physics.flu-dyn\n",
            "physics.plasm-ph math-ph math.MP nlin.CD\n",
            "math.OC cs.RO\n",
            "cond-mat.dis-nn q-bio.TO\n",
            "physics.ins-det astro-ph.IM physics.geo-ph\n",
            "cond-mat.quant-gas physics.flu-dyn\n",
            "eess.SP eess.IV\n",
            "quant-ph cs.CC cs.LG\n",
            "physics.soc-ph cond-mat.stat-mech q-bio.PE\n",
            "physics.soc-ph cs.SI q-bio.PE\n",
            "cs.ET physics.optics\n",
            "astro-ph.IM astro-ph.GA astro-ph.SR\n",
            "hep-ph hep-lat nucl-ex nucl-th physics.atom-ph\n",
            "q-bio.GN cs.DC\n",
            "cond-mat.mtrl-sci physics.comp-ph physics.optics\n",
            "cond-mat.mtrl-sci physics.data-an\n",
            "math.RA cs.DM\n",
            "nucl-th astro-ph.HE cond-mat.supr-con\n",
            "physics.plasm-ph physics.atom-ph physics.comp-ph\n",
            "cond-mat.stat-mech cond-mat.mtrl-sci hep-ph\n",
            "eess.IV cs.LG q-bio.NC\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.str-el quant-ph\n",
            "cond-mat.mtrl-sci physics.plasm-ph\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.supr-con\n",
            "physics.flu-dyn physics.class-ph\n",
            "astro-ph.SR physics.atom-ph\n",
            "physics.med-ph q-bio.QM q-bio.TO\n",
            "cs.CY cs.CL cs.HC cs.LG stat.ML\n",
            "nlin.PS math-ph math.MP nlin.SI\n",
            "hep-ex stat.ML\n",
            "cond-mat.mes-hall physics.data-an\n",
            "physics.atom-ph physics.ins-det physics.med-ph\n",
            "nucl-th physics.atom-ph\n",
            "cs.LG nlin.CD physics.data-an q-bio.QM stat.ML\n",
            "physics.plasm-ph nucl-ex physics.atom-ph\n",
            "cond-mat.str-el cond-mat.quant-gas hep-lat\n",
            "cs.DL cs.DB\n",
            "hep-lat cond-mat.stat-mech nucl-th quant-ph\n",
            "cs.LG math.OC math.PR stat.ML\n",
            "physics.comp-ph hep-ex\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech physics.atom-ph\n",
            "physics.soc-ph cs.DL cs.SI\n",
            "astro-ph.HE nucl-ex nucl-th\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.other\n",
            "astro-ph.IM astro-ph.EP gr-qc physics.ins-det physics.optics\n",
            "physics.app-ph nlin.CD physics.optics\n",
            "cs.OH\n",
            "physics.bio-ph cond-mat.soft cond-mat.stat-mech q-bio.CB q-bio.QM\n",
            "hep-th cond-mat.mes-hall\n",
            "astro-ph.HE astro-ph.EP physics.plasm-ph physics.space-ph\n",
            "hep-th gr-qc math-ph math.MP physics.flu-dyn\n",
            "hep-ph astro-ph.CO cond-mat.mtrl-sci hep-ex physics.ins-det\n",
            "eess.IV cs.CV cs.LG cs.MM\n",
            "q-fin.TR physics.soc-ph\n",
            "cs.SE cs.CR\n",
            "cond-mat.stat-mech math.CO\n",
            "stat.ML cs.AI cs.LG\n",
            "math.OC stat.ML\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.str-el physics.chem-ph\n",
            "quant-ph nlin.AO\n",
            "hep-ph cond-mat.dis-nn cond-mat.mes-hall cond-mat.supr-con\n",
            "cs.GT cs.LG cs.MA stat.ML\n",
            "physics.geo-ph cond-mat.soft physics.ao-ph physics.flu-dyn\n",
            "astro-ph.IM cs.LG gr-qc stat.ML\n",
            "physics.ins-det physics.atom-ph physics.optics\n",
            "physics.soc-ph cs.LG cs.SI physics.data-an stat.ML\n",
            "astro-ph.GA stat.ME stat.ML\n",
            "nlin.AO cond-mat.other math.DS\n",
            "math.AP cond-mat.soft\n",
            "math.RT math.DS math.NT\n",
            "cs.NI cs.PF\n",
            "quant-ph nlin.AO nlin.CD physics.class-ph\n",
            "cs.SI cs.DC\n",
            "astro-ph.SR nlin.PS physics.flu-dyn stat.AP\n",
            "cond-mat.mtrl-sci cond-mat.other cond-mat.str-el\n",
            "physics.atom-ph nucl-ex physics.plasm-ph\n",
            "cs.CR cs.CV cs.LG stat.ML\n",
            "gr-qc astro-ph.HE astro-ph.SR\n",
            "cs.RO cs.CL cs.CV cs.HC\n",
            "physics.optics nlin.PS quant-ph\n",
            "astro-ph.CO physics.plasm-ph\n",
            "eess.SP astro-ph.IM physics.ao-ph\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.stat-mech\n",
            "quant-ph cs.IT cs.LG math.IT\n",
            "nlin.AO cond-mat.dis-nn cond-mat.stat-mech q-bio.NC\n",
            "cond-mat.quant-gas hep-th nlin.PS\n",
            "eess.IV cs.LG cs.NE eess.SP\n",
            "math.ST cs.LG math.PR stat.ML stat.TH\n",
            "eess.SP cs.SY eess.SY\n",
            "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
            "physics.chem-ph physics.atm-clus physics.atom-ph\n",
            "cond-mat.str-el physics.atom-ph quant-ph\n",
            "cond-mat.mes-hall cond-mat.dis-nn hep-th\n",
            "physics.plasm-ph astro-ph.CO astro-ph.HE cond-mat.soft hep-ph\n",
            "physics.geo-ph cs.CE stat.AP\n",
            "physics.data-an physics.ins-det\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.soft\n",
            "nlin.CD physics.bio-ph physics.flu-dyn\n",
            "physics.atom-ph astro-ph.EP astro-ph.IM physics.ao-ph physics.chem-ph physics.ins-det\n",
            "cs.DS cs.AI cs.DM cs.SI math.OC\n",
            "physics.soc-ph cond-mat.stat-mech nlin.CD\n",
            "math.AP cs.NA math.NA physics.comp-ph\n",
            "cs.FL cs.GT cs.LO\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.chem-ph physics.flu-dyn\n",
            "quant-ph cs.LG cs.NE\n",
            "hep-th cond-mat.mtrl-sci math-ph math.MP nlin.PS\n",
            "math-ph math.MP physics.flu-dyn\n",
            "cond-mat.quant-gas eess.IV physics.optics quant-ph\n",
            "physics.comp-ph cond-mat.other cs.NA math.NA physics.app-ph\n",
            "gr-qc astro-ph.CO physics.hist-ph\n",
            "cond-mat.mtrl-sci cond-mat.str-el physics.ins-det\n",
            "cs.PL cs.SE\n",
            "cs.CE math.OC\n",
            "cs.SC cs.CC\n",
            "q-bio.PE cond-mat.stat-mech nlin.AO physics.bio-ph\n",
            "stat.ME cs.LG stat.ML\n",
            "physics.flu-dyn cond-mat.other physics.comp-ph\n",
            "q-bio.CB q-bio.SC q-bio.TO\n",
            "astro-ph.GA astro-ph.HE astro-ph.SR gr-qc\n",
            "cond-mat.supr-con cond-mat.soft cond-mat.stat-mech\n",
            "physics.flu-dyn cs.LG physics.comp-ph\n",
            "astro-ph.GA cond-mat.mtrl-sci\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "physics.comp-ph cs.NA math.NA physics.plasm-ph\n",
            "cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "eess.IV cs.MM cs.SY eess.SY\n",
            "q-bio.PE cond-mat.dis-nn\n",
            "quant-ph cs.AI cs.ET\n",
            "gr-qc physics.space-ph\n",
            "astro-ph.SR cs.LG\n",
            "cs.SD cs.CV cs.LG eess.AS\n",
            "quant-ph nlin.CD physics.class-ph\n",
            "quant-ph hep-lat nucl-th\n",
            "physics.ao-ph nlin.CD physics.comp-ph physics.flu-dyn physics.geo-ph stat.ML\n",
            "physics.acc-ph cond-mat.mtrl-sci physics.ins-det\n",
            "cs.RO physics.comp-ph\n",
            "cond-mat.str-el cond-mat.stat-mech hep-th math-ph math.MP quant-ph\n",
            "math.GT math.GR math.MG math.NT\n",
            "stat.ML cond-mat.dis-nn cs.LG math.ST stat.TH\n",
            "astro-ph.GA gr-qc physics.atom-ph\n",
            "cond-mat.mtrl-sci physics.comp-ph physics.plasm-ph\n",
            "cond-mat.soft physics.comp-ph physics.flu-dyn\n",
            "physics.ed-ph cond-mat.mtrl-sci\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.supr-con hep-th\n",
            "physics.soc-ph cond-mat.stat-mech cs.SI stat.AP\n",
            "cs.LO cs.AI\n",
            "physics.flu-dyn physics.comp-ph physics.data-an stat.CO\n",
            "cond-mat.mes-hall cs.AI cs.LG cs.RO\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph\n",
            "cond-mat.str-el cond-mat.dis-nn physics.comp-ph quant-ph\n",
            "cs.SI cs.CL cs.IR\n",
            "cs.CR eess.SP\n",
            "gr-qc astro-ph.GA astro-ph.IM\n",
            "astro-ph.IM physics.ao-ph\n",
            "eess.AS cs.LG cs.SD stat.ML\n",
            "cond-mat.mtrl-sci cond-mat.str-el physics.optics\n",
            "physics.med-ph eess.IV physics.optics\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el hep-th nlin.CD\n",
            "stat.AP stat.ME stat.ML\n",
            "eess.SP cs.NA math.NA math.OC\n",
            "physics.med-ph eess.IV q-bio.QM\n",
            "cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech physics.flu-dyn\n",
            "quant-ph hep-th math-ph math.MP nlin.SI\n",
            "cs.AI stat.ML\n",
            "physics.atm-clus cond-mat.quant-gas\n",
            "math-ph math.GR math.MP math.RT\n",
            "physics.optics cs.CE physics.app-ph physics.comp-ph\n",
            "gr-qc math-ph math.MP quant-ph\n",
            "cs.LG math.OC\n",
            "cs.CR cs.AI\n",
            "math.AC cs.SC\n",
            "physics.soc-ph physics.data-an q-bio.PE q-bio.QM\n",
            "quant-ph cs.IT math.CO math.IT\n",
            "math.DG math.AG math.CV math.MG\n",
            "physics.comp-ph physics.med-ph q-bio.TO\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci physics.optics quant-ph\n",
            "gr-qc cond-mat.other hep-ph\n",
            "cond-mat.mtrl-sci cond-mat.str-el physics.chem-ph physics.comp-ph\n",
            "stat.ME physics.chem-ph physics.data-an\n",
            "quant-ph cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "astro-ph.GA astro-ph.IM physics.ins-det\n",
            "eess.IV physics.data-an\n",
            "astro-ph.EP astro-ph.GA astro-ph.IM astro-ph.SR\n",
            "quant-ph cond-mat.stat-mech nlin.CD nlin.SI\n",
            "astro-ph.HE gr-qc hep-ph\n",
            "cs.CG cs.CV q-bio.QM\n",
            "eess.SY cs.LG cs.RO cs.SY\n",
            "math.FA math.AP\n",
            "hep-lat cs.LG physics.comp-ph\n",
            "physics.bio-ph cond-mat.stat-mech q-bio.CB\n",
            "physics.optics gr-qc\n",
            "physics.chem-ph physics.data-an\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.comp-ph physics.optics\n",
            "cond-mat.quant-gas cs.LG eess.IV physics.atom-ph\n",
            "astro-ph.IM hep-ex nucl-ex physics.ins-det\n",
            "hep-ph physics.optics quant-ph\n",
            "stat.ME cs.LG\n",
            "eess.SY cs.SY q-bio.NC\n",
            "cond-mat.soft cond-mat.mes-hall physics.flu-dyn\n",
            "physics.soc-ph cs.DL physics.hist-ph\n",
            "cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
            "cond-mat.mtrl-sci cond-mat.soft physics.atm-clus physics.chem-ph\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.str-el physics.optics\n",
            "physics.flu-dyn cond-mat.soft math.DS\n",
            "quant-ph cond-mat.mtrl-sci cond-mat.other\n",
            "astro-ph.CO astro-ph.HE astro-ph.IM hep-ph\n",
            "cs.CE cs.NA math.NA stat.CO\n",
            "math.NA cs.NA math.AP stat.ML\n",
            "stat.ME math.DS\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall physics.app-ph\n",
            "cs.SI cond-mat.stat-mech physics.soc-ph\n",
            "math.AC math.RA\n",
            "cond-mat.mtrl-sci physics.chem-ph physics.comp-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
            "cs.NE cs.CV cs.RO\n",
            "physics.ins-det hep-ex hep-ph\n",
            "cond-mat.quant-gas physics.atom-ph physics.chem-ph quant-ph\n",
            "math.PR cs.DM\n",
            "cs.IT cond-mat.stat-mech cs.LG math.IT stat.ML\n",
            "physics.soc-ph cond-mat.soft nlin.AO\n",
            "cond-mat.stat-mech cond-mat.soft physics.optics\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
            "cs.RO cs.NE\n",
            "physics.atom-ph cond-mat.mtrl-sci cond-mat.quant-gas\n",
            "q-bio.TO q-bio.SC\n",
            "q-bio.BM q-bio.SC\n",
            "cs.CE cs.LG\n",
            "cs.AI cs.NE\n",
            "physics.atm-clus physics.atom-ph physics.chem-ph\n",
            "math.RA math.DG\n",
            "physics.comp-ph cs.NA math.NA physics.flu-dyn stat.ML\n",
            "cs.NE cs.LG math.OC\n",
            "physics.ins-det astro-ph.IM hep-ex nucl-ex\n",
            "stat.CO cs.DM cs.DS cs.IT math.IT math.PR\n",
            "q-bio.NC cond-mat.dis-nn nlin.AO nlin.CD physics.bio-ph\n",
            "cs.AI cs.NI\n",
            "cs.CV cs.IR\n",
            "physics.soc-ph econ.GN nlin.AO q-fin.EC\n",
            "math.NT cs.SC math.AG\n",
            "physics.chem-ph cond-mat.mtrl-sci cond-mat.str-el physics.comp-ph\n",
            "physics.hist-ph astro-ph.EP physics.ao-ph physics.geo-ph\n",
            "cond-mat.quant-gas math.CA\n",
            "math-ph hep-th math.MP nlin.SI\n",
            "physics.comp-ph cond-mat.dis-nn cs.LG\n",
            "physics.ins-det physics.app-ph quant-ph\n",
            "astro-ph.HE gr-qc physics.plasm-ph\n",
            "hep-th cond-mat.stat-mech math-ph math.MP nlin.SI\n",
            "quant-ph cond-mat.mes-hall physics.data-an\n",
            "hep-th cond-mat.stat-mech cond-mat.str-el math-ph math.MP math.PR\n",
            "cs.NE cs.RO\n",
            "physics.chem-ph cond-mat.str-el physics.comp-ph quant-ph\n",
            "cond-mat.stat-mech physics.chem-ph physics.comp-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.atom-ph physics.optics\n",
            "math.FA math-ph math.MP\n",
            "physics.optics physics.plasm-ph\n",
            "cs.CE cond-mat.mtrl-sci cs.NA math.NA\n",
            "cs.CR cs.HC\n",
            "nucl-ex astro-ph.SR nucl-th\n",
            "cs.CR cs.AI cs.PF\n",
            "cs.SI nlin.AO physics.soc-ph\n",
            "cs.DB cs.CY cs.LG\n",
            "cs.CV cs.CL cs.LG\n",
            "cs.MM cs.SI\n",
            "cs.ET cond-mat.dis-nn\n",
            "q-bio.NC cs.NA math.NA\n",
            "math.CO cs.DM cs.DS math.OC\n",
            "physics.ins-det nucl-ex nucl-th quant-ph\n",
            "cs.SI stat.ML\n",
            "nlin.AO cond-mat.dis-nn math.DS nlin.CD\n",
            "cond-mat.stat-mech cond-mat.quant-gas physics.flu-dyn\n",
            "cs.CR cs.RO\n",
            "physics.plasm-ph cond-mat.stat-mech physics.comp-ph\n",
            "q-bio.PE physics.bio-ph stat.AP\n",
            "cs.RO cs.MA cs.SY eess.SY math.OC\n",
            "hep-lat cond-mat.str-el hep-ph hep-th\n",
            "physics.atom-ph astro-ph.HE physics.plasm-ph\n",
            "physics.hist-ph astro-ph.EP astro-ph.SR\n",
            "physics.chem-ph cond-mat.stat-mech quant-ph\n",
            "physics.app-ph physics.ins-det physics.optics quant-ph\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.mtrl-sci\n",
            "cond-mat.stat-mech gr-qc\n",
            "physics.acc-ph hep-ex hep-ph\n",
            "physics.ins-det quant-ph\n",
            "math.GR cs.DS\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.mes-hall\n",
            "cs.LG eess.SP hep-ex\n",
            "cs.AI cs.RO\n",
            "hep-lat cond-mat.stat-mech cs.LG\n",
            "cond-mat.soft cond-mat.other physics.comp-ph\n",
            "physics.data-an math.DS nlin.CD\n",
            "cond-mat.mtrl-sci physics.comp-ph quant-ph\n",
            "eess.AS cs.SD eess.IV\n",
            "cond-mat.soft cond-mat.stat-mech nlin.PS physics.chem-ph\n",
            "math-ph cond-mat.stat-mech hep-th math.MP\n",
            "cond-mat.stat-mech hep-th math-ph math.MP nlin.SI\n",
            "math.DG math.AT math.GT\n",
            "cs.CL cs.AI cs.SI\n",
            "cond-mat.other cond-mat.soft physics.atm-clus\n",
            "physics.soc-ph econ.EM\n",
            "cond-mat.quant-gas nlin.SI\n",
            "physics.optics eess.IV physics.ins-det\n",
            "cs.SE cs.LG cs.PL\n",
            "cond-mat.supr-con cond-mat.str-el nucl-th\n",
            "physics.hist-ph astro-ph.SR\n",
            "math.NA cs.NA math.CO\n",
            "cs.CY cs.LG\n",
            "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con quant-ph\n",
            "physics.flu-dyn cond-mat.other nlin.CD\n",
            "cs.SD cs.CR cs.LG eess.AS\n",
            "gr-qc astro-ph.HE physics.space-ph\n",
            "physics.flu-dyn cond-mat.mes-hall cond-mat.soft cond-mat.stat-mech physics.chem-ph\n",
            "physics.comp-ph astro-ph.IM cs.LG gr-qc\n",
            "quant-ph cs.ET cs.PL\n",
            "cond-mat.soft physics.bio-ph physics.flu-dyn q-bio.CB\n",
            "physics.bio-ph cond-mat.soft nlin.AO\n",
            "eess.SP cs.CY cs.HC stat.AP\n",
            "astro-ph.IM hep-ex physics.data-an\n",
            "hep-ph astro-ph.CO hep-ex nucl-ex\n",
            "hep-th cond-mat.stat-mech gr-qc hep-lat\n",
            "math.AG math.AT math.CV math.NT\n",
            "gr-qc astro-ph.HE astro-ph.IM\n",
            "physics.class-ph cond-mat.other nlin.AO\n",
            "math.PR q-fin.CP\n",
            "math.NA cs.NA physics.comp-ph physics.flu-dyn\n",
            "cs.LG cs.MA stat.ML\n",
            "math-ph cond-mat.str-el hep-th math.CT math.MP math.QA\n",
            "cond-mat.dis-nn cond-mat.stat-mech physics.optics\n",
            "hep-ph hep-th nucl-th physics.plasm-ph quant-ph\n",
            "quant-ph gr-qc hep-ph hep-th physics.hist-ph\n",
            "physics.optics physics.app-ph physics.chem-ph\n",
            "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR\n",
            "cs.AI cs.LO\n",
            "cond-mat.dis-nn cond-mat.stat-mech gr-qc hep-lat physics.soc-ph\n",
            "cs.CV cs.LG eess.SP\n",
            "math-ph math.MP nlin.CD nucl-th\n",
            "math.NA cs.NA math.OC math.ST physics.data-an stat.TH\n",
            "hep-th math-ph math.DG math.MP math.SG\n",
            "q-bio.GN cs.LG physics.bio-ph q-bio.CB\n",
            "cs.DB cs.CR cs.DS\n",
            "math.AP math.CA math.DS\n",
            "physics.comp-ph cond-mat.dis-nn cond-mat.other\n",
            "q-bio.PE cs.SY eess.SY math.DS\n",
            "nlin.PS physics.app-ph physics.optics\n",
            "stat.ML cs.LG math.OC math.ST stat.ME stat.TH\n",
            "quant-ph math-ph math.MP physics.class-ph physics.comp-ph\n",
            "q-bio.PE cs.MA q-bio.QM\n",
            "nlin.CD cond-mat.stat-mech physics.hist-ph\n",
            "cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
            "cond-mat.str-el cond-mat.other cond-mat.quant-gas cond-mat.stat-mech\n",
            "physics.data-an eess.IV\n",
            "physics.soc-ph math.DS q-bio.PE\n",
            "gr-qc astro-ph.CO astro-ph.HE hep-ph hep-th\n",
            "cond-mat.mes-hall cond-mat.other physics.atom-ph physics.optics quant-ph\n",
            "cond-mat.supr-con hep-th\n",
            "physics.med-ph physics.bio-ph q-bio.PE\n",
            "physics.app-ph cond-mat.mtrl-sci cond-mat.soft\n",
            "cs.NI cs.LG eess.SP\n",
            "stat.OT econ.GN q-fin.EC\n",
            "physics.comp-ph cond-mat.str-el quant-ph\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci quant-ph\n",
            "stat.ME q-bio.QM\n",
            "physics.app-ph cond-mat.mtrl-sci cond-mat.supr-con\n",
            "nucl-th nucl-ex physics.chem-ph\n",
            "physics.acc-ph hep-ex physics.atom-ph\n",
            "quant-ph cond-mat.dis-nn hep-th\n",
            "q-fin.GN q-fin.MF\n",
            "hep-ph hep-ex nucl-ex nucl-th physics.ins-det\n",
            "cs.CL cs.LO\n",
            "cond-mat.quant-gas cond-mat.mes-hall nlin.PS physics.optics\n",
            "stat.ML cs.CL cs.LG\n",
            "cond-mat.soft physics.app-ph physics.optics\n",
            "cs.NE cs.AI cs.MA\n",
            "hep-ph hep-lat nucl-ex nucl-th\n",
            "cond-mat.quant-gas cs.LG quant-ph\n",
            "eess.SY cs.FL cs.SY\n",
            "physics.optics cond-mat.mes-hall physics.app-ph physics.chem-ph\n",
            "cond-mat.str-el cond-mat.mtrl-sci cond-mat.other cond-mat.stat-mech\n",
            "hep-lat hep-ex hep-ph physics.atom-ph\n",
            "cs.LG cs.SD eess.AS\n",
            "stat.ML cs.LG econ.GN q-fin.EC\n",
            "cs.GR cs.CE physics.comp-ph\n",
            "cs.GT cs.NI eess.SP\n",
            "physics.soc-ph cond-mat.dis-nn q-bio.PE\n",
            "nlin.PS nlin.AO physics.soc-ph\n",
            "physics.ins-det astro-ph.IM hep-ex\n",
            "physics.chem-ph physics.comp-ph stat.ML\n",
            "cond-mat.str-el physics.atm-clus\n",
            "cs.DC cond-mat.mtrl-sci\n",
            "physics.ins-det astro-ph.CO astro-ph.IM hep-ex\n",
            "physics.atom-ph physics.app-ph physics.ins-det quant-ph\n",
            "physics.chem-ph physics.ins-det physics.optics\n",
            "cs.LG cs.DM cs.SI stat.ML\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech\n",
            "cs.CV stat.AP\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph physics.data-an physics.ins-det\n",
            "physics.data-an cs.LG hep-ex\n",
            "astro-ph.CO astro-ph.SR gr-qc\n",
            "stat.ME q-bio.MN q-bio.QM\n",
            "physics.geo-ph physics.flu-dyn\n",
            "astro-ph.CO astro-ph.IM hep-ex hep-ph physics.ins-det\n",
            "physics.ed-ph physics.ao-ph physics.flu-dyn\n",
            "cond-mat.stat-mech cond-mat.str-el math-ph math.MP\n",
            "cs.CV cs.CR\n",
            "cs.SI cs.IR cs.LG cs.NE\n",
            "cond-mat.mtrl-sci physics.atom-ph\n",
            "cond-mat.mtrl-sci cs.LG physics.chem-ph\n",
            "cond-mat.mtrl-sci cs.CE cs.LG cs.NE physics.comp-ph\n",
            "cond-mat.mtrl-sci cs.LG physics.comp-ph stat.ML\n",
            "cond-mat.mtrl-sci astro-ph.HE cond-mat.other nucl-th\n",
            "gr-qc astro-ph.CO hep-ex hep-ph\n",
            "physics.class-ph math-ph math.MP\n",
            "q-bio.PE cond-mat.stat-mech physics.soc-ph\n",
            "hep-ph hep-ex nucl-th physics.atom-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
            "cond-mat.mes-hall physics.ins-det physics.optics quant-ph\n",
            "quant-ph physics.atm-clus physics.optics\n",
            "physics.bio-ph quant-ph\n",
            "cs.CV cs.GR cs.LG\n",
            "physics.atom-ph astro-ph.IM\n",
            "physics.comp-ph astro-ph.IM\n",
            "quant-ph astro-ph.IM physics.optics\n",
            "cond-mat.mes-hall hep-ph hep-th\n",
            "quant-ph cond-mat.supr-con hep-th\n",
            "physics.optics physics.comp-ph quant-ph\n",
            "physics.soc-ph cs.MA econ.GN q-fin.EC\n",
            "physics.pop-ph\n",
            "astro-ph.IM physics.ed-ph\n",
            "astro-ph.SR stat.AP\n",
            "quant-ph cond-mat.dis-nn physics.chem-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas\n",
            "cond-mat.mtrl-sci stat.ML\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.str-el hep-th quant-ph\n",
            "physics.flu-dyn cs.LG cs.SY eess.SY\n",
            "q-fin.ST q-fin.RM\n",
            "physics.soc-ph econ.GN q-fin.EC stat.AP\n",
            "hep-lat cond-mat.str-el hep-th\n",
            "q-bio.BM physics.bio-ph physics.data-an q-bio.QM\n",
            "physics.pop-ph astro-ph.CO\n",
            "physics.chem-ph nlin.AO\n",
            "cs.CL cs.CV cs.HC cs.LG\n",
            "eess.IV cs.CV cs.NA math.NA\n",
            "astro-ph.CO astro-ph.HE gr-qc hep-ph\n",
            "physics.flu-dyn physics.ins-det physics.optics\n",
            "math.PR cond-mat.stat-mech nlin.CD\n",
            "cond-mat.quant-gas nlin.PS physics.flu-dyn\n",
            "cond-mat.mes-hall cond-mat.other cond-mat.quant-gas\n",
            "q-bio.PE cs.LG\n",
            "quant-ph cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con\n",
            "quant-ph cs.FL nlin.CG\n",
            "eess.AS cs.LG cs.SD q-bio.QM stat.ML\n",
            "cs.SI cs.LG physics.soc-ph stat.ML\n",
            "econ.GN cs.CY q-fin.EC\n",
            "physics.ins-det astro-ph.IM physics.optics\n",
            "physics.soc-ph cs.NE\n",
            "physics.atom-ph physics.chem-ph physics.optics\n",
            "math-ph cond-mat.stat-mech math.MP nlin.SI\n",
            "cs.PF hep-ex physics.ins-det\n",
            "physics.comp-ph cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
            "cond-mat.stat-mech nlin.CG nlin.SI\n",
            "physics.comp-ph cond-mat.soft eess.IV\n",
            "physics.atom-ph cond-mat.str-el physics.chem-ph\n",
            "cs.LG cs.CV eess.IV stat.ML\n",
            "stat.ME cs.CL cs.LG\n",
            "cond-mat.stat-mech cond-mat.soft math-ph math.MP math.PR\n",
            "q-bio.QM q-bio.BM\n",
            "cs.LG cs.CG stat.ML\n",
            "cond-mat.stat-mech math-ph math.MP physics.comp-ph\n",
            "cs.DS cs.DM math.HO math.OC\n",
            "cs.NE eess.IV\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el hep-th\n",
            "physics.chem-ph nucl-ex\n",
            "cs.CR cs.SE\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft nlin.CG physics.comp-ph\n",
            "physics.comp-ph physics.chem-ph physics.flu-dyn\n",
            "cond-mat.str-el physics.geo-ph\n",
            "math.CT math.RT\n",
            "hep-th cond-mat.mes-hall hep-ph quant-ph\n",
            "cs.CR cs.IR\n",
            "cs.CL cs.AI cs.CV\n",
            "physics.optics cond-mat.mes-hall physics.plasm-ph\n",
            "physics.bio-ph physics.comp-ph\n",
            "q-bio.PE cond-mat.dis-nn cond-mat.stat-mech physics.soc-ph\n",
            "math-ph hep-th math.MP math.SG\n",
            "hep-ph hep-th math-ph math.MP quant-ph\n",
            "cs.SC cs.LO\n",
            "cs.HC eess.SP\n",
            "physics.class-ph physics.optics\n",
            "cs.ET cs.NE nlin.AO\n",
            "physics.pop-ph physics.soc-ph\n",
            "q-bio.NC cs.LG eess.SP\n",
            "astro-ph.IM physics.soc-ph\n",
            "astro-ph.HE astro-ph.SR hep-ph nucl-th\n",
            "quant-ph hep-ph physics.optics\n",
            "quant-ph nlin.CD physics.optics\n",
            "cond-mat.mes-hall cond-mat.other cond-mat.str-el cond-mat.supr-con physics.app-ph\n",
            "quant-ph cond-mat.dis-nn cs.LG\n",
            "nlin.AO cs.SY eess.SY math.DS\n",
            "gr-qc astro-ph.GA hep-ph hep-th\n",
            "stat.ME math.PR math.ST stat.TH\n",
            "cs.FL cs.DC cs.LO cs.PL\n",
            "cond-mat.dis-nn physics.comp-ph physics.data-an quant-ph\n",
            "cs.LG cs.AI cs.CL cs.CV\n",
            "physics.plasm-ph astro-ph.HE physics.space-ph\n",
            "astro-ph.HE astro-ph.CO astro-ph.GA\n",
            "q-bio.CB q-bio.SC\n",
            "astro-ph.HE astro-ph.IM gr-qc\n",
            "astro-ph.CO astro-ph.GA hep-ph physics.optics\n",
            "cond-mat.soft math-ph math.MP\n",
            "quant-ph gr-qc hep-th math-ph math.MP\n",
            "cs.RO cs.LG cs.MA cs.SY eess.SY math.OC\n",
            "hep-ph cond-mat.stat-mech nucl-th\n",
            "math.PR physics.bio-ph q-bio.QM\n",
            "cs.CR cs.CV eess.IV\n",
            "physics.soc-ph math.DS\n",
            "cs.CV cs.GR eess.IV\n",
            "cond-mat.other hep-ph\n",
            "quant-ph eess.SP physics.optics\n",
            "cond-mat.soft physics.bio-ph physics.flu-dyn physics.med-ph\n",
            "physics.app-ph cond-mat.mes-hall physics.ins-det physics.optics quant-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.flu-dyn\n",
            "q-bio.PE cs.CE\n",
            "cond-mat.str-el cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cs.SI physics.soc-ph stat.AP\n",
            "gr-qc astro-ph.CO astro-ph.HE hep-th\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.optics\n",
            "nlin.AO math.DS\n",
            "cond-mat.supr-con nlin.AO physics.optics\n",
            "physics.chem-ph cond-mat.mtrl-sci cond-mat.soft\n",
            "astro-ph.GA physics.comp-ph\n",
            "hep-ph cond-mat.stat-mech hep-th nucl-th\n",
            "hep-th cond-mat.other gr-qc\n",
            "cs.CL cs.SI\n",
            "physics.plasm-ph cond-mat.mes-hall\n",
            "cs.CY physics.ins-det\n",
            "astro-ph.GA astro-ph.IM gr-qc\n",
            "physics.flu-dyn math.DS math.OC\n",
            "nucl-th astro-ph.HE hep-ph nucl-ex\n",
            "hep-th cond-mat.other hep-ph quant-ph\n",
            "quant-ph cond-mat.stat-mech cs.LG math.OC\n",
            "physics.soc-ph nlin.CD\n",
            "astro-ph.HE astro-ph.SR physics.plasm-ph\n",
            "cs.CR cs.CY cs.NI cs.SI\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.quant-gas cond-mat.str-el\n",
            "cs.ET quant-ph\n",
            "cs.SI cs.DB eess.SP\n",
            "physics.flu-dyn astro-ph.GA astro-ph.SR physics.geo-ph physics.plasm-ph\n",
            "astro-ph.EP physics.flu-dyn\n",
            "physics.pop-ph physics.ed-ph\n",
            "astro-ph.IM hep-ex physics.ins-det\n",
            "cs.CL cs.IR cs.LG cs.SI stat.ML\n",
            "cond-mat.stat-mech nlin.PS q-bio.PE q-bio.SC\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall nlin.AO nlin.PS\n",
            "eess.AS cs.CL cs.SD\n",
            "cond-mat.stat-mech math-ph math.MP nlin.CD\n",
            "physics.soc-ph physics.pop-ph\n",
            "math.CT math.AC\n",
            "cs.ET eess.SP\n",
            "cond-mat.soft cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "math-ph hep-th math.MP physics.class-ph\n",
            "quant-ph cond-mat.str-el cs.DS\n",
            "physics.space-ph cs.SY eess.SY\n",
            "stat.ME q-fin.MF stat.ML\n",
            "cs.AI cs.CV cs.LG\n",
            "physics.chem-ph physics.app-ph\n",
            "physics.optics nlin.AO\n",
            "q-bio.PE math.DS math.OC physics.soc-ph\n",
            "astro-ph.CO astro-ph.GA astro-ph.IM hep-ph hep-th\n",
            "quant-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
            "physics.ao-ph physics.geo-ph\n",
            "math-ph gr-qc math.MP physics.hist-ph\n",
            "physics.app-ph cond-mat.other\n",
            "cond-mat.stat-mech physics.bio-ph q-bio.QM\n",
            "cond-mat.mtrl-sci cond-mat.other physics.comp-ph\n",
            "cond-mat.mtrl-sci nucl-ex quant-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
            "nlin.PS math.DS physics.flu-dyn\n",
            "cs.CE cs.SY eess.SP eess.SY\n",
            "cs.SE cs.NI\n",
            "nlin.PS cs.NA math.DS math.NA physics.comp-ph\n",
            "physics.ao-ph cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "gr-qc physics.ins-det\n",
            "physics.ins-det cond-mat.mes-hall physics.app-ph physics.atom-ph physics.chem-ph quant-ph\n",
            "cond-mat.stat-mech cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft\n",
            "cond-mat.stat-mech cond-mat.soft physics.data-an\n",
            "astro-ph.IM astro-ph.EP astro-ph.SR stat.CO\n",
            "gr-qc astro-ph.CO hep-th math-ph math.MP quant-ph\n",
            "astro-ph.IM hep-ex physics.data-an physics.ins-det\n",
            "hep-th cond-mat.str-el physics.flu-dyn\n",
            "quant-ph cs.DM\n",
            "cs.IT cs.PF math.IT\n",
            "math-ph hep-ph math.MP quant-ph\n",
            "cs.LG cs.AI cs.CL cs.NE eess.SP\n",
            "astro-ph.CO hep-ex hep-ph physics.ins-det\n",
            "physics.atm-clus physics.atom-ph quant-ph\n",
            "quant-ph cond-mat.mes-hall math-ph math.MP physics.hist-ph\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.soft\n",
            "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech physics.bio-ph physics.chem-ph\n",
            "cs.CE eess.IV\n",
            "cs.CE physics.ins-det\n",
            "physics.flu-dyn math.DS physics.ao-ph physics.class-ph physics.plasm-ph\n",
            "stat.ME physics.data-an\n",
            "q-bio.NC cs.AI\n",
            "physics.optics physics.acc-ph physics.atom-ph\n",
            "eess.SP math.OC\n",
            "quant-ph cond-mat.supr-con physics.atom-ph\n",
            "q-fin.TR q-fin.MF q-fin.ST stat.AP\n",
            "physics.chem-ph cond-mat.str-el quant-ph\n",
            "physics.geo-ph cond-mat.mtrl-sci\n",
            "math.NT math.CO math.PR\n",
            "quant-ph cond-mat.mes-hall cs.LG\n",
            "cond-mat.quant-gas cond-mat.stat-mech nucl-th\n",
            "cs.LG physics.data-an stat.ML\n",
            "physics.data-an cond-mat.stat-mech physics.flu-dyn\n",
            "physics.comp-ph cond-mat.soft stat.ML\n",
            "hep-ph astro-ph.CO astro-ph.GA gr-qc\n",
            "physics.space-ph astro-ph.IM physics.atom-ph physics.chem-ph\n",
            "cs.LO cs.PL cs.SE\n",
            "astro-ph.SR astro-ph.IM physics.comp-ph\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
            "cs.SE cs.HC\n",
            "cs.SE cs.HC cs.SI\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci physics.comp-ph\n",
            "physics.flu-dyn cond-mat.stat-mech nlin.PS\n",
            "cs.LO cs.AI cs.CR cs.LG cs.SE\n",
            "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP\n",
            "cond-mat.stat-mech hep-th math-ph math.MP math.PR\n",
            "hep-th math-ph math.MP nucl-th\n",
            "physics.hist-ph astro-ph.EP\n",
            "physics.app-ph physics.class-ph physics.flu-dyn\n",
            "hep-ph physics.optics physics.plasm-ph\n",
            "hep-th math.DS nlin.CD nlin.PS\n",
            "nlin.PS nlin.CD physics.optics\n",
            "cs.SE cs.GR\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.other\n",
            "cs.CR cs.PF\n",
            "physics.acc-ph hep-ex\n",
            "hep-th cond-mat.dis-nn cond-mat.soft cond-mat.stat-mech\n",
            "hep-th cond-mat.supr-con hep-lat hep-ph\n",
            "physics.plasm-ph astro-ph.SR physics.comp-ph\n",
            "quant-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.optics\n",
            "physics.flu-dyn cond-mat.mtrl-sci physics.app-ph\n",
            "cs.DB cs.AI cs.CL cs.LG stat.ML\n",
            "cs.CL cs.AI cs.CR cs.LG\n",
            "cs.CL cs.CV\n",
            "cond-mat.stat-mech cond-mat.dis-nn cs.LG hep-lat physics.comp-ph\n",
            "physics.flu-dyn cond-mat.quant-gas\n",
            "math.GT gr-qc math.DG\n",
            "cs.CV cs.LG cs.MM eess.IV\n",
            "physics.plasm-ph cond-mat.soft physics.flu-dyn\n",
            "nlin.AO cond-mat.stat-mech nlin.PS physics.bio-ph\n",
            "cond-mat.quant-gas physics.atom-ph physics.optics\n",
            "cs.LO cs.FL cs.IT cs.PL math.IT\n",
            "cs.SI cond-mat.stat-mech cs.LG\n",
            "physics.ins-det physics.app-ph physics.optics\n",
            "math.PR physics.bio-ph\n",
            "physics.ao-ph astro-ph.EP physics.flu-dyn\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech quant-ph\n",
            "physics.comp-ph physics.chem-ph quant-ph\n",
            "astro-ph.IM astro-ph.SR cs.LG stat.ML\n",
            "cond-mat.mes-hall math-ph math.MP nlin.CD\n",
            "astro-ph.EP physics.geo-ph\n",
            "q-bio.QM stat.AP\n",
            "astro-ph.IM astro-ph.CO astro-ph.EP\n",
            "physics.atom-ph hep-ex hep-ph quant-ph\n",
            "quant-ph cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.str-el cs.LG\n",
            "astro-ph.GA astro-ph.CO astro-ph.HE\n",
            "cs.NE cs.AI cs.SC\n",
            "stat.ME math.LO stat.CO stat.ML\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.other cond-mat.soft\n",
            "cs.SE cs.LG\n",
            "cond-mat.stat-mech math.PR physics.data-an\n",
            "cs.SE cs.CY\n",
            "stat.AP q-bio.QM\n",
            "cs.LG cs.DM math.CO stat.ML\n",
            "gr-qc physics.plasm-ph\n",
            "math-ph math.MP physics.optics\n",
            "physics.pop-ph quant-ph\n",
            "physics.med-ph eess.IV physics.ins-det\n",
            "cs.GL cs.SE\n",
            "physics.plasm-ph cs.CV\n",
            "cond-mat.quant-gas cond-mat.mtrl-sci cond-mat.other quant-ph\n",
            "astro-ph.EP astro-ph.IM q-bio.QM\n",
            "cond-mat.mes-hall physics.optics physics.plasm-ph\n",
            "physics.soc-ph cs.LG cs.SI\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.stat-mech cond-mat.supr-con quant-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph physics.optics\n",
            "hep-th cond-mat.mes-hall cond-mat.str-el\n",
            "cond-mat.stat-mech hep-th math-ph math.MP math.NT\n",
            "eess.SP cs.LG cs.RO\n",
            "cs.GR cs.LG\n",
            "q-bio.BM q-bio.QM\n",
            "q-bio.MN q-bio.GN\n",
            "cs.LG cs.ET stat.ML\n",
            "cond-mat.str-el hep-ph\n",
            "hep-ph math.GR math.RT quant-ph\n",
            "cond-mat.other nlin.CD physics.flu-dyn\n",
            "astro-ph.GA physics.atm-clus\n",
            "stat.ME math.DS physics.soc-ph q-bio.PE\n",
            "physics.ed-ph hep-ex\n",
            "math.DS nlin.CD\n",
            "econ.GN nlin.AO physics.soc-ph q-fin.EC\n",
            "q-fin.GN physics.soc-ph\n",
            "q-fin.ST q-fin.GN\n",
            "cs.DC cs.LG math.OC stat.ML\n",
            "hep-ph physics.atom-ph physics.chem-ph\n",
            "hep-th astro-ph.HE hep-ph\n",
            "astro-ph.IM physics.ins-det physics.optics\n",
            "hep-ph gr-qc hep-ex hep-th\n",
            "physics.app-ph cond-mat.dis-nn\n",
            "physics.app-ph cs.LG physics.flu-dyn\n",
            "hep-th cond-mat.dis-nn hep-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP quant-ph\n",
            "physics.app-ph eess.SP physics.optics\n",
            "eess.SP cs.CE cs.CV eess.IV\n",
            "cond-mat.mtrl-sci physics.app-ph physics.comp-ph physics.flu-dyn\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall math.AT\n",
            "cond-mat.dis-nn cond-mat.quant-gas cond-mat.stat-mech quant-ph\n",
            "astro-ph.CO physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.other nlin.AO quant-ph\n",
            "cond-mat.soft gr-qc physics.class-ph\n",
            "stat.ME econ.EM\n",
            "math-ph gr-qc hep-th math.DG math.MP math.QA\n",
            "cs.HC cs.IR\n",
            "q-bio.PE cs.SY eess.SY math.OC physics.soc-ph\n",
            "quant-ph cond-mat.str-el physics.atm-clus\n",
            "q-bio.PE nlin.AO nlin.CD physics.soc-ph\n",
            "gr-qc astro-ph.HE astro-ph.IM eess.SP\n",
            "math-ph cs.IT math.IT math.MP\n",
            "cond-mat.quant-gas cond-mat.supr-con nlin.PS\n",
            "gr-qc astro-ph.CO hep-th quant-ph\n",
            "q-bio.QM q-bio.PE\n",
            "quant-ph cond-mat.other hep-th\n",
            "cs.RO cs.CG cs.SI\n",
            "cs.CC cs.ET cs.NA math.NA\n",
            "eess.SP cs.MM cs.NI\n",
            "math.NA cs.NA physics.bio-ph physics.comp-ph q-bio.MN\n",
            "q-fin.PM math.ST q-fin.ST stat.TH\n",
            "stat.AP cs.LG q-bio.NC\n",
            "cond-mat.supr-con cond-mat.quant-gas cond-mat.stat-mech\n",
            "cs.CY cs.AI cs.LG\n",
            "physics.data-an physics.acc-ph\n",
            "stat.AP cs.LG stat.ME stat.ML\n",
            "astro-ph.HE astro-ph.GA gr-qc hep-ph\n",
            "cond-mat.str-el cond-mat.stat-mech hep-lat quant-ph\n",
            "gr-qc cond-mat.supr-con\n",
            "cond-mat.soft cond-mat.stat-mech cs.CG\n",
            "cs.CL cs.IR cs.LG cs.NE\n",
            "physics.optics cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "astro-ph.CO astro-ph.HE astro-ph.SR hep-ph\n",
            "hep-th cond-mat.str-el math.NT\n",
            "eess.IV physics.bio-ph\n",
            "cs.CL cs.IR cs.LG cs.SI\n",
            "math.PR cs.SY eess.SY\n",
            "cs.HC cs.SY eess.SY math.OC\n",
            "q-bio.GN q-bio.PE\n",
            "hep-th cond-mat.dis-nn cond-mat.str-el quant-ph\n",
            "astro-ph.IM gr-qc physics.data-an physics.ins-det\n",
            "cs.CR cs.CY cs.LG\n",
            "cs.CY cs.CL cs.LG\n",
            "cs.AI math.OC\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph physics.data-an\n",
            "eess.AS cs.LG cs.SD eess.SP\n",
            "cond-mat.stat-mech cond-mat.soft physics.bio-ph physics.chem-ph\n",
            "math.DS math.AG math.CV math.DG\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech nlin.CD\n",
            "stat.AP q-bio.PE\n",
            "cond-mat.stat-mech cond-mat.soft physics.chem-ph physics.comp-ph\n",
            "math.DG gr-qc math-ph math.MP\n",
            "q-fin.GN cs.SI\n",
            "physics.data-an physics.flu-dyn\n",
            "astro-ph.CO cs.LG\n",
            "cs.RO cs.SY eess.SY math.OC\n",
            "stat.AP cs.LG\n",
            "gr-qc astro-ph.CO astro-ph.GA astro-ph.HE physics.plasm-ph\n",
            "cs.PL quant-ph\n",
            "hep-th hep-lat hep-ph math-ph math.MP\n",
            "nucl-ex hep-ex hep-ph nucl-th physics.ins-det\n",
            "q-bio.QM cond-mat.dis-nn physics.bio-ph q-bio.BM\n",
            "astro-ph.HE astro-ph.SR physics.space-ph\n",
            "cs.SI cs.CL\n",
            "hep-ph astro-ph.CO cond-mat.other cond-mat.supr-con\n",
            "astro-ph.IM cs.RO\n",
            "cs.LG cs.DC eess.SP math.OC stat.ML\n",
            "q-bio.QM cs.SD eess.AS\n",
            "cond-mat.stat-mech cond-mat.dis-nn hep-th\n",
            "eess.SP cs.LG q-bio.QM\n",
            "eess.SP cs.RO\n",
            "cond-mat.mes-hall nlin.CD physics.app-ph\n",
            "physics.hist-ph gr-qc hep-ph\n",
            "cs.CR eess.AS\n",
            "q-bio.TO physics.optics\n",
            "cond-mat.mes-hall physics.flu-dyn\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "astro-ph.HE astro-ph.SR nucl-th\n",
            "nlin.SI gr-qc math-ph math.MP\n",
            "cs.LG cond-mat.dis-nn cond-mat.stat-mech\n",
            "physics.optics nlin.PS physics.atom-ph quant-ph\n",
            "hep-ph astro-ph.CO cond-mat.mtrl-sci\n",
            "quant-ph cond-mat.stat-mech cond-mat.str-el hep-lat\n",
            "cond-mat.soft cond-mat.mes-hall cond-mat.stat-mech physics.chem-ph\n",
            "astro-ph.SR astro-ph.EP physics.space-ph\n",
            "gr-qc cond-mat.other\n",
            "astro-ph.CO astro-ph.HE gr-qc\n",
            "nlin.CD astro-ph.EP\n",
            "physics.chem-ph math-ph math.MP quant-ph\n",
            "cs.RO cs.AI cs.LG cs.SY eess.SY\n",
            "physics.ins-det hep-ex hep-ph nucl-ex physics.soc-ph\n",
            "math.DS physics.flu-dyn\n",
            "cs.SD cs.CV cs.LG\n",
            "math.ST math.CO stat.TH\n",
            "cs.DL cs.CL cs.IR cs.LG\n",
            "cs.CV cs.CR cs.LG eess.IV\n",
            "quant-ph physics.atm-clus physics.atom-ph\n",
            "physics.optics cond-mat.mes-hall nlin.AO\n",
            "cs.HC cs.AI eess.SP\n",
            "physics.optics cond-mat.mtrl-sci cond-mat.soft physics.chem-ph\n",
            "physics.atom-ph physics.ins-det\n",
            "eess.SP stat.AP stat.ML\n",
            "cond-mat.dis-nn cs.DM math.PR\n",
            "q-bio.QM physics.app-ph physics.ins-det physics.med-ph\n",
            "eess.IV cs.NE physics.comp-ph physics.optics\n",
            "physics.ins-det physics.atm-clus physics.atom-ph physics.chem-ph\n",
            "cs.CR cs.IT cs.NI math.IT\n",
            "cond-mat.quant-gas cond-mat.str-el cond-mat.supr-con nucl-th\n",
            "hep-ph astro-ph.HE hep-ex hep-th\n",
            "cond-mat.quant-gas cond-mat.soft cond-mat.str-el cond-mat.supr-con hep-th\n",
            "cs.LG cs.SE stat.ML\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
            "hep-th hep-ph math.NT\n",
            "cs.CY cs.AI cs.HC cs.SE\n",
            "hep-ex cs.LG\n",
            "quant-ph q-bio.BM q-bio.GN q-bio.QM stat.ML\n",
            "hep-ex astro-ph.SR physics.ins-det\n",
            "cond-mat.stat-mech cond-mat.dis-nn cs.LG\n",
            "cond-mat.mes-hall cond-mat.soft cond-mat.str-el\n",
            "quant-ph cond-mat.mes-hall physics.atm-clus\n",
            "quant-ph physics.data-an\n",
            "astro-ph.EP astro-ph.SR physics.space-ph\n",
            "nucl-th cond-mat.quant-gas hep-ph\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.quant-gas quant-ph\n",
            "nucl-ex nucl-th physics.ins-det\n",
            "cond-mat.str-el cond-mat.stat-mech nlin.CD quant-ph\n",
            "q-bio.CB nlin.AO\n",
            "cs.CR cs.LG cs.SI\n",
            "cs.PF cs.NI\n",
            "hep-th cond-mat.str-el hep-lat hep-ph nucl-th\n",
            "math-ph math.MP nucl-th physics.atom-ph\n",
            "math.CO cs.DM cs.DS\n",
            "cs.SE cs.CR cs.LG\n",
            "physics.chem-ph stat.ML\n",
            "econ.EM stat.AP\n",
            "physics.chem-ph astro-ph.EP astro-ph.SR physics.atom-ph\n",
            "hep-ph astro-ph.HE astro-ph.SR\n",
            "hep-th astro-ph.HE hep-ph nucl-th\n",
            "cs.AI cs.HC\n",
            "nlin.CD cond-mat.quant-gas cond-mat.soft\n",
            "physics.comp-ph cond-mat.dis-nn cond-mat.quant-gas\n",
            "gr-qc math-ph math.MP physics.class-ph\n",
            "cond-mat.stat-mech cond-mat.str-el hep-th math-ph math.MP\n",
            "q-fin.GN cs.CY cs.LG stat.ML\n",
            "astro-ph.CO astro-ph.GA hep-ex hep-ph\n",
            "hep-th astro-ph.CO gr-qc hep-ph quant-ph\n",
            "hep-ex hep-ph nucl-ex nucl-th\n",
            "cs.GT cs.DM\n",
            "hep-ph astro-ph.CO gr-qc physics.atom-ph\n",
            "cond-mat.soft cond-mat.other cond-mat.stat-mech physics.chem-ph physics.plasm-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
            "cond-mat.mes-hall physics.app-ph physics.ins-det physics.optics\n",
            "cs.SE cs.RO\n",
            "cs.DL cs.IR physics.soc-ph q-bio.OT\n",
            "cs.SE cs.LO\n",
            "nlin.CD cs.LG math.DS\n",
            "cs.AI cs.GT cs.LG cs.MA cs.RO\n",
            "cs.NI cs.SE\n",
            "cs.DL cs.CL\n",
            "physics.app-ph cond-mat.mes-hall physics.ins-det quant-ph\n",
            "physics.comp-ph cs.CE cs.LG physics.chem-ph stat.ML\n",
            "quant-ph cond-mat.quant-gas hep-lat hep-th\n",
            "cs.HC cs.CV cs.DL cs.LG\n",
            "hep-th physics.class-ph quant-ph\n",
            "cond-mat.quant-gas hep-ph nucl-th\n",
            "astro-ph.HE astro-ph.IM cs.LG gr-qc\n",
            "math.OC cs.LG math.AG stat.ML\n",
            "physics.ins-det astro-ph.IM physics.ao-ph physics.optics\n",
            "physics.chem-ph cs.LG physics.comp-ph stat.ML\n",
            "cond-mat.soft math.AP\n",
            "cs.HC cs.IR cs.LG stat.ML\n",
            "cs.SI cs.CY cs.LG\n",
            "cond-mat.stat-mech physics.data-an physics.space-ph\n",
            "cond-mat.stat-mech cond-mat.soft physics.flu-dyn\n",
            "cs.LG math.PR stat.ML\n",
            "physics.soc-ph cond-mat.dis-nn cs.SI nlin.AO q-bio.NC\n",
            "q-bio.PE physics.med-ph physics.soc-ph\n",
            "cs.DC cs.OS cs.PF\n",
            "astro-ph.EP cond-mat.stat-mech physics.ao-ph physics.geo-ph\n",
            "q-bio.PE cs.CY\n",
            "cs.RO cs.LG physics.optics\n",
            "cs.LG cs.NE math.DG math.GN stat.ML\n",
            "q-bio.NC cs.AI stat.ML\n",
            "cs.NE eess.SP math.DS physics.comp-ph\n",
            "cond-mat.str-el cond-mat.quant-gas cond-mat.stat-mech physics.atom-ph quant-ph\n",
            "astro-ph.CO astro-ph.GA astro-ph.SR nucl-th\n",
            "cs.CR cs.CY cs.DC cs.GT cs.LG\n",
            "eess.IV cs.NA math.NA physics.optics\n",
            "nucl-th cond-mat.supr-con physics.chem-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn physics.atom-ph quant-ph\n",
            "quant-ph cond-mat.other math-ph math.MP physics.optics\n",
            "astro-ph.HE physics.flu-dyn\n",
            "quant-ph physics.chem-ph physics.optics\n",
            "physics.ins-det cond-mat.mtrl-sci cond-mat.str-el\n",
            "gr-qc astro-ph.CO hep-ph physics.ins-det\n",
            "physics.optics cond-mat.supr-con physics.chem-ph physics.ins-det\n",
            "physics.app-ph cond-mat.mes-hall physics.comp-ph\n",
            "physics.ins-det nucl-ex quant-ph\n",
            "q-bio.NC stat.ML\n",
            "physics.med-ph physics.ins-det\n",
            "quant-ph cond-mat.mtrl-sci physics.comp-ph\n",
            "cond-mat.mes-hall physics.comp-ph physics.optics\n",
            "stat.ML cs.LG nlin.CD physics.ao-ph physics.data-an\n",
            "physics.flu-dyn nlin.PS nlin.SI\n",
            "cs.CL cs.LG q-bio.NC\n",
            "astro-ph.HE astro-ph.GA astro-ph.IM\n",
            "hep-th cond-mat.quant-gas hep-ph\n",
            "quant-ph cond-mat.dis-nn physics.optics\n",
            "math-ph cond-mat.stat-mech hep-th math.MP quant-ph\n",
            "nucl-ex hep-ex physics.ins-det\n",
            "hep-lat cond-mat.str-el\n",
            "physics.hist-ph astro-ph.EP physics.pop-ph\n",
            "astro-ph.IM astro-ph.GA gr-qc\n",
            "gr-qc astro-ph.CO astro-ph.GA astro-ph.HE\n",
            "math.FA cs.NA math.CV math.NA\n",
            "astro-ph.HE hep-ex physics.app-ph\n",
            "cs.CE physics.bio-ph\n",
            "gr-qc astro-ph.EP hep-th\n",
            "stat.ML cs.AI cs.LG math.OC\n",
            "math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH\n",
            "physics.hist-ph cond-mat.stat-mech\n",
            "hep-th cond-mat.other hep-ph\n",
            "cs.LG cond-mat.dis-nn cs.CV q-bio.NC stat.ML\n",
            "cond-mat.soft hep-th math-ph math.MP nlin.SI\n",
            "physics.ins-det cs.LG physics.data-an\n",
            "astro-ph.IM astro-ph.CO physics.ins-det\n",
            "physics.flu-dyn astro-ph.GA astro-ph.SR\n",
            "q-bio.PE stat.AP\n",
            "cond-mat.mes-hall cond-mat.dis-nn quant-ph\n",
            "cond-mat.other nucl-th quant-ph\n",
            "quant-ph cs.ET physics.atom-ph physics.comp-ph\n",
            "eess.SY cs.RO cs.SY\n",
            "nlin.AO cond-mat.dis-nn nlin.PS\n",
            "stat.ME astro-ph.GA\n",
            "cond-mat.stat-mech nlin.CD physics.comp-ph\n",
            "cs.FL\n",
            "q-bio.PE astro-ph.EP\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.chem-ph\n",
            "physics.bio-ph q-bio.MN q-bio.QM\n",
            "physics.optics cond-mat.mes-hall cond-mat.str-el\n",
            "cs.CV cs.NA math.NA math.PR stat.ML\n",
            "math.OC cs.CV cs.RO\n",
            "cs.SI cs.IR cs.LG\n",
            "cs.LG math.DS stat.ML\n",
            "cs.LG cs.CL cs.SD eess.AS stat.ML\n",
            "eess.SP cs.CV eess.IV\n",
            "cs.LG eess.SP\n",
            "physics.atom-ph physics.chem-ph physics.optics quant-ph\n",
            "cond-mat.mes-hall hep-th math-ph math.MP quant-ph\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "physics.chem-ph cond-mat.mtrl-sci quant-ph\n",
            "nucl-th astro-ph.SR nucl-ex\n",
            "astro-ph.EP astro-ph.HE physics.space-ph\n",
            "physics.pop-ph astro-ph.EP astro-ph.HE\n",
            "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO\n",
            "eess.IV cs.CV physics.optics\n",
            "astro-ph.SR astro-ph.CO astro-ph.GA\n",
            "cs.CY cs.SY eess.SY\n",
            "cs.AI cs.SE\n",
            "cs.RO cs.AI cs.MA\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci nlin.CD quant-ph\n",
            "physics.comp-ph cs.LG stat.AP\n",
            "astro-ph.EP astro-ph.IM physics.ao-ph\n",
            "hep-lat cs.LG hep-th\n",
            "cond-mat.quant-gas cond-mat.stat-mech gr-qc hep-th\n",
            "physics.flu-dyn nlin.PS\n",
            "cs.CV cs.CY cs.HC\n",
            "cs.CY stat.ML\n",
            "hep-ph astro-ph.HE gr-qc nucl-th\n",
            "hep-ex astro-ph.CO hep-ph\n",
            "cond-mat.str-el nlin.CD quant-ph\n",
            "cond-mat.quant-gas astro-ph.CO hep-th\n",
            "eess.AS cs.LG cs.MM cs.SD\n",
            "nucl-ex astro-ph.EP astro-ph.HE\n",
            "hep-th hep-lat quant-ph\n",
            "gr-qc astro-ph.CO astro-ph.HE hep-ph\n",
            "cond-mat.soft cs.LG physics.chem-ph physics.comp-ph\n",
            "physics.comp-ph cs.LG physics.flu-dyn\n",
            "physics.comp-ph physics.bio-ph physics.flu-dyn\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.quant-gas\n",
            "cs.MA cs.SE\n",
            "cs.MS stat.CO\n",
            "cs.AI physics.soc-ph\n",
            "astro-ph.GA astro-ph.SR physics.hist-ph\n",
            "hep-ex physics.acc-ph\n",
            "cond-mat.supr-con cond-mat.str-el quant-ph\n",
            "hep-th math.AG math.CO stat.ML\n",
            "cs.CY cs.CR cs.NI\n",
            "hep-th cond-mat.quant-gas cond-mat.str-el hep-ph\n",
            "q-bio.BM physics.bio-ph physics.chem-ph\n",
            "cs.CV cs.AI cs.LG cs.MM eess.IV\n",
            "nlin.AO cond-mat.dis-nn q-bio.NC\n",
            "eess.SP cs.CV cs.MM eess.IV stat.ME\n",
            "physics.chem-ph physics.optics quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph physics.optics\n",
            "math.NA cs.CE cs.NA physics.comp-ph\n",
            "astro-ph.SR astro-ph.HE astro-ph.IM\n",
            "cond-mat.stat-mech cond-mat.mes-hall nlin.CD\n",
            "nlin.CD nlin.AO\n",
            "astro-ph.HE astro-ph.GA hep-th\n",
            "q-bio.GN q-bio.PE q-bio.QM\n",
            "quant-ph math.GR\n",
            "hep-th hep-ph quant-ph\n",
            "physics.app-ph physics.atom-ph physics.bio-ph physics.optics\n",
            "eess.SP cs.LG math.OC\n",
            "hep-ph astro-ph.CO hep-th nucl-th\n",
            "quant-ph cond-mat.stat-mech gr-qc\n",
            "eess.SP physics.ins-det physics.optics\n",
            "cs.IR cs.CV cs.LG cs.NE stat.ML\n",
            "quant-ph cond-mat.mes-hall cond-mat.other cond-mat.quant-gas\n",
            "cs.RO cs.MA\n",
            "hep-ph astro-ph.GA astro-ph.SR hep-ex\n",
            "astro-ph.HE astro-ph.CO astro-ph.IM cs.LG hep-ph\n",
            "cond-mat.stat-mech hep-ph nucl-th\n",
            "cs.DB cs.SE\n",
            "stat.ML cs.LG cs.NE\n",
            "physics.acc-ph quant-ph\n",
            "physics.ao-ph cs.CE\n",
            "q-bio.OT q-bio.PE\n",
            "math.DS nlin.AO stat.AP\n",
            "math.DS cs.CG cs.GR math.DG math.MG\n",
            "math.NA cs.NA q-fin.PR\n",
            "hep-th gr-qc math-ph math.MP math.QA\n",
            "quant-ph physics.bio-ph physics.med-ph physics.optics\n",
            "cond-mat.mes-hall cond-mat.other nlin.AO nlin.CD physics.comp-ph\n",
            "cs.NE cs.CL cs.LG\n",
            "cs.DC hep-ex physics.ins-det\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "math-ph cs.IT hep-th math.IT math.MP quant-ph\n",
            "physics.flu-dyn physics.ao-ph physics.comp-ph\n",
            "cs.LG eess.SP math.OC stat.ML\n",
            "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV\n",
            "cs.MM eess.SP\n",
            "physics.soc-ph cond-mat.dis-nn cs.SI stat.ML\n",
            "cs.HC cs.SI\n",
            "eess.SP cs.AI\n",
            "physics.atom-ph cond-mat.stat-mech\n",
            "cs.LG cs.AI stat.AP stat.ML\n",
            "eess.IV cs.LG physics.optics\n",
            "nucl-th hep-th nucl-ex\n",
            "q-bio.PE physics.soc-ph q-bio.QM\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci physics.chem-ph\n",
            "cs.RO cs.HC cs.NE\n",
            "q-bio.NC cs.CV cs.LG eess.IV stat.ML\n",
            "physics.ao-ph nlin.CD physics.app-ph physics.data-an\n",
            "physics.plasm-ph cond-mat.mes-hall physics.acc-ph physics.app-ph\n",
            "cs.LG cs.AI cs.SI stat.ML\n",
            "cs.HC cs.RO cs.SY eess.SY\n",
            "cs.LG cs.AI cs.CL stat.ML\n",
            "cs.LG cs.CL eess.IV stat.ML\n",
            "math.HO cs.LO\n",
            "math.OC cs.AI cs.LG cs.MA\n",
            "physics.flu-dyn cs.SY eess.SY\n",
            "physics.ins-det physics.chem-ph\n",
            "cond-mat.mtrl-sci cond-mat.other physics.chem-ph physics.comp-ph\n",
            "physics.hist-ph astro-ph.IM\n",
            "cs.LG cond-mat.dis-nn cs.NI physics.data-an stat.ML\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.soft physics.chem-ph\n",
            "math-ph cond-mat.other math.MP nlin.SI\n",
            "cond-mat.mtrl-sci cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph\n",
            "physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech quant-ph\n",
            "cs.LO math.LO\n",
            "cs.GR math.DG\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.other\n",
            "cond-mat.stat-mech cond-mat.dis-nn hep-lat hep-th\n",
            "hep-ph nucl-ex nucl-th physics.ins-det\n",
            "physics.chem-ph cond-mat.soft physics.comp-ph\n",
            "quant-ph astro-ph.CO hep-th\n",
            "physics.comp-ph physics.chem-ph stat.AP\n",
            "astro-ph.CO astro-ph.EP astro-ph.SR\n",
            "physics.optics cs.CV eess.IV physics.app-ph\n",
            "cs.CV cond-mat.stat-mech\n",
            "cs.NI cs.MM\n",
            "cs.CV cs.LG cs.NE eess.IV\n",
            "physics.gen-ph astro-ph.HE quant-ph\n",
            "quant-ph cond-mat.mes-hall math-ph math.AT math.MP\n",
            "nlin.AO cs.SI math.DS\n",
            "eess.SP cs.CE cs.IT math.IT\n",
            "cond-mat.stat-mech math-ph math.MP nlin.CD physics.ao-ph physics.flu-dyn\n",
            "cs.LG cs.CV eess.IV\n",
            "hep-lat hep-ph hep-th nucl-th\n",
            "q-bio.CB nlin.PS physics.bio-ph\n",
            "astro-ph.HE astro-ph.SR physics.geo-ph\n",
            "cond-mat.mes-hall cond-mat.other physics.optics\n",
            "cs.CV cs.LG math.DS stat.AP stat.ML\n",
            "math.DS cs.SY eess.SY\n",
            "cs.LG cs.GR stat.ML\n",
            "cond-mat.mes-hall physics.atm-clus physics.comp-ph\n",
            "math.CO math.ST stat.CO stat.TH\n",
            "physics.comp-ph cond-mat.dis-nn cs.LG physics.flu-dyn stat.ML\n",
            "astro-ph.GA astro-ph.CO astro-ph.HE hep-ph\n",
            "astro-ph.GA nucl-th\n",
            "cond-mat.supr-con cond-mat.dis-nn\n",
            "q-bio.QM physics.bio-ph q-bio.MN\n",
            "physics.ao-ph astro-ph.IM\n",
            "math.OC cs.LG cs.NE\n",
            "astro-ph.HE astro-ph.GA cond-mat.mtrl-sci\n",
            "q-fin.ST q-fin.TR\n",
            "physics.pop-ph gr-qc physics.class-ph physics.space-ph\n",
            "cs.NE physics.acc-ph physics.comp-ph\n",
            "cs.LG cs.IR cs.SI stat.ML\n",
            "astro-ph.SR astro-ph.GA hep-ph\n",
            "cond-mat.stat-mech math-ph math.MP math.SP quant-ph\n",
            "astro-ph.IM nucl-th\n",
            "astro-ph.HE astro-ph.IM physics.atom-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "astro-ph.SR astro-ph.EP physics.flu-dyn\n",
            "cond-mat.stat-mech cond-mat.dis-nn nlin.CD quant-ph\n",
            "hep-th hep-lat nucl-th quant-ph\n",
            "math.CO math-ph math.KT math.MP\n",
            "math.AG math.CA math.CV math.DS\n",
            "astro-ph.GA gr-qc hep-th\n",
            "quant-ph math.ST stat.TH\n",
            "cond-mat.quant-gas cond-mat.stat-mech nlin.SI\n",
            "cs.DC cs.PF\n",
            "cond-mat.soft cond-mat.mes-hall physics.comp-ph\n",
            "math.NA cs.NA math.OC stat.OT\n",
            "eess.AS cs.LG\n",
            "cs.DC cs.DS\n",
            "nlin.PS physics.flu-dyn physics.plasm-ph\n",
            "astro-ph.CO physics.data-an stat.ML\n",
            "physics.comp-ph cond-mat.mtrl-sci cond-mat.str-el\n",
            "cs.CL cs.SD eess.AS\n",
            "cs.AR cs.LG eess.SP\n",
            "cs.IT cs.SY eess.SP eess.SY math.IT\n",
            "physics.comp-ph astro-ph.HE gr-qc\n",
            "cs.CY cs.HC cs.RO\n",
            "q-bio.QM q-bio.BM q-bio.MN q-bio.SC\n",
            "cs.CR cs.NI cs.SI\n",
            "quant-ph cond-mat.quant-gas hep-th physics.optics\n",
            "cond-mat.str-el cond-mat.mes-hall hep-th math-ph math.MP\n",
            "physics.app-ph math.DS\n",
            "nucl-th hep-lat hep-ph physics.comp-ph\n",
            "astro-ph.HE cs.LG gr-qc\n",
            "quant-ph cond-mat.other gr-qc hep-th\n",
            "physics.app-ph physics.ins-det quant-ph\n",
            "physics.bio-ph cond-mat.soft cond-mat.stat-mech nlin.CG physics.chem-ph\n",
            "physics.soc-ph cs.SY eess.SY math.PR\n",
            "cond-mat.mtrl-sci cs.CE cs.NA math.NA physics.app-ph\n",
            "gr-qc cond-mat.quant-gas\n",
            "astro-ph.EP physics.ed-ph\n",
            "physics.comp-ph cs.CE\n",
            "math.OC eess.SP\n",
            "physics.comp-ph cond-mat.supr-con\n",
            "cond-mat.mtrl-sci cs.LG physics.chem-ph physics.comp-ph\n",
            "cond-mat.mes-hall physics.chem-ph physics.comp-ph physics.optics quant-ph\n",
            "physics.space-ph astro-ph.IM\n",
            "physics.optics gr-qc physics.flu-dyn\n",
            "cond-mat.mtrl-sci cond-mat.other cond-mat.supr-con physics.app-ph\n",
            "gr-qc astro-ph.CO astro-ph.HE astro-ph.SR hep-ph\n",
            "hep-ph astro-ph.IM hep-ex\n",
            "gr-qc astro-ph.HE math-ph math.MP\n",
            "astro-ph.HE astro-ph.GA astro-ph.SR physics.space-ph\n",
            "cs.CV cs.CL cs.IR cs.LG\n",
            "nlin.AO cs.LG nlin.CD\n",
            "astro-ph.SR physics.flu-dyn physics.plasm-ph\n",
            "hep-th hep-ph physics.class-ph physics.optics\n",
            "quant-ph cond-mat.mes-hall cond-mat.stat-mech physics.optics\n",
            "astro-ph.HE physics.plasm-ph physics.space-ph\n",
            "gr-qc astro-ph.CO hep-ph hep-th quant-ph\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech nlin.AO physics.flu-dyn\n",
            "gr-qc cond-mat.supr-con quant-ph\n",
            "physics.ed-ph physics.hist-ph\n",
            "physics.geo-ph cs.LG\n",
            "cs.NI cs.MS\n",
            "cs.CL cs.AI cs.CV cs.LG\n",
            "cs.AI cs.CL cs.LG cs.MA\n",
            "cs.CL cs.HC\n",
            "eess.SP cs.CR cs.LG\n",
            "cs.AI stat.OT\n",
            "physics.ins-det astro-ph.CO nucl-ex\n",
            "math-ph math.MP nlin.CD\n",
            "eess.IV cs.CV physics.app-ph\n",
            "q-bio.QM cs.LG q-bio.MN\n",
            "astro-ph.GA astro-ph.EP astro-ph.SR\n",
            "quant-ph math-ph math.MP physics.class-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el physics.app-ph\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.stat-mech cond-mat.str-el\n",
            "cs.CY cs.AI cs.HC cs.RO cs.SY eess.SY\n",
            "cs.MA cs.RO cs.SE\n",
            "q-bio.NC stat.AP\n",
            "cond-mat.mes-hall cond-mat.supr-con physics.app-ph\n",
            "cond-mat.stat-mech math.DS\n",
            "stat.ML cs.LG physics.ao-ph physics.data-an\n",
            "cond-mat.dis-nn cond-mat.soft physics.comp-ph\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci physics.chem-ph\n",
            "cond-mat.stat-mech nlin.CD physics.flu-dyn\n",
            "hep-ex astro-ph.CO hep-ph physics.ins-det\n",
            "cond-mat.soft cond-mat.stat-mech nlin.PS\n",
            "astro-ph.SR astro-ph.EP physics.ao-ph\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph\n",
            "cond-mat.stat-mech cs.SI physics.soc-ph\n",
            "physics.ed-ph astro-ph.IM\n",
            "astro-ph.IM physics.space-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.str-el cond-mat.supr-con\n",
            "physics.comp-ph cond-mat.stat-mech physics.med-ph\n",
            "physics.hist-ph math-ph math.MP\n",
            "astro-ph.SR astro-ph.EP physics.ao-ph physics.chem-ph\n",
            "q-bio.PE math.CA\n",
            "math.NA cs.NA math.AP math.DS math.FA\n",
            "eess.SP cs.LG cs.SY eess.IV eess.SY\n",
            "physics.optics physics.class-ph physics.flu-dyn\n",
            "cond-mat.soft nlin.CD nlin.PS\n",
            "cs.LG q-bio.BM stat.ML\n",
            "cs.CV cs.IT math.IT stat.ML\n",
            "cs.CV cs.GR cs.LG cs.RO eess.IV\n",
            "physics.ao-ph cond-mat.stat-mech\n",
            "math-ph cond-mat.mes-hall math.MP quant-ph\n",
            "gr-qc hep-th nlin.CD physics.class-ph quant-ph\n",
            "cs.DC cs.LG eess.SP\n",
            "q-bio.QM stat.ME\n",
            "cond-mat.dis-nn cond-mat.stat-mech cond-mat.str-el hep-lat physics.comp-ph\n",
            "hep-ph nucl-ex nucl-th physics.atom-ph\n",
            "cond-mat.str-el cond-mat.mes-hall cond-mat.other cond-mat.stat-mech\n",
            "q-bio.PE cs.GT cs.MA physics.bio-ph physics.soc-ph\n",
            "q-bio.TO\n",
            "cs.NI cs.SY eess.SP eess.SY\n",
            "physics.flu-dyn cond-mat.stat-mech nlin.CG\n",
            "q-bio.QM cs.CV cs.LG\n",
            "cond-mat.stat-mech stat.ML\n",
            "eess.SP cs.LG math.OC physics.comp-ph stat.ML\n",
            "astro-ph.CO hep-ex\n",
            "astro-ph.IM astro-ph.GA astro-ph.HE\n",
            "cond-mat.quant-gas nucl-th physics.atm-clus quant-ph\n",
            "nlin.AO physics.bio-ph\n",
            "cond-mat.str-el cs.LG quant-ph\n",
            "math.PR math.DS\n",
            "cond-mat.dis-nn cond-mat.quant-gas physics.atom-ph quant-ph\n",
            "math.NA cs.NA math-ph math.MP\n",
            "hep-ph hep-th math-ph math.MP\n",
            "physics.plasm-ph physics.app-ph\n",
            "quant-ph hep-lat hep-th\n",
            "cs.AR q-bio.GN stat.CO\n",
            "nlin.CD cond-mat.other math.DS math.NT\n",
            "physics.plasm-ph cond-mat.other physics.chem-ph physics.comp-ph\n",
            "cs.DC cs.CV cs.GR\n",
            "cs.LG cs.MM cs.SD eess.AS stat.ML\n",
            "q-bio.PE math.DS\n",
            "physics.optics cond-mat.quant-gas physics.atm-clus\n",
            "physics.ins-det astro-ph.HE astro-ph.IM\n",
            "gr-qc cond-mat.quant-gas hep-th\n",
            "astro-ph.CO astro-ph.HE hep-ph hep-th\n",
            "physics.atom-ph nucl-th physics.chem-ph\n",
            "cs.CE cs.NA math.NA\n",
            "cs.CV cs.MM eess.IV\n",
            "physics.class-ph quant-ph\n",
            "quant-ph hep-th math-ph math.MP physics.hist-ph\n",
            "cond-mat.quant-gas physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.str-el hep-lat hep-th\n",
            "physics.ins-det cond-mat.mtrl-sci physics.acc-ph\n",
            "physics.plasm-ph cond-mat.str-el physics.comp-ph\n",
            "cs.LG cs.CV cs.RO\n",
            "hep-ph astro-ph.CO astro-ph.IM gr-qc physics.optics\n",
            "cs.LO cs.PL cs.SC\n",
            "astro-ph.GA astro-ph.CO cond-mat.stat-mech\n",
            "quant-ph cond-mat.mes-hall cond-mat.quant-gas physics.optics\n",
            "cs.CL cs.AI cs.IR cs.LG cs.NE\n",
            "cs.CV cs.GR cs.HC eess.IV\n",
            "quant-ph cond-mat.str-el cs.CC\n",
            "hep-lat cs.NA math.NA physics.comp-ph\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech physics.chem-ph\n",
            "cs.HC cs.CY\n",
            "stat.OT cs.CY\n",
            "astro-ph.SR astro-ph.HE hep-ph nucl-th\n",
            "physics.atom-ph hep-ph physics.chem-ph\n",
            "eess.SP physics.data-an\n",
            "cs.SD cs.IT eess.AS math.IT\n",
            "cs.CY cs.DB cs.LG stat.AP\n",
            "cs.SI cs.LG physics.data-an\n",
            "eess.AS cs.CL cs.LG\n",
            "astro-ph.CO physics.hist-ph\n",
            "cs.NE cs.LG nlin.CD physics.comp-ph\n",
            "math.SP math.AP math.CV\n",
            "cs.DL astro-ph.IM physics.soc-ph\n",
            "cs.SE cs.SI\n",
            "q-bio.PE cs.OH physics.soc-ph q-bio.QM\n",
            "econ.GN physics.soc-ph q-fin.EC\n",
            "q-bio.NC cs.NE\n",
            "quant-ph math-ph math.MP physics.data-an stat.AP\n",
            "cs.GR math.GN\n",
            "physics.hist-ph physics.ed-ph\n",
            "physics.acc-ph nucl-ex\n",
            "cs.SI physics.soc-ph stat.ME\n",
            "hep-th cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech\n",
            "cond-mat.stat-mech cond-mat.mes-hall cond-mat.str-el physics.comp-ph\n",
            "math.NA cs.NA math-ph math.MP physics.comp-ph\n",
            "cond-mat.mtrl-sci nucl-ex\n",
            "cs.CY physics.soc-ph stat.AP\n",
            "physics.soc-ph econ.TH\n",
            "physics.pop-ph astro-ph.CO physics.bio-ph\n",
            "cs.CL eess.AS\n",
            "physics.pop-ph cond-mat.str-el\n",
            "hep-th math-ph math.AG math.CO math.MP\n",
            "physics.optics cond-mat.dis-nn quant-ph\n",
            "cond-mat.supr-con cond-mat.str-el physics.comp-ph\n",
            "cs.DB cs.LG\n",
            "cs.CE cs.CV\n",
            "stat.AP cs.SI\n",
            "cs.LG q-bio.NC stat.ML\n",
            "cs.SD cs.CL cs.LG eess.AS\n",
            "cs.SD cs.IR cs.LG eess.AS\n",
            "eess.SP cs.DC\n",
            "cond-mat.mes-hall math-ph math.MP physics.optics quant-ph\n",
            "cs.CY physics.ed-ph physics.hist-ph\n",
            "eess.AS cs.LG cs.SD eess.SP stat.ML\n",
            "physics.app-ph cond-mat.mtrl-sci physics.optics quant-ph\n",
            "quant-ph physics.atom-ph physics.plasm-ph\n",
            "physics.ao-ph cs.CV physics.geo-ph\n",
            "cs.CV cs.GR cs.LG stat.ML\n",
            "cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph physics.plasm-ph\n",
            "physics.space-ph astro-ph.SR physics.data-an physics.flu-dyn physics.plasm-ph\n",
            "physics.atom-ph astro-ph.IM hep-ph physics.optics\n",
            "cond-mat.str-el cond-mat.quant-gas cond-mat.supr-con hep-ph quant-ph\n",
            "physics.bio-ph cond-mat.stat-mech physics.comp-ph q-bio.BM\n",
            "cs.DC cs.NI cs.PF\n",
            "cs.PL cs.CY\n",
            "cs.CY cs.AI cs.LG cs.SI\n",
            "quant-ph cond-mat.mes-hall math-ph math.MP\n",
            "cond-mat.mes-hall astro-ph.CO cond-mat.dis-nn cond-mat.supr-con hep-ph\n",
            "physics.data-an physics.soc-ph\n",
            "hep-th cond-mat.str-el hep-ph\n",
            "physics.flu-dyn gr-qc\n",
            "q-fin.ST cs.CE cs.CV\n",
            "cs.SI cs.CR\n",
            "nlin.AO cond-mat.dis-nn physics.chem-ph q-bio.QM\n",
            "eess.SP cs.AR stat.CO stat.ME\n",
            "cs.CV cs.GR cs.HC\n",
            "cs.LG cs.DB stat.ML\n",
            "astro-ph.EP astro-ph.GA astro-ph.HE\n",
            "gr-qc astro-ph.GA math-ph math.MP\n",
            "q-bio.BM cond-mat.stat-mech\n",
            "cond-mat.supr-con cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci\n",
            "physics.optics nlin.PS physics.bio-ph q-bio.BM\n",
            "nlin.PS nlin.CD\n",
            "physics.comp-ph cs.PF\n",
            "hep-lat hep-ex hep-ph hep-th\n",
            "cs.SI cond-mat.stat-mech\n",
            "astro-ph.GA hep-ex hep-ph\n",
            "cs.DS cs.DC cs.PF\n",
            "nucl-th astro-ph.HE astro-ph.SR nucl-ex\n",
            "cs.IR cs.DC cs.LG cs.SI\n",
            "math-ph cond-mat.dis-nn cond-mat.stat-mech math.MP math.PR\n",
            "cs.CV cs.DC cs.PL\n",
            "cond-mat.quant-gas cond-mat.stat-mech cond-mat.str-el cond-mat.supr-con\n",
            "cs.DC cs.DB\n",
            "cond-mat.dis-nn cond-mat.str-el hep-lat\n",
            "astro-ph.IM astro-ph.EP astro-ph.SR cs.LG\n",
            "eess.SP cs.SY eess.SY math.DS\n",
            "physics.geo-ph nlin.PS physics.flu-dyn\n",
            "physics.soc-ph cs.SI econ.GN q-fin.EC\n",
            "cs.DC hep-ex\n",
            "hep-ph cond-mat.supr-con gr-qc\n",
            "cs.NI cs.CY cs.DS\n",
            "physics.flu-dyn cond-mat.mtrl-sci\n",
            "astro-ph.SR physics.geo-ph physics.space-ph\n",
            "cs.IR cs.AI cs.HC cs.NE\n",
            "cond-mat.mtrl-sci cond-mat.quant-gas\n",
            "cs.SI cs.CR cs.CY\n",
            "cs.DS math.ST stat.ML stat.TH\n",
            "astro-ph.HE astro-ph.IM astro-ph.SR hep-ph\n",
            "cs.CL cond-mat.stat-mech\n",
            "cond-mat.mtrl-sci physics.atom-ph physics.chem-ph physics.comp-ph\n",
            "q-bio.MN math.OC\n",
            "cs.IR cs.CL cs.DL cs.LG\n",
            "cs.LG stat.ME stat.ML\n",
            "cs.AR cs.LG cs.PF\n",
            "cond-mat.quant-gas cond-mat.mes-hall cond-mat.stat-mech quant-ph\n",
            "physics.space-ph astro-ph.SR physics.geo-ph physics.plasm-ph\n",
            "q-bio.TO cond-mat.stat-mech math.DS nlin.AO physics.bio-ph\n",
            "astro-ph.GA nlin.CD\n",
            "eess.SY cs.AI cs.RO cs.SY\n",
            "physics.med-ph eess.SP physics.ins-det\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.stat-mech physics.class-ph\n",
            "hep-lat cond-mat.stat-mech cond-mat.str-el hep-th\n",
            "physics.flu-dyn math.DS nlin.CD physics.comp-ph stat.ML\n",
            "physics.bio-ph cs.AI\n",
            "physics.app-ph astro-ph.IM physics.optics\n",
            "physics.flu-dyn physics.app-ph physics.geo-ph\n",
            "physics.acc-ph cond-mat.other physics.class-ph\n",
            "physics.optics physics.flu-dyn\n",
            "cs.SE cs.PL\n",
            "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech\n",
            "math-ph cond-mat.stat-mech math.DS math.MP\n",
            "cond-mat.str-el physics.chem-ph physics.comp-ph quant-ph\n",
            "cs.CV cs.AI cs.RO\n",
            "physics.plasm-ph cs.LG\n",
            "physics.app-ph physics.med-ph\n",
            "nlin.CD cond-mat.stat-mech hep-th quant-ph\n",
            "cond-mat.quant-gas cond-mat.stat-mech math-ph math.MP quant-ph\n",
            "astro-ph.SR astro-ph.EP cs.LG\n",
            "cs.CR cs.SI\n",
            "physics.ins-det physics.ao-ph\n",
            "cs.LG cs.AI cs.DC stat.ML\n",
            "hep-th astro-ph.CO cond-mat.other gr-qc hep-ph\n",
            "cs.LG cs.CV cs.RO stat.ML\n",
            "quant-ph cond-mat.mes-hall physics.chem-ph physics.comp-ph\n",
            "q-bio.QM cs.LG physics.bio-ph\n",
            "physics.optics cond-mat.dis-nn cs.ET math.OC nlin.AO\n",
            "cs.CV cs.HC cs.RO\n",
            "physics.flu-dyn cs.CE cs.NA math.NA\n",
            "stat.ME stat.OT\n",
            "quant-ph physics.comp-ph physics.optics\n",
            "stat.AP stat.OT\n",
            "physics.class-ph physics.atom-ph\n",
            "cs.CV cs.LG cs.SY eess.SY\n",
            "nlin.AO cond-mat.dis-nn math.DS\n",
            "physics.space-ph astro-ph.EP astro-ph.HE physics.plasm-ph\n",
            "gr-qc physics.ed-ph\n",
            "cs.CY cs.DL\n",
            "eess.SY cs.SY math.AG math.DS\n",
            "cond-mat.mes-hall physics.bio-ph\n",
            "cs.CV cs.AI cs.GR\n",
            "q-bio.BM q-bio.TO\n",
            "cs.LG cs.HC\n",
            "cond-mat.str-el hep-th math.CT math.QA\n",
            "math.NA cs.NA math.PR physics.comp-ph\n",
            "cond-mat.str-el cond-mat.soft\n",
            "quant-ph cs.DM cs.ET\n",
            "eess.AS cs.CR cs.SD\n",
            "physics.data-an astro-ph.IM\n",
            "cond-mat.soft cond-mat.dis-nn physics.comp-ph physics.data-an\n",
            "cond-mat.dis-nn math-ph math.MP math.PR\n",
            "cs.CV cs.LG cs.SD eess.AS\n",
            "astro-ph.EP astro-ph.HE\n",
            "astro-ph.SR astro-ph.EP astro-ph.HE\n",
            "cond-mat.dis-nn physics.chem-ph\n",
            "math.GT math.AT math.RA\n",
            "cond-mat.soft cond-mat.mtrl-sci cond-mat.stat-mech nlin.PS physics.flu-dyn\n",
            "physics.med-ph cs.CV cs.CY\n",
            "eess.SP cs.CY\n",
            "nlin.AO cond-mat.dis-nn math.DS physics.bio-ph\n",
            "cond-mat.mes-hall cond-mat.stat-mech physics.comp-ph\n",
            "cond-mat.soft cond-mat.other physics.flu-dyn\n",
            "astro-ph.GA astro-ph.CO physics.data-an\n",
            "cond-mat.stat-mech cond-mat.supr-con quant-ph\n",
            "cs.LO cs.AI cs.PL\n",
            "cs.LO cs.FL cs.GT\n",
            "hep-ph hep-ex hep-th nucl-ex\n",
            "nlin.CD nlin.AO quant-ph\n",
            "cs.LG cs.GT cs.NI\n",
            "astro-ph.SR astro-ph.IM gr-qc\n",
            "math.AP math.SP\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con quant-ph\n",
            "astro-ph.SR astro-ph.HE astro-ph.IM physics.space-ph\n",
            "cond-mat.mes-hall cond-mat.soft physics.app-ph\n",
            "astro-ph.CO astro-ph.GA cond-mat.stat-mech\n",
            "cs.IR cs.CY\n",
            "astro-ph.IM astro-ph.EP physics.space-ph\n",
            "cs.LG cs.SY eess.SY stat.ML\n",
            "q-bio.OT nlin.CD\n",
            "cs.CL cs.LG cs.LO\n",
            "cs.CL cs.HC cs.LG\n",
            "cs.LG cs.AI cs.CR stat.ML\n",
            "q-bio.PE stat.AP stat.ML\n",
            "cs.CY cs.AR cs.CV\n",
            "astro-ph.GA astro-ph.EP\n",
            "cs.CV cs.AI cs.MM\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.mtrl-sci physics.comp-ph\n",
            "physics.comp-ph cond-mat.soft physics.chem-ph\n",
            "cs.LG cs.AI cs.SY eess.SY stat.ML\n",
            "physics.chem-ph cond-mat.mtrl-sci cond-mat.other physics.plasm-ph\n",
            "cs.MS physics.data-an physics.ins-det\n",
            "quant-ph cond-mat.mes-hall hep-th\n",
            "cond-mat.stat-mech astro-ph.HE physics.comp-ph\n",
            "astro-ph.SR physics.comp-ph\n",
            "cond-mat.soft nlin.AO nlin.PS\n",
            "cs.CL cs.AI cs.CY cs.LG\n",
            "physics.acc-ph physics.app-ph\n",
            "cs.AI cs.HC cs.LG\n",
            "cs.CL cs.AI cs.MA\n",
            "nlin.CD cond-mat.stat-mech quant-ph\n",
            "cond-mat.mtrl-sci physics.app-ph physics.comp-ph\n",
            "cond-mat.quant-gas cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.stat-mech cond-mat.str-el\n",
            "cs.HC cs.CR\n",
            "physics.app-ph cs.LG\n",
            "physics.flu-dyn cond-mat.dis-nn cond-mat.soft physics.geo-ph\n",
            "cs.RO cs.AI cs.HC\n",
            "hep-ex hep-ph nucl-ex nucl-th physics.ins-det\n",
            "cs.LG cs.CL cs.IR\n",
            "eess.AS cs.CL cs.LG cs.SD stat.ML\n",
            "cs.NI quant-ph\n",
            "cs.CV cs.CR eess.IV\n",
            "cs.LG cs.CR\n",
            "physics.geo-ph cs.LG stat.AP stat.ML\n",
            "cond-mat.mes-hall cond-mat.quant-gas cond-mat.stat-mech\n",
            "quant-ph cs.LG physics.optics stat.ML\n",
            "physics.plasm-ph nlin.PS\n",
            "astro-ph.SR hep-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft physics.app-ph physics.chem-ph\n",
            "cs.LG cs.AR cs.PL\n",
            "astro-ph.EP physics.comp-ph physics.geo-ph physics.optics\n",
            "cs.AI cs.CY cs.HC\n",
            "cs.CL cs.FL cs.LG\n",
            "physics.atom-ph physics.app-ph physics.ins-det\n",
            "eess.IV cs.LG eess.SP\n",
            "cs.MM cs.GR\n",
            "physics.chem-ph cond-mat.str-el physics.atm-clus physics.comp-ph\n",
            "astro-ph.IM cs.DC\n",
            "cs.SD cs.LG eess.AS quant-ph\n",
            "cs.LG cs.NE\n",
            "cs.NI cs.GT cs.LG cs.MA\n",
            "cs.SD cs.HC cs.LG eess.AS\n",
            "q-fin.RM q-fin.CP q-fin.ST\n",
            "cond-mat.soft physics.bio-ph q-bio.SC\n",
            "cs.AI cs.CL\n",
            "cs.LG q-bio.MN stat.AP stat.ML\n",
            "q-bio.PE q-bio.MN q-bio.QM\n",
            "cs.AI cs.LG physics.flu-dyn physics.soc-ph\n",
            "hep-ph astro-ph.SR\n",
            "physics.chem-ph nucl-th\n",
            "astro-ph.EP physics.chem-ph\n",
            "cs.CY cs.MA cs.SE\n",
            "physics.data-an cs.NE\n",
            "cs.SD eess.AS q-bio.NC\n",
            "nlin.PS cond-mat.mes-hall cond-mat.soft physics.chem-ph\n",
            "math.RT math.GR\n",
            "hep-th hep-lat hep-ph quant-ph\n",
            "physics.app-ph cond-mat.mtrl-sci cs.NA math.NA\n",
            "physics.pop-ph gr-qc\n",
            "cs.AI cs.CL cs.IR cs.LG\n",
            "physics.bio-ph physics.med-ph\n",
            "gr-qc astro-ph.CO astro-ph.HE astro-ph.SR\n",
            "cs.NI cs.CY\n",
            "cs.LG cs.AI cs.SY eess.SY\n",
            "q-bio.QM cs.LG q-bio.GN q-bio.PE\n",
            "physics.chem-ph physics.acc-ph physics.app-ph\n",
            "cs.CR cs.AI cs.CV cs.LG\n",
            "quant-ph cond-mat.stat-mech physics.optics\n",
            "cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph physics.data-an\n",
            "cs.MA cs.DC\n",
            "cs.SD cs.CV eess.AS\n",
            "q-bio.NC cs.AI cs.LG cs.RO\n",
            "cond-mat.dis-nn physics.geo-ph\n",
            "quant-ph cond-mat.str-el physics.optics\n",
            "cs.LG cs.DC\n",
            "cs.CV cs.CG cs.LG\n",
            "cs.SI cs.HC\n",
            "cond-mat.mtrl-sci physics.flu-dyn\n",
            "physics.optics cs.NE physics.comp-ph\n",
            "q-fin.TR cs.MA\n",
            "eess.SY cs.CR cs.SY math.OC\n",
            "quant-ph cs.LO math.PR\n",
            "cs.SD eess.AS physics.app-ph\n",
            "cs.GR cs.AI cs.CV cs.LG\n",
            "physics.plasm-ph astro-ph.HE physics.comp-ph\n",
            "math-ph math.DG math.MP quant-ph\n",
            "cs.LG cs.DS\n",
            "q-bio.NC cs.SY eess.SY\n",
            "math.CO math.FA math.NT\n",
            "cs.GR astro-ph.IM math.MG physics.data-an\n",
            "cs.LG cs.CV cs.IT cs.MM math.IT\n",
            "cond-mat.mes-hall physics.atom-ph quant-ph\n",
            "q-fin.MF math.OC math.PR\n",
            "physics.atom-ph hep-ph physics.ins-det physics.plasm-ph quant-ph\n",
            "cs.CR cs.CV\n",
            "cs.LG cs.AI cs.CL\n",
            "cs.DM math.MG q-bio.MN\n",
            "cond-mat.mtrl-sci eess.IV\n",
            "physics.space-ph physics.ao-ph\n",
            "physics.app-ph cond-mat.mes-hall cond-mat.str-el\n",
            "cs.AR cs.LG eess.IV\n",
            "cs.SI cs.GT\n",
            "cs.LG cs.CV q-bio.QM\n",
            "physics.flu-dyn physics.data-an\n",
            "eess.SP cs.LG cs.SD eess.AS\n",
            "cs.LG nlin.CD\n",
            "cs.LG physics.flu-dyn stat.AP\n",
            "physics.class-ph hep-th physics.plasm-ph\n",
            "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.HE astro-ph.SR\n",
            "physics.bio-ph cond-mat.soft physics.flu-dyn physics.med-ph\n",
            "cs.RO physics.app-ph\n",
            "physics.med-ph cs.CV cs.LG\n",
            "physics.gen-ph physics.optics\n",
            "physics.pop-ph astro-ph.IM cs.CY\n",
            "cs.RO cs.HC cs.SY eess.SY\n",
            "cond-mat.soft physics.flu-dyn physics.optics\n",
            "physics.plasm-ph cond-mat.stat-mech\n",
            "cs.RO cs.AI cs.LG cs.MA cs.NE\n",
            "econ.GN physics.soc-ph q-fin.EC stat.AP\n",
            "math.CO math.CA math.FA math.NT\n",
            "astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR physics.chem-ph\n",
            "cond-mat.mes-hall physics.app-ph physics.comp-ph\n",
            "cs.LG cs.AI cs.CV cs.RO\n",
            "cs.ET cond-mat.stat-mech cs.CC cs.NE\n",
            "cs.LO math.CT\n",
            "eess.IV cs.CV physics.ins-det\n",
            "astro-ph.EP nlin.CD\n",
            "cs.LG q-bio.BM\n",
            "astro-ph.EP physics.ao-ph physics.flu-dyn physics.geo-ph\n",
            "cs.CY cs.DB\n",
            "physics.chem-ph cond-mat.mtrl-sci cs.LG\n",
            "cs.LG cond-mat.mtrl-sci physics.chem-ph\n",
            "physics.atm-clus cond-mat.quant-gas physics.chem-ph\n",
            "cs.LG cs.CL cs.CV\n",
            "astro-ph.EP astro-ph.IM physics.chem-ph physics.space-ph\n",
            "physics.flu-dyn math.DS nlin.CD\n",
            "eess.SP cs.AI cs.CV cs.LG cs.RO\n",
            "cond-mat.mes-hall nlin.CD quant-ph\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph q-bio.BM\n",
            "astro-ph.EP physics.class-ph\n",
            "q-bio.QM cs.LG cs.SY eess.SY\n",
            "astro-ph.EP astro-ph.IM physics.flu-dyn\n",
            "eess.SP physics.med-ph\n",
            "eess.IV cs.CV cs.PF\n",
            "physics.soc-ph physics.app-ph\n",
            "cs.LG cs.RO\n",
            "cs.NI cs.OS\n",
            "physics.flu-dyn astro-ph.EP\n",
            "physics.atm-clus cond-mat.quant-gas physics.atom-ph\n",
            "cs.RO cs.CC cs.MA\n",
            "cs.AI cs.LG cs.NI cs.SE\n",
            "astro-ph.EP astro-ph.HE astro-ph.IM astro-ph.SR\n",
            "cs.RO cs.SE\n",
            "cond-mat.supr-con cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el quant-ph\n",
            "cond-mat.dis-nn cond-mat.mtrl-sci cond-mat.soft cond-mat.stat-mech\n",
            "cs.LG cs.AR\n",
            "astro-ph.EP math.OC\n",
            "physics.chem-ph physics.plasm-ph\n",
            "cond-mat.soft cond-mat.mtrl-sci physics.bio-ph physics.flu-dyn\n",
            "cs.NE cs.NI\n",
            "cond-mat.supr-con cond-mat.mtrl-sci physics.ins-det\n",
            "cs.SD cs.DB cs.IR cs.MM eess.AS\n",
            "physics.plasm-ph cond-mat.stat-mech physics.ao-ph\n",
            "cond-mat.dis-nn cond-mat.stat-mech physics.bio-ph physics.comp-ph quant-ph\n",
            "cs.LG cs.DS math.CO\n",
            "physics.atom-ph cond-mat.quant-gas nlin.PS\n",
            "cs.MM cs.CR\n",
            "astro-ph.EP astro-ph.HE astro-ph.SR physics.space-ph\n",
            "eess.SY cs.AR cs.SY eess.SP\n",
            "cs.LO cs.AI cs.RO\n",
            "cs.FL cs.SC\n",
            "cs.LG cs.AI math.ST stat.ML stat.TH\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci physics.app-ph quant-ph\n",
            "astro-ph.IM quant-ph\n",
            "cond-mat.mtrl-sci cond-mat.stat-mech physics.chem-ph physics.class-ph physics.comp-ph\n",
            "physics.plasm-ph physics.ins-det\n",
            "physics.ao-ph cs.LG\n",
            "math.DS nlin.AO physics.chem-ph\n",
            "physics.atom-ph cond-mat.quant-gas nucl-th\n",
            "physics.plasm-ph astro-ph.IM cond-mat.stat-mech physics.space-ph\n",
            "cond-mat.mes-hall cond-mat.str-el nlin.PS nlin.SI physics.optics\n",
            "cs.MM cs.LG\n",
            "cs.CV cs.DC\n",
            "cond-mat.supr-con cond-mat.mtrl-sci cond-mat.other\n",
            "cond-mat.soft cs.NA cs.SY eess.SY math.DG math.NA\n",
            "cs.LG stat.AP\n",
            "cs.DC cs.MM\n",
            "physics.flu-dyn cond-mat.soft nlin.PS\n",
            "cs.LG cs.AI math.OC q-fin.ST\n",
            "cs.AI cs.DS\n",
            "astro-ph.IM astro-ph.HE physics.ins-det\n",
            "astro-ph.IM cond-mat.supr-con\n",
            "cs.CY cs.AI cs.DC\n",
            "cond-mat.stat-mech physics.chem-ph quant-ph\n",
            "stat.ML cs.AI cs.LG q-bio.GN\n",
            "stat.ME q-bio.GN q-bio.QM stat.AP stat.ML\n",
            "physics.hist-ph physics.pop-ph\n",
            "cond-mat.str-el cond-mat.supr-con physics.comp-ph\n",
            "cs.CR physics.optics quant-ph\n",
            "q-fin.GN econ.GN q-fin.EC\n",
            "math-ph math.AG math.MP\n",
            "eess.SP cs.ET\n",
            "physics.space-ph astro-ph.EP physics.ao-ph physics.comp-ph physics.flu-dyn\n",
            "cs.CV cs.AI cs.CL\n",
            "physics.class-ph eess.SP\n",
            "physics.optics cond-mat.stat-mech nlin.PS\n",
            "astro-ph.GA astro-ph.SR gr-qc\n",
            "cs.MM cs.AI cs.CV\n",
            "cs.RO cs.CV cs.HC cs.LG\n",
            "cs.HC cs.CY cs.MM cs.SI\n",
            "math.PR math.RA\n",
            "cs.CR math.ST stat.TH\n",
            "astro-ph.IM astro-ph.EP physics.geo-ph physics.space-ph\n",
            "cs.MA cs.LG cs.PL\n",
            "astro-ph.IM physics.data-an\n",
            "nucl-ex cond-mat.mtrl-sci physics.ins-det\n",
            "cs.CR cs.CY cs.SY eess.SY\n",
            "math.AP cs.NA math.NA\n",
            "math.DS math-ph math.MP\n",
            "cs.HC cs.CY cs.MM\n",
            "stat.AP physics.app-ph physics.data-an\n",
            "cs.CY cs.CL cs.LG q-bio.QM\n",
            "cs.IR cs.HC cs.LG\n",
            "cs.DC cs.AI\n",
            "physics.pop-ph astro-ph.CO hep-ph\n",
            "q-bio.QM cs.AI cs.LG\n",
            "cs.PL cs.DS\n",
            "astro-ph.IM astro-ph.CO gr-qc hep-ex hep-ph\n",
            "gr-qc astro-ph.SR hep-th\n",
            "cs.LG cs.SI\n",
            "astro-ph.IM astro-ph.GA astro-ph.SR physics.atom-ph\n",
            "cs.DL cs.IR cs.LG\n",
            "eess.IV cs.CC cs.CV cs.LG cs.MM\n",
            "cs.CV physics.med-ph\n",
            "physics.comp-ph cs.ET cs.NA math.NA\n",
            "math.HO math.DS\n",
            "cs.SI cs.MA cs.SY eess.SY math.DS physics.soc-ph\n",
            "nlin.AO physics.bio-ph q-bio.PE\n",
            "eess.SP cs.LG eess.IV\n",
            "eess.IV cs.AI cs.LG\n",
            "cs.CV cs.AI cs.LG cs.MM cs.NE\n",
            "physics.bio-ph cond-mat.stat-mech physics.data-an\n",
            "cs.AR cs.DC\n",
            "hep-ph physics.atom-ph physics.class-ph\n",
            "cs.CY math.OC\n",
            "physics.app-ph cs.SY eess.SY physics.optics\n",
            "q-bio.MN cs.LG q-bio.BM q-bio.GN\n",
            "cs.AI cs.RO cs.SE\n",
            "cs.OH cs.AI\n",
            "cs.NI q-bio.MN\n",
            "q-fin.PR\n",
            "physics.chem-ph astro-ph.SR\n",
            "cs.RO cs.AI cs.CV cs.SY eess.SY\n",
            "cond-mat.soft cond-mat.dis-nn cond-mat.mes-hall cond-mat.mtrl-sci physics.flu-dyn\n",
            "physics.plasm-ph physics.class-ph\n",
            "physics.pop-ph physics.hist-ph\n",
            "cs.LG cs.SY eess.SY math.OC\n",
            "cs.SE cs.DC\n",
            "cs.DB cs.LG cs.PF\n",
            "nlin.CD math-ph math.MP\n",
            "cs.CV cs.AI cs.HC\n",
            "physics.med-ph cs.CV cs.GR cs.RO\n",
            "q-bio.PE cs.GT physics.soc-ph\n",
            "physics.hist-ph physics.soc-ph\n",
            "math.OC cs.GT\n",
            "cs.SD cs.DL eess.AS\n",
            "cs.AI cs.MM\n",
            "astro-ph.HE hep-ex physics.space-ph\n",
            "stat.AP cs.AI\n",
            "cs.SD eess.AS math.CO\n",
            "cs.CV cs.AR eess.IV\n",
            "math.NT math.CO math.CV\n",
            "cs.SI cs.DL physics.data-an\n",
            "cs.DC cs.RO\n",
            "math.CT math.GR\n",
            "cs.LG cs.AI cs.SI\n",
            "eess.SY cs.CR cs.SY\n",
            "cs.SI cs.CL physics.soc-ph\n",
            "math.OC physics.soc-ph\n",
            "physics.ao-ph astro-ph.EP physics.geo-ph\n",
            "cs.CL cs.SI econ.GN physics.soc-ph q-fin.EC\n",
            "physics.pop-ph astro-ph.SR\n",
            "cs.SI econ.GN q-fin.EC\n",
            "cs.LG cs.AI stat.AP\n",
            "cs.CL cs.NE\n",
            "physics.data-an physics.bio-ph q-bio.QM stat.ME\n",
            "cs.CR cs.DS\n",
            "cs.NI cs.CY cs.PF\n",
            "physics.pop-ph astro-ph.SR physics.hist-ph\n",
            "physics.optics nlin.CD physics.app-ph\n",
            "cs.CV cs.AI stat.ML\n",
            "cs.AI q-bio.QM\n",
            "cs.AI cs.DC cs.LO cs.NI\n",
            "math.AG cs.IT math.IT\n",
            "quant-ph physics.atom-ph physics.comp-ph\n",
            "stat.ML cs.LG cs.SI\n",
            "nlin.AO math.DS nlin.CD\n",
            "q-bio.NC eess.SP\n",
            "eess.SP cs.CR\n",
            "cs.GT cs.LG\n",
            "stat.CO cs.AI cs.NA eess.SP math.NA\n",
            "math.LO math.PR\n",
            "q-fin.ST cs.LG stat.ML\n",
            "cs.CL cs.CY cs.LG\n",
            "math.NA cs.CE cs.NA physics.flu-dyn\n",
            "cond-mat.mtrl-sci physics.app-ph physics.optics physics.plasm-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.str-el physics.app-ph\n",
            "q-bio.SC q-bio.CB\n",
            "cs.HC cs.CY cs.GT\n",
            "physics.geo-ph cond-mat.mtrl-sci physics.app-ph\n",
            "cond-mat.mes-hall cond-mat.dis-nn cond-mat.mtrl-sci physics.app-ph quant-ph\n",
            "cs.IR cs.SI econ.GN q-fin.EC\n",
            "eess.SP cs.LG cs.NI\n",
            "eess.SY cs.AI cs.SY\n",
            "physics.ed-ph cs.GR cs.HC math.HO\n",
            "nlin.AO nlin.PS physics.app-ph physics.chem-ph\n",
            "cs.PF cs.DC cs.NI\n",
            "physics.soc-ph cond-mat.stat-mech q-bio.QM\n",
            "cs.LG cs.DC cs.NE\n",
            "cs.RO cs.DS cs.MA\n",
            "cs.SE cs.AI cs.RO cs.SY eess.SY\n",
            "cs.LG cs.AI cs.CV cs.NE\n",
            "cs.PF cs.IR\n",
            "q-bio.QM cs.LG eess.SP\n",
            "stat.ML cs.LG stat.AP\n",
            "cs.SE cs.DB\n",
            "physics.ins-det eess.SP\n",
            "math.CT math.AT math.KT\n",
            "cs.GT cs.AI\n",
            "cond-mat.quant-gas cond-mat.supr-con physics.flu-dyn quant-ph\n",
            "cs.LG cs.AI cs.CV\n",
            "astro-ph.IM astro-ph.GA physics.atom-ph\n",
            "cs.LG cs.CE cs.SY eess.SY\n",
            "cs.DB cs.FL\n",
            "cs.CE cs.SY eess.SY\n",
            "cs.RO cs.AR\n",
            "physics.med-ph cs.CV eess.IV\n",
            "physics.optics physics.class-ph physics.pop-ph\n",
            "cs.CV cs.LG cs.NE cs.RO\n",
            "astro-ph.IM cond-mat.supr-con physics.ins-det\n",
            "cs.LG cs.CY physics.geo-ph\n",
            "cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.soft\n",
            "cs.SE cs.AI cs.LG\n",
            "eess.SP cs.LG cs.SY eess.SY stat.AP\n",
            "eess.SP cs.NA math.NA physics.app-ph\n",
            "physics.pop-ph nlin.PS physics.ed-ph\n",
            "cond-mat.mes-hall cond-mat.mtrl-sci math-ph math.MP physics.comp-ph\n",
            "eess.SP cs.NA cs.SY eess.SY math.NA physics.ins-det stat.ME\n",
            "eess.SP cs.NA cs.SY eess.SY math.NA stat.ME\n",
            "stat.ME cs.CV eess.IV physics.data-an stat.AP\n",
            "astro-ph.IM physics.ao-ph physics.ins-det\n",
            "eess.SY cs.AR cs.SY\n",
            "cs.AR cs.DC cs.SY eess.SY\n",
            "cs.LG cs.AI cs.CY\n",
            "cond-mat.soft cond-mat.stat-mech physics.bio-ph physics.comp-ph\n",
            "cs.AI cs.CR cs.LG\n",
            "math.AG math.KT\n",
            "stat.ME q-bio.GN stat.AP\n",
            "physics.ed-ph cs.RO\n",
            "eess.SP physics.app-ph physics.optics\n",
            "cs.RO cs.MA cs.SY eess.SY\n",
            "eess.SY cs.NI cs.SY eess.SP\n",
            "astro-ph.GA astro-ph.SR physics.flu-dyn\n",
            "physics.chem-ph cond-mat.stat-mech physics.comp-ph\n",
            "cs.CL stat.ML\n",
            "cs.AI cs.IT cs.RO cs.SY eess.SY math.IT\n",
            "nlin.CD math.PR physics.chem-ph\n",
            "eess.SP physics.med-ph physics.optics\n",
            "cs.CV cs.AI cs.CL cs.LG\n",
            "cs.IT cs.AI math.IT\n",
            "cs.DL cs.AI cs.CL cs.LG\n",
            "gr-qc astro-ph hep-th\n",
            "math.GT math.AT\n",
            "nlin.SI hep-th quant-ph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_df = df[df['categories'].str.contains(\"cs.AI\")]\n",
        "\n",
        "sentencesList= ai_df['abstract'].tolist()"
      ],
      "metadata": {
        "id": "M2x1SyFdnvC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ai_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJsjkaxhnDmk",
        "outputId": "f3855199-86f0-4bf5-d75a-652253171af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentencesList[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvNlPhHLnDrP",
        "outputId": "3d72f276-706b-44e0-97dd-1a08be89ebcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Markov models lie at the interface between statistical independence in a\n",
            "probability distribution and graph separation properties. We review model\n",
            "selection and estimation in directed and undirected Markov models with Gaussian\n",
            "parametrization, emphasizing the main similarities and differences. These two\n",
            "model classes are similar but not equivalent, although they share a common\n",
            "intersection. We present the existing results from a historical perspective,\n",
            "taking into account the amount of literature existing from both the artificial\n",
            "intelligence and statistics research communities, where these models were\n",
            "originated. We cover classical topics such as maximum likelihood estimation and\n",
            "model selection via hypothesis testing, but also more modern approaches like\n",
            "regularization and Bayesian methods. We also discuss how the Markov models\n",
            "reviewed fit in the rich hierarchy of other, higher level Markov model classes.\n",
            "Finally, we close the paper overviewing relaxations of the Gaussian assumption\n",
            "and pointing out the main areas of application where these Markov models are\n",
            "nowadays used.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_df.iloc[1]['id']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-tLNcS0CoHnh",
        "outputId": "98766308-2137-4cf4-cd0d-0daf020bb05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1611.10351'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "embeddings = model.encode(sentencesList, convert_to_tensor=True)\n",
        "end_time = time.time()\n",
        "print(\"Time for computing embeddings:\"+ str(end_time-start_time) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLGPqV4koPK5",
        "outputId": "4bf4bacd-176d-48b9-fc0f-93c39d5b7f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for computing embeddings:1.5948326587677002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S91LRt_noX8D",
        "outputId": "6dd21400-e4b9-4c13-c9cf-5a66cda364fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([425, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)"
      ],
      "metadata": {
        "id": "zCCfQc7VofSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for scores in cosine_scores:\n",
        "    scores[count]=0.0\n",
        "    max_elements, max_indices = torch.max(scores,dim=0)\n",
        "    max_index = max_indices.item()\n",
        "    print(\"\\n*********\\n\")\n",
        "    print(\"**Paper Id :\"+ai_df.iloc[count]['id']+' '+'\\nTitle :'+ai_df.iloc[count]['title']\n",
        "          +'\\n'+ai_df.iloc[count]['abstract']+\n",
        "          '\\n**Paper Id :' +ai_df.iloc[max_index]['id']+' '+'\\nTitle :'+\n",
        "          ai_df.iloc[max_index]['title']+'\\n'+\n",
        "          ai_df.iloc[max_index]['abstract'])\n",
        "    count =count+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZoTc2F9ohIU",
        "outputId": "00184b5a-9c0d-4edc-f306-52fb63e662a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "over strong baselines by 4.4% execution accuracy. Oracle experiments suggest\n",
            "that annotated alignments can support further accuracy gains of up to 23.9%.\n",
            "\n",
            "**Paper Id :2004.07633 \n",
            "Title :A Methodology for Creating Question Answering Corpora Using Inverse Data\n",
            "  Annotation\n",
            "  In this paper, we introduce a novel methodology to efficiently construct a\n",
            "corpus for question answering over structured data. For this, we introduce an\n",
            "intermediate representation that is based on the logical query plan in a\n",
            "database called Operation Trees (OT). This representation allows us to invert\n",
            "the annotation process without losing flexibility in the types of queries that\n",
            "we generate. Furthermore, it allows for fine-grained alignment of query tokens\n",
            "to OT operations. In our method, we randomly generate OTs from a context-free\n",
            "grammar. Afterwards, annotators have to write the appropriate natural language\n",
            "question that is represented by the OT. Finally, the annotators assign the\n",
            "tokens to the OT operations. We apply the method to create a new corpus OTTA\n",
            "(Operation Trees and Token Assignment), a large semantic parsing corpus for\n",
            "evaluating natural language interfaces to databases. We compare OTTA to Spider\n",
            "and LC-QuaD 2.0 and show that our methodology more than triples the annotation\n",
            "speed while maintaining the complexity of the queries. Finally, we train a\n",
            "state-of-the-art semantic parsing model on our data and show that our corpus is\n",
            "a challenging dataset and that the token alignment can be leveraged to increase\n",
            "the performance significantly.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.11370 \n",
            "Title :A Multi-Componential Approach to Emotion Recognition and the Effect of\n",
            "  Personality\n",
            "  Emotions are an inseparable part of human nature affecting our behavior in\n",
            "response to the outside world. Although most empirical studies have been\n",
            "dominated by two theoretical models including discrete categories of emotion\n",
            "and dichotomous dimensions, results from neuroscience approaches suggest a\n",
            "multi-processes mechanism underpinning emotional experience with a large\n",
            "overlap across different emotions. While these findings are consistent with the\n",
            "influential theories of emotion in psychology that emphasize a role for\n",
            "multiple component processes to generate emotion episodes, few studies have\n",
            "systematically investigated the relationship between discrete emotions and a\n",
            "full componential view. This paper applies a componential framework with a\n",
            "data-driven approach to characterize emotional experiences evoked during movie\n",
            "watching. The results suggest that differences between various emotions can be\n",
            "captured by a few (at least 6) latent dimensions, each defined by features\n",
            "associated with component processes, including appraisal, expression,\n",
            "physiology, motivation, and feeling. In addition, the link between discrete\n",
            "emotions and component model is explored and results show that a componential\n",
            "model with a limited number of descriptors is still able to predict the level\n",
            "of experienced discrete emotion(s) to a satisfactory level. Finally, as\n",
            "appraisals may vary according to individual dispositions and biases, we also\n",
            "study the relationship between personality traits and emotions in our\n",
            "computational framework and show that the role of personality on discrete\n",
            "emotion differences can be better justified using the component model.\n",
            "\n",
            "**Paper Id :2105.00375 \n",
            "Title :Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary\n",
            "  Results\n",
            "  Given an on-board diagnostics (OBD) dataset and a physics-based emissions\n",
            "prediction model, this paper aims to develop an accurate and\n",
            "computational-efficient AI (Artificial Intelligence) method that predicts\n",
            "vehicle emissions. The problem is of societal importance because vehicular\n",
            "emissions lead to climate change and impact human health. This problem is\n",
            "challenging because the OBD data does not contain enough parameters needed by\n",
            "high-order physics models. Conversely, related work has shown that low-order\n",
            "physics models have poor predictive accuracy when using available OBD data.\n",
            "This paper uses a divergent window co-occurrence pattern detection method to\n",
            "develop a spatiotemporal variability-aware AI model for predicting emission\n",
            "values from the OBD datasets. We conducted a case study using real-world OBD\n",
            "data from a local public transportation agency. Results show that the proposed\n",
            "AI method has approximately 65% improved predictive accuracy than a non-AI\n",
            "low-order physics model and is approximately 35% more accurate than a baseline\n",
            "model.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.13864 \n",
            "Title :Diptychs of human and machine perceptions\n",
            "  We propose visual creations that put differences in algorithms and humans\n",
            "\\emph{perceptions} into perspective. We exploit saliency maps of neural\n",
            "networks and visual focus of humans to create diptychs that are\n",
            "reinterpretations of an original image according to both machine and human\n",
            "attentions. Using those diptychs as a qualitative evaluation of perception, we\n",
            "discuss some crucial issues of current \\textit{task-oriented} artificial\n",
            "intelligence.\n",
            "\n",
            "**Paper Id :2009.06295 \n",
            "Title :Deep intrinsic decomposition trained on surreal scenes yet with\n",
            "  realistic light effects\n",
            "  Estimation of intrinsic images still remains a challenging task due to\n",
            "weaknesses of ground-truth datasets, which either are too small or present\n",
            "non-realistic issues. On the other hand, end-to-end deep learning architectures\n",
            "start to achieve interesting results that we believe could be improved if\n",
            "important physical hints were not ignored. In this work, we present a twofold\n",
            "framework: (a) a flexible generation of images overcoming some classical\n",
            "dataset problems such as larger size jointly with coherent lighting appearance;\n",
            "and (b) a flexible architecture tying physical properties through intrinsic\n",
            "losses. Our proposal is versatile, presents low computation time, and achieves\n",
            "state-of-the-art results.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.15149 \n",
            "Title :Detecting Stance in Media on Global Warming\n",
            "  Citing opinions is a powerful yet understudied strategy in argumentation. For\n",
            "example, an environmental activist might say, \"Leading scientists agree that\n",
            "global warming is a serious concern,\" framing a clause which affirms their own\n",
            "stance (\"that global warming is serious\") as an opinion endorsed (\"[scientists]\n",
            "agree\") by a reputable source (\"leading\"). In contrast, a global warming denier\n",
            "might frame the same clause as the opinion of an untrustworthy source with a\n",
            "predicate connoting doubt: \"Mistaken scientists claim [...].\" Our work studies\n",
            "opinion-framing in the global warming (GW) debate, an increasingly partisan\n",
            "issue that has received little attention in NLP. We introduce Global Warming\n",
            "Stance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a\n",
            "BERT classifier to study novel aspects of argumentation in how different sides\n",
            "of a debate represent their own and each other's opinions. From 56K news\n",
            "articles, we find that similar linguistic devices for self-affirming and\n",
            "opponent-doubting discourse are used across GW-accepting and skeptic media,\n",
            "though GW-skeptical media shows more opponent-doubt. We also find that authors\n",
            "often characterize sources as hypocritical, by ascribing opinions expressing\n",
            "the author's own view to source entities known to publicly endorse the opposing\n",
            "view. We release our stance dataset, model, and lexicons of framing devices for\n",
            "future work on opinion-framing and the automatic detection of GW stance.\n",
            "\n",
            "**Paper Id :2308.01899 \n",
            "Title :How many preprints have actually been printed and why: a case study of\n",
            "  computer science preprints on arXiv\n",
            "  Preprints play an increasingly critical role in academic communities. There\n",
            "are many reasons driving researchers to post their manuscripts to preprint\n",
            "servers before formal submission to journals or conferences, but the use of\n",
            "preprints has also sparked considerable controversy, especially surrounding the\n",
            "claim of priority. In this paper, a case study of computer science preprints\n",
            "submitted to arXiv from 2008 to 2017 is conducted to quantify how many\n",
            "preprints have eventually been printed in peer-reviewed venues. Among those\n",
            "published manuscripts, some are published under different titles and without an\n",
            "update to their preprints on arXiv. In the case of these manuscripts, the\n",
            "traditional fuzzy matching method is incapable of mapping the preprint to the\n",
            "final published version. In view of this issue, we introduce a semantics-based\n",
            "mapping method with the employment of Bidirectional Encoder Representations\n",
            "from Transformers (BERT). With this new mapping method and a plurality of data\n",
            "sources, we find that 66% of all sampled preprints are published under\n",
            "unchanged titles and 11% are published under different titles and with other\n",
            "modifications. A further analysis was then performed to investigate why these\n",
            "preprints but not others were accepted for publication. Our comparison reveals\n",
            "that in the field of computer science, published preprints feature adequate\n",
            "revisions, multiple authorship, detailed abstract and introduction, extensive\n",
            "and authoritative references and available source code.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.15578 \n",
            "Title :Exploring the Nuances of Designing (with/for) Artificial Intelligence\n",
            "  Solutions relying on artificial intelligence are devised to predict data\n",
            "patterns and answer questions that are clearly defined, involve an enumerable\n",
            "set of solutions, clear rules, and inherently binary decision mechanisms. Yet,\n",
            "as they become exponentially implemented in our daily activities, they begin to\n",
            "transcend these initial boundaries and to affect the larger sociotechnical\n",
            "system in which they are situated. In this arrangement, a solution is under\n",
            "pressure to surpass true or false criteria and move to an ethical evaluation of\n",
            "right and wrong. Neither algorithmic solutions, nor purely humanistic ones will\n",
            "be enough to fully mitigate undesirable outcomes in the narrow state of AI or\n",
            "its future incarnations. We must take a holistic view. In this paper we explore\n",
            "the construct of infrastructure as a means to simultaneously address\n",
            "algorithmic and societal issues when designing AI.\n",
            "\n",
            "**Paper Id :2003.11637 \n",
            "Title :Bio-inspired Optimization: metaheuristic algorithms for optimization\n",
            "  In today's day and time solving real-world complex problems has become\n",
            "fundamentally vital and critical task. Many of these are combinatorial\n",
            "problems, where optimal solutions are sought rather than exact solutions.\n",
            "Traditional optimization methods are found to be effective for small scale\n",
            "problems. However, for real-world large scale problems, traditional methods\n",
            "either do not scale up or fail to obtain optimal solutions or they end-up\n",
            "giving solutions after a long running time. Even earlier artificial\n",
            "intelligence based techniques used to solve these problems could not give\n",
            "acceptable results. However, last two decades have seen many new methods in AI\n",
            "based on the characteristics and behaviors of the living organisms in the\n",
            "nature which are categorized as bio-inspired or nature inspired optimization\n",
            "algorithms. These methods, are also termed meta-heuristic optimization methods,\n",
            "have been proved theoretically and implemented using simulation as well used to\n",
            "create many useful applications. They have been used extensively to solve many\n",
            "industrial and engineering complex problems due to being easy to understand,\n",
            "flexible, simple to adapt to the problem at hand and most importantly their\n",
            "ability to come out of local optima traps. This local optima avoidance property\n",
            "helps in finding global optimal solutions. This paper is aimed at understanding\n",
            "how nature has inspired many optimization algorithms, basic categorization of\n",
            "them, major bio-inspired optimization algorithms invented in recent time with\n",
            "their applications.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.15832 \n",
            "Title :Proceedings 9th International Workshop on Theorem Proving Components for\n",
            "  Educational Software\n",
            "  The 9th International Workshop on Theorem-Proving Components for Educational\n",
            "Software (ThEdu'20) was scheduled to happen on June 29 as a satellite of the\n",
            "IJCAR-FSCD 2020 joint meeting, in Paris. The COVID-19 pandemic came by\n",
            "surprise, though, and the main conference was virtualised. Fearing that an\n",
            "online meeting would not allow our community to fully reproduce the usual\n",
            "face-to-face networking opportunities of the ThEdu initiative, the Steering\n",
            "Committee of ThEdu decided to cancel our workshop. Given that many of us had\n",
            "already planned and worked for that moment, we decided that ThEdu'20 could\n",
            "still live in the form of an EPTCS volume. The EPTCS concurred with us,\n",
            "recognising this very singular situation, and accepted our proposal of\n",
            "organising a special issue with papers submitted to ThEdu'20. An open call for\n",
            "papers was then issued, and attracted five submissions, all of which have been\n",
            "accepted by our reviewers, who produced three careful reports on each of the\n",
            "contributions. The resulting revised papers are collected in the present\n",
            "volume. We, the volume editors, hope that this collection of papers will help\n",
            "further promoting the development of theorem-proving-based software, and that\n",
            "it will collaborate to improve the mutual understanding between computer\n",
            "mathematicians and stakeholders in education. With some luck, we would actually\n",
            "expect that the very special circumstances set up by the worst sanitary crisis\n",
            "in a century will happen to reinforce the need for the application of certified\n",
            "components and of verification methods for the production of educational\n",
            "software that would be available even when the traditional on-site learning\n",
            "experiences turn out not to be recommendable.\n",
            "\n",
            "**Paper Id :2002.11895 \n",
            "Title :Proceedings 8th International Workshop on Theorem Proving Components for\n",
            "  Educational Software\n",
            "  This EPTCS volume contains the proceedings of the ThEdu'19 workshop, promoted\n",
            "on August 25, 2019, as a satellite event of CADE-27, in Natal, Brazil.\n",
            "Representing the eighth installment of the ThEdu series, ThEdu'19 was a vibrant\n",
            "workshop, with an invited talk by Sarah Winkler, four contributions, and the\n",
            "first edition of a Geometry Automated Provers Competition. After the workshop\n",
            "an open call for papers was issued and attracted seven submissions, six of\n",
            "which have been accepted by the reviewers, and collected in the present\n",
            "post-proceedings volume.\n",
            "  The ThEdu series pursues the smooth transition from an intuitive way of doing\n",
            "mathematics at secondary school to a more formal approach to the subject in\n",
            "STEM education, while favoring software support for this transition by\n",
            "exploiting the power of theorem-proving technologies.\n",
            "  The volume editors hope that this collection of papers will further promote\n",
            "the development of theorem-proving-based software, and that it will collaborate\n",
            "on improving mutual understanding between computer mathematicians and\n",
            "stakeholders in education.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2010.16336 \n",
            "Title :Leveraging Extracted Model Adversaries for Improved Black Box Attacks\n",
            "  We present a method for adversarial input generation against black box models\n",
            "for reading comprehension based question answering. Our approach is composed of\n",
            "two steps. First, we approximate a victim black box model via model extraction\n",
            "(Krishna et al., 2020). Second, we use our own white box method to generate\n",
            "input perturbations that cause the approximate model to fail. These perturbed\n",
            "inputs are used against the victim. In experiments we find that our method\n",
            "improves on the efficacy of the AddAny---a white box attack---performed on the\n",
            "approximate model by 25% F1, and the AddSent attack---a black box attack---by\n",
            "11% F1 (Jia and Liang, 2017).\n",
            "\n",
            "**Paper Id :2004.14174 \n",
            "Title :Reevaluating Adversarial Examples in Natural Language\n",
            "  State-of-the-art attacks on NLP models lack a shared definition of a what\n",
            "constitutes a successful attack. We distill ideas from past work into a unified\n",
            "framework: a successful natural language adversarial example is a perturbation\n",
            "that fools the model and follows some linguistic constraints. We then analyze\n",
            "the outputs of two state-of-the-art synonym substitution attacks. We find that\n",
            "their perturbations often do not preserve semantics, and 38% introduce\n",
            "grammatical errors. Human surveys reveal that to successfully preserve\n",
            "semantics, we need to significantly increase the minimum cosine similarities\n",
            "between the embeddings of swapped words and between the sentence encodings of\n",
            "original and perturbed sentences.With constraints adjusted to better preserve\n",
            "semantics and grammaticality, the attack success rate drops by over 70\n",
            "percentage points.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.01397 \n",
            "Title :Guided Navigation from Multiple Viewpoints using Qualitative Spatial\n",
            "  Reasoning\n",
            "  Navigation is an essential ability for mobile agents to be completely\n",
            "autonomous and able to perform complex actions. However, the problem of\n",
            "navigation for agents with limited (or no) perception of the world, or devoid\n",
            "of a fully defined motion model, has received little attention from research in\n",
            "AI and Robotics. One way to tackle this problem is to use guided navigation, in\n",
            "which other autonomous agents, endowed with perception, can combine their\n",
            "distinct viewpoints to infer the localisation and the appropriate commands to\n",
            "guide a sensory deprived agent through a particular path. Due to the limited\n",
            "knowledge about the physical and perceptual characteristics of the guided\n",
            "agent, this task should be conducted on a level of abstraction allowing the use\n",
            "of a generic motion model, and high-level commands, that can be applied by any\n",
            "type of autonomous agents, including humans. The main task considered in this\n",
            "work is, given a group of autonomous agents perceiving their common environment\n",
            "with their independent, egocentric and local vision sensors, the development\n",
            "and evaluation of algorithms capable of producing a set of high-level commands\n",
            "(involving qualitative directions: e.g. move left, go straight ahead) capable\n",
            "of guiding a sensory deprived robot to a goal location.\n",
            "\n",
            "**Paper Id :1903.00401 \n",
            "Title :Learning To Follow Directions in Street View\n",
            "  Navigating and understanding the real world remains a key challenge in\n",
            "machine learning and inspires a great variety of research in areas such as\n",
            "language grounding, planning, navigation and computer vision. We propose an\n",
            "instruction-following task that requires all of the above, and which combines\n",
            "the practicality of simulated environments with the challenges of ambiguous,\n",
            "noisy real world data. StreetNav is built on top of Google Street View and\n",
            "provides visually accurate environments representing real places. Agents are\n",
            "given driving instructions which they must learn to interpret in order to\n",
            "successfully navigate in this environment. Since humans equipped with driving\n",
            "instructions can readily navigate in previously unseen cities, we set a high\n",
            "bar and test our trained agents for similar cognitive capabilities. Although\n",
            "deep reinforcement learning (RL) methods are frequently evaluated only on data\n",
            "that closely follow the training distribution, our dataset extends to multiple\n",
            "cities and has a clean train/test separation. This allows for thorough testing\n",
            "of generalisation ability. This paper presents the StreetNav environment and\n",
            "tasks, models that establish strong baselines, and extensive analysis of the\n",
            "task and the trained agents.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.01542 \n",
            "Title :Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space\n",
            "  Entropy Search Approach\n",
            "  We study the novel problem of blackbox optimization of multiple objectives\n",
            "via multi-fidelity function evaluations that vary in the amount of resources\n",
            "consumed and their accuracy. The overall goal is to approximate the true Pareto\n",
            "set of solutions by minimizing the resources consumed for function evaluations.\n",
            "For example, in power system design optimization, we need to find designs that\n",
            "trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity\n",
            "simulators for design evaluations. In this paper, we propose a novel approach\n",
            "referred as Multi-Fidelity Output Space Entropy Search for Multi-objective\n",
            "Optimization (MF-OSEMO) to solve this problem. The key idea is to select the\n",
            "sequence of candidate input and fidelity-vector pairs that maximize the\n",
            "information gained about the true Pareto front per unit resource cost. Our\n",
            "experiments on several synthetic and real-world benchmark problems show that\n",
            "MF-OSEMO, with both approximations, significantly improves over the\n",
            "state-of-the-art single-fidelity algorithms for multi-objective optimization.\n",
            "\n",
            "**Paper Id :2009.01721 \n",
            "Title :Max-value Entropy Search for Multi-Objective Bayesian Optimization with\n",
            "  Constraints\n",
            "  We consider the problem of constrained multi-objective blackbox optimization\n",
            "using expensive function evaluations, where the goal is to approximate the true\n",
            "Pareto set of solutions satisfying a set of constraints while minimizing the\n",
            "number of function evaluations. For example, in aviation power system design\n",
            "applications, we need to find the designs that trade-off total energy and the\n",
            "mass while satisfying specific thresholds for motor temperature and voltage of\n",
            "cells. This optimization requires performing expensive computational\n",
            "simulations to evaluate designs. In this paper, we propose a new approach\n",
            "referred as {\\em Max-value Entropy Search for Multi-objective Optimization with\n",
            "Constraints (MESMOC)} to solve this problem. MESMOC employs an output-space\n",
            "entropy based acquisition function to efficiently select the sequence of inputs\n",
            "for evaluation to uncover high-quality pareto-set solutions while satisfying\n",
            "constraints.\n",
            "  We apply MESMOC to two real-world engineering design applications to\n",
            "demonstrate its effectiveness over state-of-the-art algorithms.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.04016 \n",
            "Title :Provenance-Based Interpretation of Multi-Agent Information Analysis\n",
            "  Analytic software tools and workflows are increasing in capability,\n",
            "complexity, number, and scale, and the integrity of our workflows is as\n",
            "important as ever. Specifically, we must be able to inspect the process of\n",
            "analytic workflows to assess (1) confidence of the conclusions, (2) risks and\n",
            "biases of the operations involved, (3) sensitivity of the conclusions to\n",
            "sources and agents, (4) impact and pertinence of various sources and agents,\n",
            "and (5) diversity of the sources that support the conclusions. We present an\n",
            "approach that tracks agents' provenance with PROV-O in conjunction with agents'\n",
            "appraisals and evidence links (expressed in our novel DIVE ontology). Together,\n",
            "PROV-O and DIVE enable dynamic propagation of confidence and counter-factual\n",
            "refutation to improve human-machine trust and analytic integrity. We\n",
            "demonstrate representative software developed for user interaction with that\n",
            "provenance, and discuss key needs for organizations adopting such approaches.\n",
            "We demonstrate all of these assessments in a multi-agent analysis scenario,\n",
            "using an interactive web-based information validation UI.\n",
            "\n",
            "**Paper Id :1806.03517 \n",
            "Title :A Taxonomy of Network Threats and the Effect of Current Datasets on\n",
            "  Intrusion Detection Systems\n",
            "  As the world moves towards being increasingly dependent on computers and\n",
            "automation, building secure applications, systems and networks are some of the\n",
            "main challenges faced in the current decade. The number of threats that\n",
            "individuals and businesses face is rising exponentially due to the increasing\n",
            "complexity of networks and services of modern networks. To alleviate the impact\n",
            "of these threats, researchers have proposed numerous solutions for anomaly\n",
            "detection; however, current tools often fail to adapt to ever-changing\n",
            "architectures, associated threats and zero-day attacks. This manuscript aims to\n",
            "pinpoint research gaps and shortcomings of current datasets, their impact on\n",
            "building Network Intrusion Detection Systems (NIDS) and the growing number of\n",
            "sophisticated threats. To this end, this manuscript provides researchers with\n",
            "two key pieces of information; a survey of prominent datasets, analyzing their\n",
            "use and impact on the development of the past decade's Intrusion Detection\n",
            "Systems (IDS) and a taxonomy of network threats and associated tools to carry\n",
            "out these attacks. The manuscript highlights that current IDS research covers\n",
            "only 33.3% of our threat taxonomy. Current datasets demonstrate a clear lack of\n",
            "real-network threats, attack representation and include a large number of\n",
            "deprecated threats, which together limit the detection accuracy of current\n",
            "machine learning IDS approaches. The unique combination of the taxonomy and the\n",
            "analysis of the datasets provided in this manuscript aims to improve the\n",
            "creation of datasets and the collection of real-world data. As a result, this\n",
            "will improve the efficiency of the next generation IDS and reflect network\n",
            "threats more accurately within new datasets.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.04044 \n",
            "Title :Exploring End-to-End Differentiable Natural Logic Modeling\n",
            "  We explore end-to-end trained differentiable models that integrate natural\n",
            "logic with neural networks, aiming to keep the backbone of natural language\n",
            "reasoning based on the natural logic formalism while introducing subsymbolic\n",
            "vector representations and neural components. The proposed model adapts module\n",
            "networks to model natural logic operations, which is enhanced with a memory\n",
            "component to model contextual information. Experiments show that the proposed\n",
            "framework can effectively model monotonicity-based reasoning, compared to the\n",
            "baseline neural network models without built-in inductive bias for\n",
            "monotonicity-based reasoning. Our proposed model shows to be robust when\n",
            "transferred from upward to downward inference. We perform further analyses on\n",
            "the performance of the proposed model on aggregation, showing the effectiveness\n",
            "of the proposed subcomponents on helping achieve better intermediate\n",
            "aggregation performance.\n",
            "\n",
            "**Paper Id :1910.02486 \n",
            "Title :Interpretable neural networks based on continuous-valued logic and\n",
            "  multicriteria decision operators\n",
            "  Combining neural networks with continuous logic and multicriteria decision\n",
            "making tools can reduce the black box nature of neural models. In this study,\n",
            "we show that nilpotent logical systems offer an appropriate mathematical\n",
            "framework for a hybridization of continuous nilpotent logic and neural models,\n",
            "helping to improve the interpretability and safety of machine learning. In our\n",
            "concept, perceptrons model soft inequalities; namely membership functions and\n",
            "continuous logical operators. We design the network architecture before\n",
            "training, using continuous logical operators and multicriteria decision tools\n",
            "with given weights working in the hidden layers. Designing the structure\n",
            "appropriately leads to a drastic reduction in the number of parameters to be\n",
            "learned. The theoretical basis offers a straightforward choice of activation\n",
            "functions (the cutting function or its differentiable approximation, the\n",
            "squashing function), and also suggests an explanation to the great success of\n",
            "the rectified linear unit (ReLU). In this study, we focus on the architecture\n",
            "of a hybrid model and introduce the building blocks for future application in\n",
            "deep neural networks. The concept is illustrated with some toy examples taken\n",
            "from an extended version of the tensorflow playground.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.04767 \n",
            "Title :An Analysis of Dataset Overlap on Winograd-Style Tasks\n",
            "  The Winograd Schema Challenge (WSC) and variants inspired by it have become\n",
            "important benchmarks for common-sense reasoning (CSR). Model performance on the\n",
            "WSC has quickly progressed from chance-level to near-human using neural\n",
            "language models trained on massive corpora. In this paper, we analyze the\n",
            "effects of varying degrees of overlap between these training corpora and the\n",
            "test instances in WSC-style tasks. We find that a large number of test\n",
            "instances overlap considerably with the corpora on which state-of-the-art\n",
            "models are (pre)trained, and that a significant drop in classification accuracy\n",
            "occurs when we evaluate models on instances with minimal overlap. Based on\n",
            "these results, we develop the KnowRef-60K dataset, which consists of over 60k\n",
            "pronoun disambiguation problems scraped from web data. KnowRef-60K is the\n",
            "largest corpus to date for WSC-style common-sense reasoning and exhibits a\n",
            "significantly lower proportion of overlaps with current pretraining corpora.\n",
            "\n",
            "**Paper Id :1912.00667 \n",
            "Title :A Human-AI Loop Approach for Joint Keyword Discovery and Expectation\n",
            "  Estimation in Micropost Event Detection\n",
            "  Microblogging platforms such as Twitter are increasingly being used in event\n",
            "detection. Existing approaches mainly use machine learning models and rely on\n",
            "event-related keywords to collect the data for model training. These approaches\n",
            "make strong assumptions on the distribution of the relevant micro-posts\n",
            "containing the keyword -- referred to as the expectation of the distribution --\n",
            "and use it as a posterior regularization parameter during model training. Such\n",
            "approaches are, however, limited as they fail to reliably estimate the\n",
            "informativeness of a keyword and its expectation for model training. This paper\n",
            "introduces a Human-AI loop approach to jointly discover informative keywords\n",
            "for model training while estimating their expectation. Our approach iteratively\n",
            "leverages the crowd to estimate both keyword specific expectation and the\n",
            "disagreement between the crowd and the model in order to discover new keywords\n",
            "that are most beneficial for model training. These keywords and their\n",
            "expectation not only improve the resulting performance but also make the model\n",
            "training process more transparent. We empirically demonstrate the merits of our\n",
            "approach, both in terms of accuracy and interpretability, on multiple\n",
            "real-world datasets and show that our approach improves the state of the art by\n",
            "24.3%.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.04999 \n",
            "Title :Untangling Dense Knots by Learning Task-Relevant Keypoints\n",
            "  Untangling ropes, wires, and cables is a challenging task for robots due to\n",
            "the high-dimensional configuration space, visual homogeneity, self-occlusions,\n",
            "and complex dynamics. We consider dense (tight) knots that lack space between\n",
            "self-intersections and present an iterative approach that uses learned\n",
            "geometric structure in configurations. We instantiate this into an algorithm,\n",
            "HULK: Hierarchical Untangling from Learned Keypoints, which combines\n",
            "learning-based perception with a geometric planner into a policy that guides a\n",
            "bilateral robot to untangle knots. To evaluate the policy, we perform\n",
            "experiments both in a novel simulation environment modelling cables with varied\n",
            "knot types and textures and in a physical system using the da Vinci surgical\n",
            "robot. We find that HULK is able to untangle cables with dense figure-eight and\n",
            "overhand knots and generalize to varied textures and appearances. We compare\n",
            "two variants of HULK to three baselines and observe that HULK achieves 43.3%\n",
            "higher success rates on a physical system compared to the next best baseline.\n",
            "HULK successfully untangles a cable from a dense initial configuration\n",
            "containing up to two overhand and figure-eight knots in 97.9% of 378 simulation\n",
            "experiments with an average of 12.1 actions per trial. In physical experiments,\n",
            "HULK achieves 61.7% untangling success, averaging 8.48 actions per trial.\n",
            "Supplementary material, code, and videos can be found at\n",
            "https://tinyurl.com/y3a88ycu.\n",
            "\n",
            "**Paper Id :2008.03787 \n",
            "Title :Neural Manipulation Planning on Constraint Manifolds\n",
            "  The presence of task constraints imposes a significant challenge to motion\n",
            "planning. Despite all recent advancements, existing algorithms are still\n",
            "computationally expensive for most planning problems. In this paper, we present\n",
            "Constrained Motion Planning Networks (CoMPNet), the first neural planner for\n",
            "multimodal kinematic constraints. Our approach comprises the following\n",
            "components: i) constraint and environment perception encoders; ii) neural robot\n",
            "configuration generator that outputs configurations on/near the constraint\n",
            "manifold(s), and iii) a bidirectional planning algorithm that takes the\n",
            "generated configurations to create a feasible robot motion trajectory. We show\n",
            "that CoMPNet solves practical motion planning tasks involving both\n",
            "unconstrained and constrained problems. Furthermore, it generalizes to new\n",
            "unseen locations of the objects, i.e., not seen during training, in the given\n",
            "environments with high success rates. When compared to the state-of-the-art\n",
            "constrained motion planning algorithms, CoMPNet outperforms by order of\n",
            "magnitude improvement in computational speed with a significantly lower\n",
            "variance.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.05605 \n",
            "Title :Decentralized Motion Planning for Multi-Robot Navigation using Deep\n",
            "  Reinforcement Learning\n",
            "  This work presents a decentralized motion planning framework for addressing\n",
            "the task of multi-robot navigation using deep reinforcement learning. A custom\n",
            "simulator was developed in order to experimentally investigate the navigation\n",
            "problem of 4 cooperative non-holonomic robots sharing limited state information\n",
            "with each other in 3 different settings. The notion of decentralized motion\n",
            "planning with common and shared policy learning was adopted, which allowed\n",
            "robust training and testing of this approach in a stochastic environment since\n",
            "the agents were mutually independent and exhibited asynchronous motion\n",
            "behavior. The task was further aggravated by providing the agents with a sparse\n",
            "observation space and requiring them to generate continuous action commands so\n",
            "as to efficiently, yet safely navigate to their respective goal locations,\n",
            "while avoiding collisions with other dynamic peers and static obstacles at all\n",
            "times. The experimental results are reported in terms of quantitative measures\n",
            "and qualitative remarks for both training and deployment phases.\n",
            "\n",
            "**Paper Id :2003.08727 \n",
            "Title :Decentralized MCTS via Learned Teammate Models\n",
            "  Decentralized online planning can be an attractive paradigm for cooperative\n",
            "multi-agent systems, due to improved scalability and robustness. A key\n",
            "difficulty of such approach lies in making accurate predictions about the\n",
            "decisions of other agents. In this paper, we present a trainable online\n",
            "decentralized planning algorithm based on decentralized Monte Carlo Tree\n",
            "Search, combined with models of teammates learned from previous episodic runs.\n",
            "By only allowing one agent to adapt its models at a time, under the assumption\n",
            "of ideal policy approximation, successive iterations of our method are\n",
            "guaranteed to improve joint policies, and eventually lead to convergence to a\n",
            "Nash equilibrium. We test the efficiency of the algorithm by performing\n",
            "experiments in several scenarios of the spatial task allocation environment\n",
            "introduced in [Claes et al., 2015]. We show that deep learning and\n",
            "convolutional neural networks can be employed to produce accurate policy\n",
            "approximators which exploit the spatial features of the problem, and that the\n",
            "proposed algorithm improves over the baseline planning performance for\n",
            "particularly challenging domain configurations.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.06306 \n",
            "Title :Analyzing Neural Discourse Coherence Models\n",
            "  In this work, we systematically investigate how well current models of\n",
            "coherence can capture aspects of text implicated in discourse organisation. We\n",
            "devise two datasets of various linguistic alterations that undermine coherence\n",
            "and test model sensitivity to changes in syntax and semantics. We furthermore\n",
            "probe discourse embedding space and examine the knowledge that is encoded in\n",
            "representations of coherence. We hope this study shall provide further insight\n",
            "into how to frame the task and improve models of coherence assessment further.\n",
            "Finally, we make our datasets publicly available as a resource for researchers\n",
            "to use to test discourse coherence models.\n",
            "\n",
            "**Paper Id :2009.13033 \n",
            "Title :Where Does the Robustness Come from? A Study of the Transformation-based\n",
            "  Ensemble Defence\n",
            "  This paper aims to provide a thorough study on the effectiveness of the\n",
            "transformation-based ensemble defence for image classification and its reasons.\n",
            "It has been empirically shown that they can enhance the robustness against\n",
            "evasion attacks, while there is little analysis on the reasons. In particular,\n",
            "it is not clear whether the robustness improvement is a result of\n",
            "transformation or ensemble. In this paper, we design two adaptive attacks to\n",
            "better evaluate the transformation-based ensemble defence. We conduct\n",
            "experiments to show that 1) the transferability of adversarial examples exists\n",
            "among the models trained on data records after different reversible\n",
            "transformations; 2) the robustness gained through transformation-based ensemble\n",
            "is limited; 3) this limited robustness is mainly from the irreversible\n",
            "transformations rather than the ensemble of a number of models; and 4) blindly\n",
            "increasing the number of sub-models in a transformation-based ensemble does not\n",
            "bring extra robustness gain.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.06382 \n",
            "Title :Towards A Sentiment Analyzer for Low-Resource Languages\n",
            "  Twitter is one of the top influenced social media which has a million number\n",
            "of active users. It is commonly used for microblogging that allows users to\n",
            "share messages, ideas, thoughts and many more. Thus, millions interaction such\n",
            "as short messages or tweets are flowing around among the twitter users\n",
            "discussing various topics that has been happening world-wide. This research\n",
            "aims to analyse a sentiment of the users towards a particular trending topic\n",
            "that has been actively and massively discussed at that time. We chose a hashtag\n",
            "\\textit{\\#kpujangancurang} that was the trending topic during the Indonesia\n",
            "presidential election in 2019. We use the hashtag to obtain a set of data from\n",
            "Twitter to analyse and investigate further the positive or the negative\n",
            "sentiment of the users from their tweets. This research utilizes rapid miner\n",
            "tool to generate the twitter data and comparing Naive Bayes, K-Nearest\n",
            "Neighbor, Decision Tree, and Multi-Layer Perceptron classification methods to\n",
            "classify the sentiment of the twitter data. There are overall 200 labeled data\n",
            "in this experiment. Overall, Naive Bayes and Multi-Layer Perceptron\n",
            "classification outperformed the other two methods on 11 experiments with\n",
            "different size of training-testing data split. The two classifiers are\n",
            "potential to be used in creating sentiment analyzer for low-resource languages\n",
            "with small corpus.\n",
            "\n",
            "**Paper Id :1912.00667 \n",
            "Title :A Human-AI Loop Approach for Joint Keyword Discovery and Expectation\n",
            "  Estimation in Micropost Event Detection\n",
            "  Microblogging platforms such as Twitter are increasingly being used in event\n",
            "detection. Existing approaches mainly use machine learning models and rely on\n",
            "event-related keywords to collect the data for model training. These approaches\n",
            "make strong assumptions on the distribution of the relevant micro-posts\n",
            "containing the keyword -- referred to as the expectation of the distribution --\n",
            "and use it as a posterior regularization parameter during model training. Such\n",
            "approaches are, however, limited as they fail to reliably estimate the\n",
            "informativeness of a keyword and its expectation for model training. This paper\n",
            "introduces a Human-AI loop approach to jointly discover informative keywords\n",
            "for model training while estimating their expectation. Our approach iteratively\n",
            "leverages the crowd to estimate both keyword specific expectation and the\n",
            "disagreement between the crowd and the model in order to discover new keywords\n",
            "that are most beneficial for model training. These keywords and their\n",
            "expectation not only improve the resulting performance but also make the model\n",
            "training process more transparent. We empirically demonstrate the merits of our\n",
            "approach, both in terms of accuracy and interpretability, on multiple\n",
            "real-world datasets and show that our approach improves the state of the art by\n",
            "24.3%.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.06507 \n",
            "Title :Reinforcement Learning with Videos: Combining Offline Observations with\n",
            "  Interaction\n",
            "  Reinforcement learning is a powerful framework for robots to acquire skills\n",
            "from experience, but often requires a substantial amount of online data\n",
            "collection. As a result, it is difficult to collect sufficiently diverse\n",
            "experiences that are needed for robots to generalize broadly. Videos of humans,\n",
            "on the other hand, are a readily available source of broad and interesting\n",
            "experiences. In this paper, we consider the question: can we perform\n",
            "reinforcement learning directly on experience collected by humans? This problem\n",
            "is particularly difficult, as such videos are not annotated with actions and\n",
            "exhibit substantial visual domain shift relative to the robot's embodiment. To\n",
            "address these challenges, we propose a framework for reinforcement learning\n",
            "with videos (RLV). RLV learns a policy and value function using experience\n",
            "collected by humans in combination with data collected by robots. In our\n",
            "experiments, we find that RLV is able to leverage such videos to learn\n",
            "challenging vision-based skills with less than half as many samples as RL\n",
            "methods that learn from scratch.\n",
            "\n",
            "**Paper Id :2003.04960 \n",
            "Title :Curriculum Learning for Reinforcement Learning Domains: A Framework and\n",
            "  Survey\n",
            "  Reinforcement learning (RL) is a popular paradigm for addressing sequential\n",
            "decision tasks in which the agent has only limited environmental feedback.\n",
            "Despite many advances over the past three decades, learning in many domains\n",
            "still requires a large amount of interaction with the environment, which can be\n",
            "prohibitively expensive in realistic scenarios. To address this problem,\n",
            "transfer learning has been applied to reinforcement learning such that\n",
            "experience gained in one task can be leveraged when starting to learn the next,\n",
            "harder task. More recently, several lines of research have explored how tasks,\n",
            "or data samples themselves, can be sequenced into a curriculum for the purpose\n",
            "of learning a problem that may otherwise be too difficult to learn from\n",
            "scratch. In this article, we present a framework for curriculum learning (CL)\n",
            "in reinforcement learning, and use it to survey and classify existing CL\n",
            "methods in terms of their assumptions, capabilities, and goals. Finally, we use\n",
            "our framework to find open problems and suggest directions for future RL\n",
            "curriculum learning research.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.08027 \n",
            "Title :ACDER: Augmented Curiosity-Driven Experience Replay\n",
            "  Exploration in environments with sparse feedback remains a challenging\n",
            "research problem in reinforcement learning (RL). When the RL agent explores the\n",
            "environment randomly, it results in low exploration efficiency, especially in\n",
            "robotic manipulation tasks with high dimensional continuous state and action\n",
            "space. In this paper, we propose a novel method, called Augmented\n",
            "Curiosity-Driven Experience Replay (ACDER), which leverages (i) a new\n",
            "goal-oriented curiosity-driven exploration to encourage the agent to pursue\n",
            "novel and task-relevant states more purposefully and (ii) the dynamic initial\n",
            "states selection as an automatic exploratory curriculum to further improve the\n",
            "sample-efficiency. Our approach complements Hindsight Experience Replay (HER)\n",
            "by introducing a new way to pursue valuable states. Experiments conducted on\n",
            "four challenging robotic manipulation tasks with binary rewards, including\n",
            "Reach, Push, Pick&Place and Multi-step Push. The empirical results show that\n",
            "our proposed method significantly outperforms existing methods in the first\n",
            "three basic tasks and also achieves satisfactory performance in multi-step\n",
            "robotic task learning.\n",
            "\n",
            "**Paper Id :2005.04912 \n",
            "Title :Maximizing Information Gain in Partially Observable Environments via\n",
            "  Prediction Reward\n",
            "  Information gathering in a partially observable environment can be formulated\n",
            "as a reinforcement learning (RL), problem where the reward depends on the\n",
            "agent's uncertainty. For example, the reward can be the negative entropy of the\n",
            "agent's belief over an unknown (or hidden) variable. Typically, the rewards of\n",
            "an RL agent are defined as a function of the state-action pairs and not as a\n",
            "function of the belief of the agent; this hinders the direct application of\n",
            "deep RL methods for such tasks. This paper tackles the challenge of using\n",
            "belief-based rewards for a deep RL agent, by offering a simple insight that\n",
            "maximizing any convex function of the belief of the agent can be approximated\n",
            "by instead maximizing a prediction reward: a reward based on prediction\n",
            "accuracy. In particular, we derive the exact error between negative entropy and\n",
            "the expected prediction reward. This insight provides theoretical motivation\n",
            "for several fields using prediction rewards---namely visual attention, question\n",
            "answering systems, and intrinsic motivation---and highlights their connection\n",
            "to the usually distinct fields of active perception, active sensing, and sensor\n",
            "placement. Based on this insight we present deep anticipatory networks (DANs),\n",
            "which enables an agent to take actions to reduce its uncertainty without\n",
            "performing explicit belief inference. We present two applications of DANs:\n",
            "building a sensor selection system for tracking people in a shopping mall and\n",
            "learning discrete models of attention on fashion MNIST and MNIST digit\n",
            "classification.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.08981 \n",
            "Title :RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object\n",
            "  Recognition\n",
            "  Millimeter-wave radars are being increasingly integrated into commercial\n",
            "vehicles to support new advanced driver-assistance systems by enabling robust\n",
            "and high-performance object detection, localization, as well as recognition - a\n",
            "key component of new environmental perception. In this paper, we propose a\n",
            "novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that\n",
            "extracts the location and class of objects based on further processing of the\n",
            "range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D\n",
            "convolutional neural networks (NN), we propose to combine several\n",
            "lower-dimension NN models within our RAMP-CNN model that nonetheless approaches\n",
            "the performance upper-bound with lower complexity. The extensive experiments\n",
            "show that the proposed RAMP-CNN model achieves better average recall and\n",
            "average precision than prior works in all testing scenarios. Besides, the\n",
            "RAMP-CNN model is validated to work robustly under nighttime, which enables\n",
            "low-cost radars as a potential substitute for pure optical sensing under severe\n",
            "conditions.\n",
            "\n",
            "**Paper Id :1908.06180 \n",
            "Title :Multi-View Broad Learning System for Primate Oculomotor Decision\n",
            "  Decoding\n",
            "  Multi-view learning improves the learning performance by utilizing multi-view\n",
            "data: data collected from multiple sources, or feature sets extracted from the\n",
            "same data source. This approach is suitable for primate brain state decoding\n",
            "using cortical neural signals. This is because the complementary components of\n",
            "simultaneously recorded neural signals, local field potentials (LFPs) and\n",
            "action potentials (spikes), can be treated as two views. In this paper, we\n",
            "extended broad learning system (BLS), a recently proposed wide neural network\n",
            "architecture, from single-view learning to multi-view learning, and validated\n",
            "its performance in decoding monkeys' oculomotor decision from medial frontal\n",
            "LFPs and spikes. We demonstrated that medial frontal LFPs and spikes in\n",
            "non-human primate do contain complementary information about the oculomotor\n",
            "decision, and that the proposed multi-view BLS is a more effective approach for\n",
            "decoding the oculomotor decision than several classical and state-of-the-art\n",
            "single-view and multi-view learning approaches.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.09858 \n",
            "Title :Conservative Extensions in Horn Description Logics with Inverse Roles\n",
            "  We investigate the decidability and computational complexity of conservative\n",
            "extensions and the related notions of inseparability and entailment in Horn\n",
            "description logics (DLs) with inverse roles. We consider both query\n",
            "conservative extensions, defined by requiring that the answers to all\n",
            "conjunctive queries are left unchanged, and deductive conservative extensions,\n",
            "which require that the entailed concept inclusions, role inclusions, and\n",
            "functionality assertions do not change. Upper bounds for query conservative\n",
            "extensions are particularly challenging because characterizations in terms of\n",
            "unbounded homomorphisms between universal models, which are the foundation of\n",
            "the standard approach to establishing decidability, fail in the presence of\n",
            "inverse roles. We resort to a characterization that carefully mixes unbounded\n",
            "and bounded homomorphisms and enables a decision procedure that combines tree\n",
            "automata and a mosaic technique. Our main results are that query conservative\n",
            "extensions are 2ExpTime-complete in all DLs between ELI and Horn-ALCHIF and\n",
            "between Horn-ALC and Horn-ALCHIF, and that deductive conservative extensions\n",
            "are 2ExpTime-complete in all DLs between ELI and ELHIF_\\bot. The same results\n",
            "hold for inseparability and entailment.\n",
            "\n",
            "**Paper Id :2010.11246 \n",
            "Title :On the Potential of Lexico-logical Alignments for Semantic Parsing to\n",
            "  SQL Queries\n",
            "  Large-scale semantic parsing datasets annotated with logical forms have\n",
            "enabled major advances in supervised approaches. But can richer supervision\n",
            "help even more? To explore the utility of fine-grained, lexical-level\n",
            "supervision, we introduce Squall, a dataset that enriches 11,276\n",
            "WikiTableQuestions English-language questions with manually created SQL\n",
            "equivalents plus alignments between SQL and question fragments. Our annotation\n",
            "enables new training possibilities for encoder-decoder models, including\n",
            "approaches from machine translation previously precluded by the absence of\n",
            "alignments. We propose and test two methods: (1) supervised attention; (2)\n",
            "adopting an auxiliary objective of disambiguating references in the input\n",
            "queries to table columns. In 5-fold cross validation, these strategies improve\n",
            "over strong baselines by 4.4% execution accuracy. Oracle experiments suggest\n",
            "that annotated alignments can support further accuracy gains of up to 23.9%.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.10307 \n",
            "Title :Filtering Rules for Flow Time Minimization in a Parallel Machine\n",
            "  Scheduling Problem\n",
            "  This paper studies the scheduling of jobs of different families on parallel\n",
            "machines with qualification constraints. Originating from semiconductor\n",
            "manufacturing, this constraint imposes a time threshold between the execution\n",
            "of two jobs of the same family. Otherwise, the machine becomes disqualified for\n",
            "this family. The goal is to minimize both the flow time and the number of\n",
            "disqualifications. Recently, an efficient constraint programming model has been\n",
            "proposed. However, when priority is given to the flow time objective, the\n",
            "efficiency of the model can be improved. This paper uses a polynomial-time\n",
            "algorithm which minimize the flow time for a single machine relaxation where\n",
            "disqualifications are not considered. Using this algorithm one can derived\n",
            "filtering rules on different variables of the model. Experimental results are\n",
            "presented showing the effectiveness of these rules. They improve the\n",
            "competitiveness with the mixed integer linear program of the literature.\n",
            "\n",
            "**Paper Id :2005.00603 \n",
            "Title :It is Time for New Perspectives on How to Fight Bloat in GP\n",
            "  The present and future of evolutionary algorithms depends on the proper use\n",
            "of modern parallel and distributed computing infrastructures. Although still\n",
            "sequential approaches dominate the landscape, available multi-core, many-core\n",
            "and distributed systems will make users and researchers to more frequently\n",
            "deploy parallel version of the algorithms. In such a scenario, new\n",
            "possibilities arise regarding the time saved when parallel evaluation of\n",
            "individuals are performed. And this time saving is particularly relevant in\n",
            "Genetic Programming. This paper studies how evaluation time influences not only\n",
            "time to solution in parallel/distributed systems, but may also affect size\n",
            "evolution of individuals in the population, and eventually will reduce the\n",
            "bloat phenomenon GP features. This paper considers time and space as two sides\n",
            "of a single coin when devising a more natural method for fighting bloat. This\n",
            "new perspective allows us to understand that new methods for bloat control can\n",
            "be derived, and the first of such a method is described and tested.\n",
            "Experimental data confirms the strength of the approach: using computing time\n",
            "as a measure of individuals' complexity allows to control the growth in size of\n",
            "genetic programming individuals.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.10640 \n",
            "Title :Assessment and Linear Programming under Fuzzy Conditions\n",
            "  A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers\n",
            "for evaluating a group's mean performance, when qualitative grades instead of\n",
            "numerical scores are used for assessing its members' individual performance.\n",
            "Also, a new technique is developed for solving Linear Programming problems with\n",
            "fuzzy coefficients and everyday life applications are presented to illustrate\n",
            "our results.\n",
            "\n",
            "**Paper Id :2006.08347 \n",
            "Title :Application of Fuzzy Rule based System for Highway Research Board\n",
            "  Classification of Soils\n",
            "  Fuzzy rule-based model is a powerful tool for imitating the human way of\n",
            "thinking and solving uncertainty-related problems as it allows for\n",
            "understandable and interpretable rule bases. The objective of this paper is to\n",
            "study the applicability of fuzzy rule-based modelling to quantify soil\n",
            "classification for engineering purposes by qualitatively considering soil index\n",
            "properties. The classification system of the Highway Research Board is\n",
            "considered to illustrate a fuzzy rule-based model. The soil's index properties\n",
            "are fuzzified using triangular functions, and the fuzzy membership values are\n",
            "calculated. Fuzzy arithmetical operators are then applied to the membership\n",
            "values obtained for classification. Fuzzy decision tree classification\n",
            "algorithm is used to derive fuzzy if-then rules to quantify qualitative soil\n",
            "classification. The proposed system is implemented in MATLAB. The results\n",
            "obtained are checked and the implementation of the proposed model is measured\n",
            "against the outcomes of the laboratory tests.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.12715 \n",
            "Title :Resonance: Replacing Software Constants with Context-Aware Models in\n",
            "  Real-time Communication\n",
            "  Large software systems tune hundreds of 'constants' to optimize their runtime\n",
            "performance. These values are commonly derived through intuition, lab tests, or\n",
            "A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best\n",
            "value depends on runtime context. In this paper, we provide an experimental\n",
            "approach to replace constants with learned contextual functions for Skype - a\n",
            "widely used real-time communication (RTC) application. We present Resonance, a\n",
            "system based on contextual bandits (CB). We describe experiences from three\n",
            "real-world experiments: applying it to the audio, video, and transport\n",
            "components in Skype. We surface a unique and practical challenge of performing\n",
            "machine learning (ML) inference in large software systems written using\n",
            "encapsulation principles. Finally, we open-source FeatureBroker, a library to\n",
            "reduce the friction in adopting ML models in such development environments\n",
            "\n",
            "**Paper Id :2009.05700 \n",
            "Title :Information-Theoretic Multi-Objective Bayesian Optimization with\n",
            "  Continuous Approximations\n",
            "  Many real-world applications involve black-box optimization of multiple\n",
            "objectives using continuous function approximations that trade-off accuracy and\n",
            "resource cost of evaluation. For example, in rocket launching research, we need\n",
            "to find designs that trade-off return-time and angular distance using\n",
            "continuous-fidelity simulators (e.g., varying tolerance parameter to trade-off\n",
            "simulation time and accuracy) for design evaluations. The goal is to\n",
            "approximate the optimal Pareto set by minimizing the cost for evaluations. In\n",
            "this paper, we propose a novel approach referred to as information-Theoretic\n",
            "Multi-Objective Bayesian Optimization with Continuous Approximations (iMOCA)}\n",
            "to solve this problem. The key idea is to select the sequence of input and\n",
            "function approximations for multiple objectives which maximize the information\n",
            "gain per unit cost for the optimal Pareto front. Our experiments on diverse\n",
            "synthetic and real-world benchmarks show that iMOCA significantly improves over\n",
            "existing single-fidelity methods.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.13297 \n",
            "Title :Totally and Partially Ordered Hierarchical Planners in PDDL4J Library\n",
            "  In this paper, we outline the implementation of the TFD (Totally Ordered Fast\n",
            "Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners\n",
            "that participated in the first HTN IPC competition in 2020. These two planners\n",
            "are based on forward-chaining task decomposition coupled with a compact\n",
            "grounding of actions, methods, tasks and HTN problems.\n",
            "\n",
            "**Paper Id :2103.05481 \n",
            "Title :From Classical to Hierarchical: benchmarks for the HTN Track of the\n",
            "  International Planning Competition\n",
            "  In this short paper, we outline nine classical benchmarks submitted to the\n",
            "first hierarchical planning track of the International Planning competition in\n",
            "2020. All of these benchmarks are based on the HDDL language. The choice of the\n",
            "benchmarks was based on a questionnaire sent to the HTN community. They are the\n",
            "following: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper,\n",
            "and Hiking. In the rest of the paper we give a short description of these\n",
            "benchmarks. All are totally ordered.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2011.14266 \n",
            "Title :Distilled Thompson Sampling: Practical and Efficient Thompson Sampling\n",
            "  via Imitation Learning\n",
            "  Thompson sampling (TS) has emerged as a robust technique for contextual\n",
            "bandit problems. However, TS requires posterior inference and optimization for\n",
            "action generation, prohibiting its use in many internet applications where\n",
            "latency and ease of deployment are of concern. We propose a novel\n",
            "imitation-learning-based algorithm that distills a TS policy into an explicit\n",
            "policy representation by performing posterior inference and optimization\n",
            "offline. The explicit policy representation enables fast online decision-making\n",
            "and easy deployment in mobile and server-based environments. Our algorithm\n",
            "iteratively performs offline batch updates to the TS policy and learns a new\n",
            "imitation policy. Since we update the TS policy with observations collected\n",
            "under the imitation policy, our algorithm emulates an off-policy version of TS.\n",
            "Our imitation algorithm guarantees Bayes regret comparable to TS, up to the sum\n",
            "of single-step imitation errors. We show these imitation errors can be made\n",
            "arbitrarily small when unlabeled contexts are cheaply available, which is the\n",
            "case for most large-scale internet applications. Empirically, we show that our\n",
            "imitation policy achieves comparable regret to TS, while reducing decision-time\n",
            "latency by over an order of magnitude.\n",
            "\n",
            "**Paper Id :2001.03809 \n",
            "Title :Point-Based Methods for Model Checking in Partially Observable Markov\n",
            "  Decision Processes\n",
            "  Autonomous systems are often required to operate in partially observable\n",
            "environments. They must reliably execute a specified objective even with\n",
            "incomplete information about the state of the environment. We propose a\n",
            "methodology to synthesize policies that satisfy a linear temporal logic formula\n",
            "in a partially observable Markov decision process (POMDP). By formulating a\n",
            "planning problem, we show how to use point-based value iteration methods to\n",
            "efficiently approximate the maximum probability of satisfying a desired logical\n",
            "formula and compute the associated belief state policy. We demonstrate that our\n",
            "method scales to large POMDP domains and provides strong bounds on the\n",
            "performance of the resulting policy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.00868 \n",
            "Title :Towards Good Practices in Self-supervised Representation Learning\n",
            "  Self-supervised representation learning has seen remarkable progress in the\n",
            "last few years. More recently, contrastive instance learning has shown\n",
            "impressive results compared to its supervised learning counterparts. However,\n",
            "even with the ever increased interest in contrastive instance learning, it is\n",
            "still largely unclear why these methods work so well. In this paper, we aim to\n",
            "unravel some of the mysteries behind their success, which are the good\n",
            "practices. Through an extensive empirical analysis, we hope to not only provide\n",
            "insights but also lay out a set of best practices that led to the success of\n",
            "recent work in self-supervised representation learning.\n",
            "\n",
            "**Paper Id :1905.01258 \n",
            "Title :Disentangling Factors of Variation Using Few Labels\n",
            "  Learning disentangled representations is considered a cornerstone problem in\n",
            "representation learning. Recently, Locatello et al. (2019) demonstrated that\n",
            "unsupervised disentanglement learning without inductive biases is theoretically\n",
            "impossible and that existing inductive biases and unsupervised methods do not\n",
            "allow to consistently learn disentangled representations. However, in many\n",
            "practical settings, one might have access to a limited amount of supervision,\n",
            "for example through manual labeling of (some) factors of variation in a few\n",
            "training examples. In this paper, we investigate the impact of such supervision\n",
            "on state-of-the-art disentanglement methods and perform a large scale study,\n",
            "training over 52000 models under well-defined and reproducible experimental\n",
            "conditions. We observe that a small number of labeled examples (0.01--0.5\\% of\n",
            "the data set), with potentially imprecise and incomplete labels, is sufficient\n",
            "to perform model selection on state-of-the-art unsupervised models. Further, we\n",
            "investigate the benefit of incorporating supervision into the training process.\n",
            "Overall, we empirically validate that with little and imprecise supervision it\n",
            "is possible to reliably learn disentangled representations.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.01176 \n",
            "Title :Proceedings Second Workshop on Formal Methods for Autonomous Systems\n",
            "  Autonomous systems are highly complex and present unique challenges for the\n",
            "application of formal methods. Autonomous systems act without human\n",
            "intervention, and are often embedded in a robotic system, so that they can\n",
            "interact with the real world. As such, they exhibit the properties of\n",
            "safety-critical, cyber-physical, hybrid, and real-time systems.\n",
            "  The goal of FMAS is to bring together leading researchers who are tackling\n",
            "the unique challenges of autonomous systems using formal methods, to present\n",
            "recent and ongoing work. We are interested in the use of formal methods to\n",
            "specify, model, or verify autonomous or robotic systems; in whole or in part.\n",
            "We are also interested in successful industrial applications and potential\n",
            "future directions for this emerging application of formal methods.\n",
            "\n",
            "**Paper Id :2111.10871 \n",
            "Title :A Software Tool for Evaluating Unmanned Autonomous Systems\n",
            "  The North Carolina Agriculture and Technical State University (NC A&T) in\n",
            "collaboration with Georgia Tech Research Institute (GTRI) has developed\n",
            "methodologies for creating simulation-based technology tools that are capable\n",
            "of inferring the perceptions and behavioral states of autonomous systems. These\n",
            "methodologies have the potential to provide the Test and Evaluation (T&E)\n",
            "community at the Department of Defense (DoD) with a greater insight into the\n",
            "internal processes of these systems. The methodologies use only external\n",
            "observations and do not require complete knowledge of the internal processing\n",
            "of and/or any modifications to the system under test. This paper presents an\n",
            "example of one such simulation-based technology tool, named as the Data-Driven\n",
            "Intelligent Prediction Tool (DIPT). DIPT was developed for testing a\n",
            "multi-platform Unmanned Aerial Vehicle (UAV) system capable of conducting\n",
            "collaborative search missions. DIPT's Graphical User Interface (GUI) enables\n",
            "the testers to view the aircraft's current operating state, predicts its\n",
            "current target-detection status, and provides reasoning for exhibiting a\n",
            "particular behavior along with an explanation of assigning a particular task to\n",
            "it.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.01747 \n",
            "Title :Bengali Abstractive News Summarization(BANS): A Neural Attention\n",
            "  Approach\n",
            "  Abstractive summarization is the process of generating novel sentences based\n",
            "on the information extracted from the original text document while retaining\n",
            "the context. Due to abstractive summarization's underlying complexities, most\n",
            "of the past research work has been done on the extractive summarization\n",
            "approach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\n",
            "model, abstractive summarization becomes more viable. Although a significant\n",
            "number of notable research has been done in the English language based on\n",
            "abstractive summarization, only a couple of works have been done on Bengali\n",
            "abstractive news summarization (BANS). In this article, we presented a seq2seq\n",
            "based Long Short-Term Memory (LSTM) network model with attention at\n",
            "encoder-decoder. Our proposed system deploys a local attention-based model that\n",
            "produces a long sequence of words with lucid and human-like generated sentences\n",
            "with noteworthy information of the original document. We also prepared a\n",
            "dataset of more than 19k articles and corresponding human-written summaries\n",
            "collected from bangla.bdnews24.com1 which is till now the most extensive\n",
            "dataset for Bengali news document summarization and publicly published in\n",
            "Kaggle2. We evaluated our model qualitatively and quantitatively and compared\n",
            "it with other published results. It showed significant improvement in terms of\n",
            "human evaluation scores with state-of-the-art approaches for BANS.\n",
            "\n",
            "**Paper Id :2101.11436 \n",
            "Title :Challenges Encountered in Turkish Natural Language Processing Studies\n",
            "  Natural language processing is a branch of computer science that combines\n",
            "artificial intelligence with linguistics. It aims to analyze a language element\n",
            "such as writing or speaking with software and convert it into information.\n",
            "Considering that each language has its own grammatical rules and vocabulary\n",
            "diversity, the complexity of the studies in this field is somewhat\n",
            "understandable. For instance, Turkish is a very interesting language in many\n",
            "ways. Examples of this are agglutinative word structure, consonant/vowel\n",
            "harmony, a large number of productive derivational morphemes (practically\n",
            "infinite vocabulary), derivation and syntactic relations, a complex emphasis on\n",
            "vocabulary and phonological rules. In this study, the interesting features of\n",
            "Turkish in terms of natural language processing are mentioned. In addition,\n",
            "summary info about natural language processing techniques, systems and various\n",
            "sources developed for Turkish are given.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.01929 \n",
            "Title :A Stochastic Path-Integrated Differential EstimatoR Expectation\n",
            "  Maximization Algorithm\n",
            "  The Expectation Maximization (EM) algorithm is of key importance for\n",
            "inference in latent variable models including mixture of regressors and\n",
            "experts, missing observations. This paper introduces a novel EM algorithm,\n",
            "called \\texttt{SPIDER-EM}, for inference from a training set of size $n$, $n\n",
            "\\gg 1$. At the core of our algorithm is an estimator of the full conditional\n",
            "expectation in the {\\sf E}-step, adapted from the stochastic path-integrated\n",
            "differential estimator ({\\tt SPIDER}) technique. We derive finite-time\n",
            "complexity bounds for smooth non-convex likelihood: we show that for\n",
            "convergence to an $\\epsilon$-approximate stationary point, the complexity\n",
            "scales as $K_{\\operatorname{Opt}} (n,\\epsilon )={\\cal O}(\\epsilon^{-1})$ and\n",
            "$K_{\\operatorname{CE}}( n,\\epsilon ) = n+ \\sqrt{n} {\\cal O}(\\epsilon^{-1} )$,\n",
            "where $K_{\\operatorname{Opt}}( n,\\epsilon )$ and $K_{\\operatorname{CE}}(n,\n",
            "\\epsilon )$ are respectively the number of {\\sf M}-steps and the number of\n",
            "per-sample conditional expectations evaluations. This improves over the\n",
            "state-of-the-art algorithms. Numerical results support our findings.\n",
            "\n",
            "**Paper Id :1805.09235 \n",
            "Title :Cramer-Wold AutoEncoder\n",
            "  We propose a new generative model, Cramer-Wold Autoencoder (CWAE). Following\n",
            "WAE, we directly encourage normality of the latent space. Our paper uses also\n",
            "the recent idea from Sliced WAE (SWAE) model, which uses one-dimensional\n",
            "projections as a method of verifying closeness of two distributions. The\n",
            "crucial new ingredient is the introduction of a new (Cramer-Wold) metric in the\n",
            "space of densities, which replaces the Wasserstein metric used in SWAE. We show\n",
            "that the Cramer-Wold metric between Gaussian mixtures is given by a simple\n",
            "analytic formula, which results in the removal of sampling necessary to\n",
            "estimate the cost function in WAE and SWAE models. As a consequence, while\n",
            "drastically simplifying the optimization procedure, CWAE produces samples of a\n",
            "matching perceptual quality to other SOTA models.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.03105 \n",
            "Title :Obstacle avoidance and path finding for mobile robot navigation\n",
            "  This paper investigates different methods to detect obstacles ahead of a\n",
            "robot using a camera in the robot, an aerial camera, and an ultrasound sensor.\n",
            "We also explored various efficient path finding methods for the robot to\n",
            "navigate to the target source. Single and multi-iteration angle-based\n",
            "navigation algorithms were developed. The theta-based path finding algorithms\n",
            "were compared with the Dijkstra Algorithm and their performance were analyzed.\n",
            "\n",
            "**Paper Id :2103.11276 \n",
            "Title :High precision control and deep learning-based corn stand counting\n",
            "  algorithms for agricultural robot\n",
            "  This paper presents high precision control and deep learning-based corn stand\n",
            "counting algorithms for a low-cost, ultra-compact 3D printed and autonomous\n",
            "field robot for agricultural operations. Currently, plant traits, such as\n",
            "emergence rate, biomass, vigor, and stand counting, are measured manually. This\n",
            "is highly labor-intensive and prone to errors. The robot, termed TerraSentia,\n",
            "is designed to automate the measurement of plant traits for efficient\n",
            "phenotyping as an alternative to manual measurements. In this paper, we\n",
            "formulate a Nonlinear Moving Horizon Estimator (NMHE) that identifies key\n",
            "terrain parameters using onboard robot sensors and a learning-based Nonlinear\n",
            "Model Predictive Control (NMPC) that ensures high precision path tracking in\n",
            "the presence of unknown wheel-terrain interaction. Moreover, we develop a\n",
            "machine vision algorithm designed to enable an ultra-compact ground robot to\n",
            "count corn stands by driving through the fields autonomously. The algorithm\n",
            "leverages a deep network to detect corn plants in images, and a visual tracking\n",
            "model to re-identify detected objects at different time steps. We collected\n",
            "data from 53 corn plots in various fields for corn plants around 14 days after\n",
            "emergence (stage V3 - V4). The robot predictions have agreed well with the\n",
            "ground truth with $C_{robot}=1.02 \\times C_{human}-0.86$ and a correlation\n",
            "coefficient $R=0.96$. The mean relative error given by the algorithm is\n",
            "$-3.78\\%$, and the standard deviation is $6.76\\%$. These results indicate a\n",
            "first and significant step towards autonomous robot-based real-time phenotyping\n",
            "using low-cost, ultra-compact ground robots for corn and potentially other\n",
            "crops.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.05810 \n",
            "Title :Multi-expert learning of adaptive legged locomotion\n",
            "  Achieving versatile robot locomotion requires motor skills which can adapt to\n",
            "previously unseen situations. We propose a Multi-Expert Learning Architecture\n",
            "(MELA) that learns to generate adaptive skills from a group of representative\n",
            "expert skills. During training, MELA is first initialised by a distinct set of\n",
            "pre-trained experts, each in a separate deep neural network (DNN). Then by\n",
            "learning the combination of these DNNs using a Gating Neural Network (GNN),\n",
            "MELA can acquire more specialised experts and transitional skills across\n",
            "various locomotion modes. During runtime, MELA constantly blends multiple DNNs\n",
            "and dynamically synthesises a new DNN to produce adaptive behaviours in\n",
            "response to changing situations. This approach leverages the advantages of\n",
            "trained expert skills and the fast online synthesis of adaptive policies to\n",
            "generate responsive motor skills during the changing tasks. Using a unified\n",
            "MELA framework, we demonstrated successful multi-skill locomotion on a real\n",
            "quadruped robot that performed coherent trotting, steering, and fall recovery\n",
            "autonomously, and showed the merit of multi-expert learning generating\n",
            "behaviours which can adapt to unseen scenarios.\n",
            "\n",
            "**Paper Id :1901.03887 \n",
            "Title :Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n",
            "  Learning through Memory-driven Communication\n",
            "  Deep reinforcement learning algorithms have recently been used to train\n",
            "multiple interacting agents in a centralised manner whilst keeping their\n",
            "execution decentralised. When the agents can only acquire partial observations\n",
            "and are faced with tasks requiring coordination and synchronisation skills,\n",
            "inter-agent communication plays an essential role. In this work, we propose a\n",
            "framework for multi-agent training using deep deterministic policy gradients\n",
            "that enables concurrent, end-to-end learning of an explicit communication\n",
            "protocol through a memory device. During training, the agents learn to perform\n",
            "read and write operations enabling them to infer a shared representation of the\n",
            "world. We empirically demonstrate that concurrent learning of the communication\n",
            "device and individual policies can improve inter-agent coordination and\n",
            "performance in small-scale systems. Our experimental results show that the\n",
            "proposed method achieves superior performance in scenarios with up to six\n",
            "agents. We illustrate how different communication patterns can emerge on six\n",
            "different tasks of increasing complexity. Furthermore, we study the effects of\n",
            "corrupting the communication channel, provide a visualisation of the\n",
            "time-varying memory content as the underlying task is being solved and validate\n",
            "the building blocks of the proposed memory device through ablation studies.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.06325 \n",
            "Title :Deep Reinforcement Learning for Stock Portfolio Optimization\n",
            "  Stock portfolio optimization is the process of constant re-distribution of\n",
            "money to a pool of various stocks. In this paper, we will formulate the problem\n",
            "such that we can apply Reinforcement Learning for the task properly. To\n",
            "maintain a realistic assumption about the market, we will incorporate\n",
            "transaction cost and risk factor into the state as well. On top of that, we\n",
            "will apply various state-of-the-art Deep Reinforcement Learning algorithms for\n",
            "comparison. Since the action space is continuous, the realistic formulation\n",
            "were tested under a family of state-of-the-art continuous policy gradients\n",
            "algorithms: Deep Deterministic Policy Gradient (DDPG), Generalized\n",
            "Deterministic Policy Gradient (GDPG) and Proximal Policy Optimization (PPO),\n",
            "where the former two perform much better than the last one. Next, we will\n",
            "present the end-to-end solution for the task with Minimum Variance Portfolio\n",
            "Theory for stock subset selection, and Wavelet Transform for extracting\n",
            "multi-frequency data pattern. Observations and hypothesis were discussed about\n",
            "the results, as well as possible future research directions.1\n",
            "\n",
            "**Paper Id :1907.04543 \n",
            "Title :An Optimistic Perspective on Offline Reinforcement Learning\n",
            "  Off-policy reinforcement learning (RL) using a fixed offline dataset of\n",
            "logged interactions is an important consideration in real world applications.\n",
            "This paper studies offline RL using the DQN replay dataset comprising the\n",
            "entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate\n",
            "that recent off-policy deep RL algorithms, even when trained solely on this\n",
            "fixed dataset, outperform the fully trained DQN agent. To enhance\n",
            "generalization in the offline setting, we present Random Ensemble Mixture\n",
            "(REM), a robust Q-learning algorithm that enforces optimal Bellman consistency\n",
            "on random convex combinations of multiple Q-value estimates. Offline REM\n",
            "trained on the DQN replay dataset surpasses strong RL baselines. Ablation\n",
            "studies highlight the role of offline dataset size and diversity as well as the\n",
            "algorithm choice in our positive results. Overall, the results here present an\n",
            "optimistic view that robust RL algorithms trained on sufficiently large and\n",
            "diverse offline datasets can lead to high quality policies. The DQN replay\n",
            "dataset can serve as an offline RL benchmark and is open-sourced.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.06845 \n",
            "Title :A Unified Model for the Two-stage Offline-then-Online Resource\n",
            "  Allocation\n",
            "  With the popularity of the Internet, traditional offline resource allocation\n",
            "has evolved into a new form, called online resource allocation. It features the\n",
            "online arrivals of agents in the system and the real-time decision-making\n",
            "requirement upon the arrival of each online agent. Both offline and online\n",
            "resource allocation have wide applications in various real-world matching\n",
            "markets ranging from ridesharing to crowdsourcing. There are some emerging\n",
            "applications such as rebalancing in bike sharing and trip-vehicle dispatching\n",
            "in ridesharing, which involve a two-stage resource allocation process. The\n",
            "process consists of an offline phase and another sequential online phase, and\n",
            "both phases compete for the same set of resources. In this paper, we propose a\n",
            "unified model which incorporates both offline and online resource allocation\n",
            "into a single framework. Our model assumes non-uniform and known arrival\n",
            "distributions for online agents in the second online phase, which can be\n",
            "learned from historical data. We propose a parameterized linear programming\n",
            "(LP)-based algorithm, which is shown to be at most a constant factor of $1/4$\n",
            "from the optimal. Experimental results on the real dataset show that our\n",
            "LP-based approaches outperform the LP-agnostic heuristics in terms of\n",
            "robustness and effectiveness.\n",
            "\n",
            "**Paper Id :2003.11637 \n",
            "Title :Bio-inspired Optimization: metaheuristic algorithms for optimization\n",
            "  In today's day and time solving real-world complex problems has become\n",
            "fundamentally vital and critical task. Many of these are combinatorial\n",
            "problems, where optimal solutions are sought rather than exact solutions.\n",
            "Traditional optimization methods are found to be effective for small scale\n",
            "problems. However, for real-world large scale problems, traditional methods\n",
            "either do not scale up or fail to obtain optimal solutions or they end-up\n",
            "giving solutions after a long running time. Even earlier artificial\n",
            "intelligence based techniques used to solve these problems could not give\n",
            "acceptable results. However, last two decades have seen many new methods in AI\n",
            "based on the characteristics and behaviors of the living organisms in the\n",
            "nature which are categorized as bio-inspired or nature inspired optimization\n",
            "algorithms. These methods, are also termed meta-heuristic optimization methods,\n",
            "have been proved theoretically and implemented using simulation as well used to\n",
            "create many useful applications. They have been used extensively to solve many\n",
            "industrial and engineering complex problems due to being easy to understand,\n",
            "flexible, simple to adapt to the problem at hand and most importantly their\n",
            "ability to come out of local optima traps. This local optima avoidance property\n",
            "helps in finding global optimal solutions. This paper is aimed at understanding\n",
            "how nature has inspired many optimization algorithms, basic categorization of\n",
            "them, major bio-inspired optimization algorithms invented in recent time with\n",
            "their applications.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.06850 \n",
            "Title :Trading the System Efficiency for the Income Equality of Drivers in\n",
            "  Rideshare\n",
            "  Several scientific studies have reported the existence of the income gap\n",
            "among rideshare drivers based on demographic factors such as gender, age, race,\n",
            "etc. In this paper, we study the income inequality among rideshare drivers due\n",
            "to discriminative cancellations from riders, and the tradeoff between the\n",
            "income inequality (called fairness objective) with the system efficiency\n",
            "(called profit objective). We proposed an online bipartite-matching model where\n",
            "riders are assumed to arrive sequentially following a distribution known in\n",
            "advance. The highlight of our model is the concept of acceptance rate between\n",
            "any pair of driver-rider types, where types are defined based on demographic\n",
            "factors. Specially, we assume each rider can accept or cancel the driver\n",
            "assigned to her, each occurs with a certain probability which reflects the\n",
            "acceptance degree from the rider type towards the driver type. We construct a\n",
            "bi-objective linear program as a valid benchmark and propose two LP-based\n",
            "parameterized online algorithms. Rigorous online competitive ratio analysis is\n",
            "offered to demonstrate the flexibility and efficiency of our online algorithms\n",
            "in balancing the two conflicting goals, promotions of fairness and profit.\n",
            "Experimental results on a real-world dataset are provided as well, which\n",
            "confirm our theoretical predictions.\n",
            "\n",
            "**Paper Id :1805.09235 \n",
            "Title :Cramer-Wold AutoEncoder\n",
            "  We propose a new generative model, Cramer-Wold Autoencoder (CWAE). Following\n",
            "WAE, we directly encourage normality of the latent space. Our paper uses also\n",
            "the recent idea from Sliced WAE (SWAE) model, which uses one-dimensional\n",
            "projections as a method of verifying closeness of two distributions. The\n",
            "crucial new ingredient is the introduction of a new (Cramer-Wold) metric in the\n",
            "space of densities, which replaces the Wasserstein metric used in SWAE. We show\n",
            "that the Cramer-Wold metric between Gaussian mixtures is given by a simple\n",
            "analytic formula, which results in the removal of sampling necessary to\n",
            "estimate the cost function in WAE and SWAE models. As a consequence, while\n",
            "drastically simplifying the optimization procedure, CWAE produces samples of a\n",
            "matching perceptual quality to other SOTA models.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.06907 \n",
            "Title :PAIRS AutoGeo: an Automated Machine Learning Framework for Massive\n",
            "  Geospatial Data\n",
            "  An automated machine learning framework for geospatial data named PAIRS\n",
            "AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform.\n",
            "The framework simplifies the development of industrial machine learning\n",
            "solutions leveraging geospatial data to the extent that the user inputs are\n",
            "minimized to merely a text file containing labeled GPS coordinates. PAIRS\n",
            "AutoGeo automatically gathers required data at the location coordinates,\n",
            "assembles the training data, performs quality check, and trains multiple\n",
            "machine learning models for subsequent deployment. The framework is validated\n",
            "using a realistic industrial use case of tree species classification.\n",
            "Open-source tree species data are used as the input to train a random forest\n",
            "classifier and a modified ResNet model for 10-way tree species classification\n",
            "based on aerial imagery, which leads to an accuracy of $59.8\\%$ and $81.4\\%$,\n",
            "respectively. This use case exemplifies how PAIRS AutoGeo enables users to\n",
            "leverage machine learning without extensive geospatial expertise.\n",
            "\n",
            "**Paper Id :2003.01668 \n",
            "Title :Model Assertions for Monitoring and Improving ML Models\n",
            "  ML models are increasingly deployed in settings with real world interactions\n",
            "such as vehicles, but unfortunately, these models can fail in systematic ways.\n",
            "To prevent errors, ML engineering teams monitor and continuously improve these\n",
            "models. We propose a new abstraction, model assertions, that adapts the\n",
            "classical use of program assertions as a way to monitor and improve ML models.\n",
            "Model assertions are arbitrary functions over a model's input and output that\n",
            "indicate when errors may be occurring, e.g., a function that triggers if an\n",
            "object rapidly changes its class in a video. We propose methods of using model\n",
            "assertions at all stages of ML system deployment, including runtime monitoring,\n",
            "validating labels, and continuously improving ML models. For runtime\n",
            "monitoring, we show that model assertions can find high confidence errors,\n",
            "where a model returns the wrong output with high confidence, which\n",
            "uncertainty-based monitoring techniques would not detect. For training, we\n",
            "propose two methods of using model assertions. First, we propose a bandit-based\n",
            "active learning algorithm that can sample from data flagged by assertions and\n",
            "show that it can reduce labeling costs by up to 40% over traditional\n",
            "uncertainty-based methods. Second, we propose an API for generating\n",
            "\"consistency assertions\" (e.g., the class change example) and weak labels for\n",
            "inputs where the consistency assertions fail, and show that these weak labels\n",
            "can improve relative model quality by up to 46%. We evaluate model assertions\n",
            "on four real-world tasks with video, LIDAR, and ECG data.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.09108 \n",
            "Title :A Systematic Mapping Study in AIOps\n",
            "  IT systems of today are becoming larger and more complex, rendering their\n",
            "human supervision more difficult. Artificial Intelligence for IT Operations\n",
            "(AIOps) has been proposed to tackle modern IT administration challenges thanks\n",
            "to AI and Big Data. However, past AIOps contributions are scattered,\n",
            "unorganized and missing a common terminology convention, which renders their\n",
            "discovery and comparison impractical. In this work, we conduct an in-depth\n",
            "mapping study to collect and organize the numerous scattered contributions to\n",
            "AIOps in a unique reference index. We create an AIOps taxonomy to build a\n",
            "foundation for future contributions and allow an efficient comparison of AIOps\n",
            "papers treating similar problems. We investigate temporal trends and classify\n",
            "AIOps contributions based on the choice of algorithms, data sources and the\n",
            "target components. Our results show a recent and growing interest towards\n",
            "AIOps, specifically to those contributions treating failure-related tasks\n",
            "(62%), such as anomaly detection and root cause analysis.\n",
            "\n",
            "**Paper Id :2010.01479 \n",
            "Title :Explanation Ontology: A Model of Explanations for User-Centered AI\n",
            "  Explainability has been a goal for Artificial Intelligence (AI) systems since\n",
            "their conception, with the need for explainability growing as more complex AI\n",
            "models are increasingly used in critical, high-stakes settings such as\n",
            "healthcare. Explanations have often added to an AI system in a non-principled,\n",
            "post-hoc manner. With greater adoption of these systems and emphasis on\n",
            "user-centric explainability, there is a need for a structured representation\n",
            "that treats explainability as a primary consideration, mapping end user needs\n",
            "to specific explanation types and the system's AI capabilities. We design an\n",
            "explanation ontology to model both the role of explanations, accounting for the\n",
            "system and user attributes in the process, and the range of different\n",
            "literature-derived explanation types. We indicate how the ontology can support\n",
            "user requirements for explanations in the domain of healthcare. We evaluate our\n",
            "ontology with a set of competency questions geared towards a system designer\n",
            "who might use our ontology to decide which explanation types to include, given\n",
            "a combination of users' needs and a system's capabilities, both in system\n",
            "design settings and in real-time operations. Through the use of this ontology,\n",
            "system designers will be able to make informed choices on which explanations AI\n",
            "systems can and should provide.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.10141 \n",
            "Title :MASSIVE: Tractable and Robust Bayesian Learning of Many-Dimensional\n",
            "  Instrumental Variable Models\n",
            "  The recent availability of huge, many-dimensional data sets, like those\n",
            "arising from genome-wide association studies (GWAS), provides many\n",
            "opportunities for strengthening causal inference. One popular approach is to\n",
            "utilize these many-dimensional measurements as instrumental variables\n",
            "(instruments) for improving the causal effect estimate between other pairs of\n",
            "variables. Unfortunately, searching for proper instruments in a\n",
            "many-dimensional set of candidates is a daunting task due to the intractable\n",
            "model space and the fact that we cannot directly test which of these candidates\n",
            "are valid, so most existing search methods either rely on overly stringent\n",
            "modeling assumptions or fail to capture the inherent model uncertainty in the\n",
            "selection process. We show that, as long as at least some of the candidates are\n",
            "(close to) valid, without knowing a priori which ones, they collectively still\n",
            "pose enough restrictions on the target interaction to obtain a reliable causal\n",
            "effect estimate. We propose a general and efficient causal inference algorithm\n",
            "that accounts for model uncertainty by performing Bayesian model averaging over\n",
            "the most promising many-dimensional instrumental variable models, while at the\n",
            "same time employing weaker assumptions regarding the data generating process.\n",
            "We showcase the efficiency, robustness and predictive performance of our\n",
            "algorithm through experimental results on both simulated and real-world data.\n",
            "\n",
            "**Paper Id :2011.01542 \n",
            "Title :Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space\n",
            "  Entropy Search Approach\n",
            "  We study the novel problem of blackbox optimization of multiple objectives\n",
            "via multi-fidelity function evaluations that vary in the amount of resources\n",
            "consumed and their accuracy. The overall goal is to approximate the true Pareto\n",
            "set of solutions by minimizing the resources consumed for function evaluations.\n",
            "For example, in power system design optimization, we need to find designs that\n",
            "trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity\n",
            "simulators for design evaluations. In this paper, we propose a novel approach\n",
            "referred as Multi-Fidelity Output Space Entropy Search for Multi-objective\n",
            "Optimization (MF-OSEMO) to solve this problem. The key idea is to select the\n",
            "sequence of candidate input and fidelity-vector pairs that maximize the\n",
            "information gained about the true Pareto front per unit resource cost. Our\n",
            "experiments on several synthetic and real-world benchmark problems show that\n",
            "MF-OSEMO, with both approximations, significantly improves over the\n",
            "state-of-the-art single-fidelity algorithms for multi-objective optimization.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.10232 \n",
            "Title :Artificial Intelligence ordered 3D vertex importance\n",
            "  Ranking vertices of multidimensional networks is crucial in many areas of\n",
            "research, including selecting and determining the importance of decisions. Some\n",
            "decisions are significantly more important than others, and their weight\n",
            "categorization is also imortant. This paper defines a completely new method for\n",
            "determining the weight decisions using artificial intelligence for importance\n",
            "ranking of three-dimensional network vertices, improving the existing Ordered\n",
            "Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on\n",
            "modulation of quantized indices (QIM) and error correction codes. The technique\n",
            "we propose in this paper offers significant improvements the efficiency of\n",
            "determination the importance of network vertices in relation to statistical\n",
            "OSVETA criteria, replacing heuristic methods with methods of precise prediction\n",
            "of modern neural networks. The new artificial intelligence technique enables a\n",
            "significantly better definition of the 3D meshes and a better assessment of\n",
            "their topological features. The new method contributions result in a greater\n",
            "precision in defining stable vertices, significantly reducing the probability\n",
            "of deleting mesh vertices.\n",
            "\n",
            "**Paper Id :1911.11460 \n",
            "Title :Comprehensive decision-strategy space exploration for efficient\n",
            "  territorial planning strategies\n",
            "  GIS-based Multi-Criteria Decision Analysis is a well-known decision support\n",
            "tool that can be used in a wide variety of contexts. It is particularly useful\n",
            "for territorial planning in situations where several actors with different, and\n",
            "sometimes contradictory, point of views have to take a decision regarding land\n",
            "use development. While the impact of the weights used to represent the relative\n",
            "importance of criteria has been widely studied in the recent literature, the\n",
            "impact of the order weights used to combine the criteria have rarely been\n",
            "investigated. This paper presents a spatial sensitivity analysis to assess the\n",
            "impact of order weights determination in GIS-based Multi-Criteria Analysis by\n",
            "Ordered Weighted Averaging. We propose a methodology based on an efficient\n",
            "exploration of the decision-strategy space defined by the level of risk and\n",
            "trade-off in the decision process. We illustrate our approach with a land use\n",
            "planning process in the South of France. The objective is to find suitable\n",
            "areas for urban development while preserving green areas and their associated\n",
            "ecosystem services. The ecosystem service approach has indeed the potential to\n",
            "widen the scope of traditional landscape-ecological planning by including\n",
            "ecosystem-based benefits, including social and economic benefits, green\n",
            "infrastructures and biophysical parameters in urban and territorial planning.\n",
            "We show that in this particular case the decision-strategy space can be divided\n",
            "into four clusters. Each of them is associated with a map summarizing the\n",
            "average spatial suitability distribution used to identify potential areas for\n",
            "urban development. We also demonstrate the pertinence of a spatial variance\n",
            "within-cluster analysis to disentangle the relationship between risk and\n",
            "trade-off values. At the end, we perform a site suitability ranking analysis to\n",
            "assess the relationship between the four detected clusters.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.10473 \n",
            "Title :State Estimation of Power Flows for Smart Grids via Belief Propagation\n",
            "  Belief propagation is an algorithm that is known from statistical physics and\n",
            "computer science. It provides an efficient way of calculating marginals that\n",
            "involve large sums of products which are efficiently rearranged into nested\n",
            "products of sums to approximate the marginals. It allows a reliable estimation\n",
            "of the state and its variance of power grids that is needed for the control and\n",
            "forecast of power grid management. At prototypical examples of IEEE-grids we\n",
            "show that belief propagation not only scales linearly with the grid size for\n",
            "the state estimation itself, but also facilitates and accelerates the retrieval\n",
            "of missing data and allows an optimized positioning of measurement units. Based\n",
            "on belief propagation, we give a criterion for how to assess whether other\n",
            "algorithms, using only local information, are adequate for state estimation for\n",
            "a given grid. We also demonstrate how belief propagation can be utilized for\n",
            "coarse-graining power grids towards representations that reduce the\n",
            "computational effort when the coarse-grained version is integrated into a\n",
            "larger grid. It provides a criterion for partitioning power grids into areas in\n",
            "order to minimize the error of flow estimates between different areas.\n",
            "\n",
            "**Paper Id :2211.08430 \n",
            "Title :Power-law Scaling to Assist with Key Challenges in Artificial\n",
            "  Intelligence\n",
            "  Power-law scaling, a central concept in critical phenomena, is found to be\n",
            "useful in deep learning, where optimized test errors on handwritten digit\n",
            "examples converge as a power-law to zero with database size. For rapid decision\n",
            "making with one training epoch, each example is presented only once to the\n",
            "trained network, the power-law exponent increased with the number of hidden\n",
            "layers. For the largest dataset, the obtained test error was estimated to be in\n",
            "the proximity of state-of-the-art algorithms for large epoch numbers. Power-law\n",
            "scaling assists with key challenges found in current artificial intelligence\n",
            "applications and facilitates an a priori dataset size estimation to achieve a\n",
            "desired test accuracy. It establishes a benchmark for measuring training\n",
            "complexity and a quantitative hierarchy of machine learning tasks and\n",
            "algorithms.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.13137 \n",
            "Title :WEmbSim: A Simple yet Effective Metric for Image Captioning\n",
            "  The area of automatic image caption evaluation is still undergoing intensive\n",
            "research to address the needs of generating captions which can meet adequacy\n",
            "and fluency requirements. Based on our past attempts at developing highly\n",
            "sophisticated learning-based metrics, we have discovered that a simple cosine\n",
            "similarity measure using the Mean of Word Embeddings(MOWE) of captions can\n",
            "actually achieve a surprisingly high performance on unsupervised caption\n",
            "evaluation. This inspires our proposed work on an effective metric WEmbSim,\n",
            "which beats complex measures such as SPICE, CIDEr and WMD at system-level\n",
            "correlation with human judgments. Moreover, it also achieves the best accuracy\n",
            "at matching human consensus scores for caption pairs, against commonly used\n",
            "unsupervised methods. Therefore, we believe that WEmbSim sets a new baseline\n",
            "for any complex metric to be justified.\n",
            "\n",
            "**Paper Id :2005.10716 \n",
            "Title :Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for\n",
            "  Automatic Dialog Evaluation\n",
            "  Open Domain dialog system evaluation is one of the most important challenges\n",
            "in dialog research. Existing automatic evaluation metrics, such as BLEU are\n",
            "mostly reference-based. They calculate the difference between the generated\n",
            "response and a limited number of available references. Likert-score based\n",
            "self-reported user rating is widely adopted by social conversational systems,\n",
            "such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\n",
            "from bias and variance among different users. To alleviate this problem, we\n",
            "formulate dialog evaluation as a comparison task. We also propose an automatic\n",
            "evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\n",
            "automatically cleans self-reported user ratings as it trains on them.\n",
            "Specifically, we first use a self-supervised method to learn better dialog\n",
            "feature representation, and then use KNN and Shapley to remove confusing\n",
            "samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\n",
            "comparison task.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.13204 \n",
            "Title :Predicting Seminal Quality with the Dominance-Based Rough Sets Approach\n",
            "  The paper relies on the clinical data of a previously published study. We\n",
            "identify two very questionable assumptions of said work, namely confusing\n",
            "evidence of absence and absence of evidence, and neglecting the ordinal nature\n",
            "of attributes' domains. We then show that using an adequate ordinal methodology\n",
            "such as the dominance-based rough sets approach (DRSA) can significantly\n",
            "improve the predictive accuracy of the expert system, resulting in almost\n",
            "complete accuracy for a dataset of 100 instances. Beyond the performance of\n",
            "DRSA in solving the diagnosis problem at hand, these results suggest the\n",
            "inadequacy and triviality of the underlying dataset. We provide links to open\n",
            "data from the UCI machine learning repository to allow for an easy\n",
            "verification/refutation of the claims made in this paper.\n",
            "\n",
            "**Paper Id :2009.13033 \n",
            "Title :Where Does the Robustness Come from? A Study of the Transformation-based\n",
            "  Ensemble Defence\n",
            "  This paper aims to provide a thorough study on the effectiveness of the\n",
            "transformation-based ensemble defence for image classification and its reasons.\n",
            "It has been empirically shown that they can enhance the robustness against\n",
            "evasion attacks, while there is little analysis on the reasons. In particular,\n",
            "it is not clear whether the robustness improvement is a result of\n",
            "transformation or ensemble. In this paper, we design two adaptive attacks to\n",
            "better evaluate the transformation-based ensemble defence. We conduct\n",
            "experiments to show that 1) the transferability of adversarial examples exists\n",
            "among the models trained on data records after different reversible\n",
            "transformations; 2) the robustness gained through transformation-based ensemble\n",
            "is limited; 3) this limited robustness is mainly from the irreversible\n",
            "transformations rather than the ensemble of a number of models; and 4) blindly\n",
            "increasing the number of sub-models in a transformation-based ensemble does not\n",
            "bring extra robustness gain.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.13978 \n",
            "Title :MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language\n",
            "  Understanding Pretraining\n",
            "  One of the biggest challenges that prohibit the use of many current NLP\n",
            "methods in clinical settings is the availability of public datasets. In this\n",
            "work, we present MeDAL, a large medical text dataset curated for abbreviation\n",
            "disambiguation, designed for natural language understanding pre-training in the\n",
            "medical domain. We pre-trained several models of common architectures on this\n",
            "dataset and empirically showed that such pre-training leads to improved\n",
            "performance and convergence speed when fine-tuning on downstream medical tasks.\n",
            "\n",
            "**Paper Id :2006.05832 \n",
            "Title :Adaptive Reinforcement Learning through Evolving Self-Modifying Neural\n",
            "  Networks\n",
            "  The adaptive learning capabilities seen in biological neural networks are\n",
            "largely a product of the self-modifying behavior emerging from online plastic\n",
            "changes in synaptic connectivity. Current methods in Reinforcement Learning\n",
            "(RL) only adjust to new interactions after reflection over a specified time\n",
            "interval, preventing the emergence of online adaptivity. Recent work addressing\n",
            "this by endowing artificial neural networks with neuromodulated plasticity have\n",
            "been shown to improve performance on simple RL tasks trained using\n",
            "backpropagation, but have yet to scale up to larger problems. Here we study the\n",
            "problem of meta-learning in a challenging quadruped domain, where each leg of\n",
            "the quadruped has a chance of becoming unusable, requiring the agent to adapt\n",
            "by continuing locomotion with the remaining limbs. Results demonstrate that\n",
            "agents evolved using self-modifying plastic networks are more capable of\n",
            "adapting to complex meta-learning learning tasks, even outperforming the same\n",
            "network updated using gradient-based algorithms while taking less time to\n",
            "train.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.14794 \n",
            "Title :A Deep Reinforcement Learning Based Multi-Criteria Decision Support\n",
            "  System for Textile Manufacturing Process Optimization\n",
            "  Textile manufacturing is a typical traditional industry involving high\n",
            "complexity in interconnected processes with limited capacity on the application\n",
            "of modern technologies. Decision-making in this domain generally takes multiple\n",
            "criteria into consideration, which usually arouses more complexity. To address\n",
            "this issue, the present paper proposes a decision support system that combines\n",
            "the intelligent data-based random forest (RF) models and a human knowledge\n",
            "based analytical hierarchical process (AHP) multi-criteria structure in\n",
            "accordance to the objective and the subjective factors of the textile\n",
            "manufacturing process. More importantly, the textile manufacturing process is\n",
            "described as the Markov decision process (MDP) paradigm, and a deep\n",
            "reinforcement learning scheme, the Deep Q-networks (DQN), is employed to\n",
            "optimize it. The effectiveness of this system has been validated in a case\n",
            "study of optimizing a textile ozonation process, showing that it can better\n",
            "master the challenging decision-making tasks in textile manufacturing\n",
            "processes.\n",
            "\n",
            "**Paper Id :2007.03580 \n",
            "Title :Using Semantic Web Services for AI-Based Research in Industry 4.0\n",
            "  The transition to Industry 4.0 requires smart manufacturing systems that are\n",
            "easily configurable and provide a high level of flexibility during\n",
            "manufacturing in order to achieve mass customization or to support cloud\n",
            "manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with\n",
            "Artificial Intelligence (AI) methods find their way into manufacturing shop\n",
            "floors. For using AI methods in the context of Industry 4.0, semantic web\n",
            "services are indispensable to provide a reasonable abstraction of the\n",
            "underlying manufacturing capabilities. In this paper, we present semantic web\n",
            "services for AI-based research in Industry 4.0. Therefore, we developed more\n",
            "than 300 semantic web services for a physical simulation factory based on Web\n",
            "Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology\n",
            "(WSMO) and linked them to an already existing domain ontology for intelligent\n",
            "manufacturing control. Suitable for the requirements of CPS environments, our\n",
            "pre- and postconditions are verified in near real-time by invoking other\n",
            "semantic web services in contrast to complex reasoning within the knowledge\n",
            "base. Finally, we evaluate our implementation by executing a cyber-physical\n",
            "workflow composed of semantic web services using a workflow management system.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.14840 \n",
            "Title :Object sorting using faster R-CNN\n",
            "  In a factory production line, different industry parts need to be quickly\n",
            "differentiated and sorted for further process. Parts can be of different colors\n",
            "and shapes. It is tedious for humans to differentiate and sort these objects in\n",
            "appropriate categories. Automating this process would save more time and cost.\n",
            "In the automation process, choosing an appropriate model to detect and classify\n",
            "different objects based on specific features is more challenging. In this\n",
            "paper, three different neural network models are compared to the object sorting\n",
            "system. They are namely CNN, Fast R-CNN, and Faster R-CNN. These models are\n",
            "tested, and their performance is analyzed. Moreover, for the object sorting\n",
            "system, an Arduino-controlled 5 DoF (degree of freedom) robot arm is programmed\n",
            "to grab and drop symmetrical objects to the targeted zone. Objects are\n",
            "categorized into classes based on color, defective and non-defective objects.\n",
            "\n",
            "**Paper Id :2010.05609 \n",
            "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
            "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
            "on a variety of Natural Language Processing data sets. However, the size of\n",
            "these models is often a drawback for their deployment in real production\n",
            "applications. In the case of multilingual models, most of the parameters are\n",
            "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
            "have an important impact on the total number of parameters. In this paper, we\n",
            "propose to generate smaller models that handle fewer number of languages\n",
            "according to the targeted corpora. We present an evaluation of smaller versions\n",
            "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
            "be applied to other multilingual transformers. The obtained results confirm\n",
            "that we can generate smaller models that keep comparable results, while\n",
            "reducing up to 45% of the total number of parameters. We compared our models\n",
            "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
            "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
            "overall accuracy on the XNLI data set. The presented models and code are\n",
            "publicly available.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.15635 \n",
            "Title :Leveraging Audio Gestalt to Predict Media Memorability\n",
            "  Memorability determines what evanesces into emptiness, and what worms its way\n",
            "into the deepest furrows of our minds. It is the key to curating more\n",
            "meaningful media content as we wade through daily digital torrents. The\n",
            "Predicting Media Memorability task in MediaEval 2020 aims to address the\n",
            "question of media memorability by setting the task of automatically predicting\n",
            "video memorability. Our approach is a multimodal deep learning-based late\n",
            "fusion that combines visual, semantic, and auditory features. We used audio\n",
            "gestalt to estimate the influence of the audio modality on overall video\n",
            "memorability, and accordingly inform which combination of features would best\n",
            "predict a given video's memorability scores.\n",
            "\n",
            "**Paper Id :2012.15650 \n",
            "Title :Overview of MediaEval 2020 Predicting Media Memorability Task: What\n",
            "  Makes a Video Memorable?\n",
            "  This paper describes the MediaEval 2020 \\textit{Predicting Media\n",
            "Memorability} task. After first being proposed at MediaEval 2018, the\n",
            "Predicting Media Memorability task is in its 3rd edition this year, as the\n",
            "prediction of short-term and long-term video memorability (VM) remains a\n",
            "challenging task. In 2020, the format remained the same as in previous\n",
            "editions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\n",
            "dataset, containing more action rich video content as compared with the 2019\n",
            "task. In this paper a description of some aspects of this task is provided,\n",
            "including its main characteristics, a description of the collection, the ground\n",
            "truth dataset, evaluation metrics and the requirements for participants' run\n",
            "submissions.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.15641 \n",
            "Title :Investigating Memorability of Dynamic Media\n",
            "  The Predicting Media Memorability task in MediaEval'20 has some challenging\n",
            "aspects compared to previous years. In this paper we identify the high-dynamic\n",
            "content in videos and dataset of limited size as the core challenges for the\n",
            "task, we propose directions to overcome some of these challenges and we present\n",
            "our initial result in these directions.\n",
            "\n",
            "**Paper Id :2012.15650 \n",
            "Title :Overview of MediaEval 2020 Predicting Media Memorability Task: What\n",
            "  Makes a Video Memorable?\n",
            "  This paper describes the MediaEval 2020 \\textit{Predicting Media\n",
            "Memorability} task. After first being proposed at MediaEval 2018, the\n",
            "Predicting Media Memorability task is in its 3rd edition this year, as the\n",
            "prediction of short-term and long-term video memorability (VM) remains a\n",
            "challenging task. In 2020, the format remained the same as in previous\n",
            "editions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\n",
            "dataset, containing more action rich video content as compared with the 2019\n",
            "task. In this paper a description of some aspects of this task is provided,\n",
            "including its main characteristics, a description of the collection, the ground\n",
            "truth dataset, evaluation metrics and the requirements for participants' run\n",
            "submissions.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2012.15650 \n",
            "Title :Overview of MediaEval 2020 Predicting Media Memorability Task: What\n",
            "  Makes a Video Memorable?\n",
            "  This paper describes the MediaEval 2020 \\textit{Predicting Media\n",
            "Memorability} task. After first being proposed at MediaEval 2018, the\n",
            "Predicting Media Memorability task is in its 3rd edition this year, as the\n",
            "prediction of short-term and long-term video memorability (VM) remains a\n",
            "challenging task. In 2020, the format remained the same as in previous\n",
            "editions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\n",
            "dataset, containing more action rich video content as compared with the 2019\n",
            "task. In this paper a description of some aspects of this task is provided,\n",
            "including its main characteristics, a description of the collection, the ground\n",
            "truth dataset, evaluation metrics and the requirements for participants' run\n",
            "submissions.\n",
            "\n",
            "**Paper Id :2012.15641 \n",
            "Title :Investigating Memorability of Dynamic Media\n",
            "  The Predicting Media Memorability task in MediaEval'20 has some challenging\n",
            "aspects compared to previous years. In this paper we identify the high-dynamic\n",
            "content in videos and dataset of limited size as the core challenges for the\n",
            "task, we propose directions to overcome some of these challenges and we present\n",
            "our initial result in these directions.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.03026 \n",
            "Title :Scalable Cross-lingual Document Similarity through Language-specific\n",
            "  Concept Hierarchies\n",
            "  With the ongoing growth in number of digital articles in a wider set of\n",
            "languages and the expanding use of different languages, we need annotation\n",
            "methods that enable browsing multi-lingual corpora. Multilingual probabilistic\n",
            "topic models have recently emerged as a group of semi-supervised machine\n",
            "learning models that can be used to perform thematic explorations on\n",
            "collections of texts in multiple languages. However, these approaches require\n",
            "theme-aligned training data to create a language-independent space. This\n",
            "constraint limits the amount of scenarios that this technique can offer\n",
            "solutions to train and makes it difficult to scale up to situations where a\n",
            "huge collection of multi-lingual documents are required during the training\n",
            "phase. This paper presents an unsupervised document similarity algorithm that\n",
            "does not require parallel or comparable corpora, or any other type of\n",
            "translation resource. The algorithm annotates topics automatically created from\n",
            "documents in a single language with cross-lingual labels and describes\n",
            "documents by hierarchies of multi-lingual concepts from independently-trained\n",
            "models. Experiments performed on the English, Spanish and French editions of\n",
            "JCR-Acquis corpora reveal promising results on classifying and sorting\n",
            "documents by similar content.\n",
            "\n",
            "**Paper Id :2010.05609 \n",
            "Title :Load What You Need: Smaller Versions of Multilingual BERT\n",
            "  Pre-trained Transformer-based models are achieving state-of-the-art results\n",
            "on a variety of Natural Language Processing data sets. However, the size of\n",
            "these models is often a drawback for their deployment in real production\n",
            "applications. In the case of multilingual models, most of the parameters are\n",
            "located in the embeddings layer. Therefore, reducing the vocabulary size should\n",
            "have an important impact on the total number of parameters. In this paper, we\n",
            "propose to generate smaller models that handle fewer number of languages\n",
            "according to the targeted corpora. We present an evaluation of smaller versions\n",
            "of multilingual BERT on the XNLI data set, but we believe that this method may\n",
            "be applied to other multilingual transformers. The obtained results confirm\n",
            "that we can generate smaller models that keep comparable results, while\n",
            "reducing up to 45% of the total number of parameters. We compared our models\n",
            "with DistilmBERT (a distilled version of multilingual BERT) and showed that\n",
            "unlike language reduction, distillation induced a 1.7% to 6% drop in the\n",
            "overall accuracy on the XNLI data set. The presented models and code are\n",
            "publicly available.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.03678 \n",
            "Title :Time-Series Regeneration with Convolutional Recurrent Generative\n",
            "  Adversarial Network for Remaining Useful Life Estimation\n",
            "  For health prognostic task, ever-increasing efforts have been focused on\n",
            "machine learning-based methods, which are capable of yielding accurate\n",
            "remaining useful life (RUL) estimation for industrial equipment or components\n",
            "without exploring the degradation mechanism. A prerequisite ensuring the\n",
            "success of these methods depends on a wealth of run-to-failure data, however,\n",
            "run-to-failure data may be insufficient in practice. That is, conducting a\n",
            "substantial amount of destructive experiments not only is high costs, but also\n",
            "may cause catastrophic consequences. Out of this consideration, an enhanced RUL\n",
            "framework focusing on data self-generation is put forward for both non-cyclic\n",
            "and cyclic degradation patterns for the first time. It is designed to enrich\n",
            "data from a data-driven way, generating realistic-like time-series to enhance\n",
            "current RUL methods. First, high-quality data generation is ensured through the\n",
            "proposed convolutional recurrent generative adversarial network (CR-GAN), which\n",
            "adopts a two-channel fusion convolutional recurrent neural network. Next, a\n",
            "hierarchical framework is proposed to combine generated data into current RUL\n",
            "estimation methods. Finally, the efficacy of the proposed method is verified\n",
            "through both non-cyclic and cyclic degradation systems. With the enhanced RUL\n",
            "framework, an aero-engine system following non-cyclic degradation has been\n",
            "tested using three typical RUL models. State-of-art RUL estimation results are\n",
            "achieved by enhancing capsule network with generated time-series. Specifically,\n",
            "estimation errors evaluated by the index score function have been reduced by\n",
            "21.77%, and 32.67% for the two employed operating conditions, respectively.\n",
            "Besides, the estimation error is reduced to zero for the Lithium-ion battery\n",
            "system, which presents cyclic degradation.\n",
            "\n",
            "**Paper Id :2010.09468 \n",
            "Title :Chance-Constrained Control with Lexicographic Deep Reinforcement\n",
            "  Learning\n",
            "  This paper proposes a lexicographic Deep Reinforcement Learning\n",
            "(DeepRL)-based approach to chance-constrained Markov Decision Processes, in\n",
            "which the controller seeks to ensure that the probability of satisfying the\n",
            "constraint is above a given threshold. Standard DeepRL approaches require i)\n",
            "the constraints to be included as additional weighted terms in the cost\n",
            "function, in a multi-objective fashion, and ii) the tuning of the introduced\n",
            "weights during the training phase of the Deep Neural Network (DNN) according to\n",
            "the probability thresholds. The proposed approach, instead, requires to\n",
            "separately train one constraint-free DNN and one DNN associated to each\n",
            "constraint and then, at each time-step, to select which DNN to use depending on\n",
            "the system observed state. The presented solution does not require any\n",
            "hyper-parameter tuning besides the standard DNN ones, even if the probability\n",
            "thresholds changes. A lexicographic version of the well-known DeepRL algorithm\n",
            "DQN is also proposed and validated via simulations.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.05077 \n",
            "Title :Restyling Images with the Bangladeshi Paintings Using Neural Style\n",
            "  Transfer: A Comprehensive Experiment, Evaluation, and Human Perspective\n",
            "  In today's world, Neural Style Transfer (NST) has become a trendsetting term.\n",
            "NST combines two pictures, a content picture and a reference image in style\n",
            "(such as the work of a renowned painter) in a way that makes the output image\n",
            "look like an image of the material, but rendered with the form of a reference\n",
            "picture. However, there is no study using the artwork or painting of\n",
            "Bangladeshi painters. Bangladeshi painting has a long history of more than two\n",
            "thousand years and is still being practiced by Bangladeshi painters. This study\n",
            "generates NST stylized image on Bangladeshi paintings and analyzes the human\n",
            "point of view regarding the aesthetic preference of NST on Bangladeshi\n",
            "paintings. To assure our study's acceptance, we performed qualitative human\n",
            "evaluations on generated stylized images by 60 individual humans of different\n",
            "age and gender groups. We have explained how NST works for Bangladeshi\n",
            "paintings and assess NST algorithms, both qualitatively \\& quantitatively. Our\n",
            "study acts as a pre-requisite for the impact of NST stylized image using\n",
            "Bangladeshi paintings on mobile UI/GUI and material translation from the human\n",
            "perspective. We hope that this study will encourage new collaborations to\n",
            "create more NST related studies and expand the use of Bangladeshi artworks.\n",
            "\n",
            "**Paper Id :2010.04922 \n",
            "Title :Structured Self-Attention Weights Encode Semantics in Sentiment Analysis\n",
            "  Neural attention, especially the self-attention made popular by the\n",
            "Transformer, has become the workhorse of state-of-the-art natural language\n",
            "processing (NLP) models. Very recent work suggests that the self-attention in\n",
            "the Transformer encodes syntactic information; Here, we show that\n",
            "self-attention scores encode semantics by considering sentiment analysis tasks.\n",
            "In contrast to gradient-based feature attribution methods, we propose a simple\n",
            "and effective Layer-wise Attention Tracing (LAT) method to analyze structured\n",
            "attention weights. We apply our method to Transformer models trained on two\n",
            "tasks that have surface dissimilarities, but share common semantics---sentiment\n",
            "analysis of movie reviews and time-series valence prediction in life story\n",
            "narratives. Across both tasks, words with high aggregated attention weights\n",
            "were rich in emotional semantics, as quantitatively validated by an emotion\n",
            "lexicon labeled by human annotators. Our results show that structured attention\n",
            "weights encode rich semantics in sentiment analysis, and match human\n",
            "interpretations of semantics.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.06634 \n",
            "Title :Regional Attention Network (RAN) for Head Pose and Fine-grained Gesture\n",
            "  Recognition\n",
            "  Affect is often expressed via non-verbal body language such as\n",
            "actions/gestures, which are vital indicators for human behaviors. Recent\n",
            "studies on recognition of fine-grained actions/gestures in monocular images\n",
            "have mainly focused on modeling spatial configuration of body parts\n",
            "representing body pose, human-objects interactions and variations in local\n",
            "appearance. The results show that this is a brittle approach since it relies on\n",
            "accurate body parts/objects detection. In this work, we argue that there exist\n",
            "local discriminative semantic regions, whose \"informativeness\" can be evaluated\n",
            "by the attention mechanism for inferring fine-grained gestures/actions. To this\n",
            "end, we propose a novel end-to-end \\textbf{Regional Attention Network (RAN)},\n",
            "which is a fully Convolutional Neural Network (CNN) to combine multiple\n",
            "contextual regions through attention mechanism, focusing on parts of the images\n",
            "that are most relevant to a given task. Our regions consist of one or more\n",
            "consecutive cells and are adapted from the strategies used in computing HOG\n",
            "(Histogram of Oriented Gradient) descriptor. The model is extensively evaluated\n",
            "on ten datasets belonging to 3 different scenarios: 1) head pose recognition,\n",
            "2) drivers state recognition, and 3) human action and facial expression\n",
            "recognition. The proposed approach outperforms the state-of-the-art by a\n",
            "considerable margin in different metrics.\n",
            "\n",
            "**Paper Id :2010.04922 \n",
            "Title :Structured Self-Attention Weights Encode Semantics in Sentiment Analysis\n",
            "  Neural attention, especially the self-attention made popular by the\n",
            "Transformer, has become the workhorse of state-of-the-art natural language\n",
            "processing (NLP) models. Very recent work suggests that the self-attention in\n",
            "the Transformer encodes syntactic information; Here, we show that\n",
            "self-attention scores encode semantics by considering sentiment analysis tasks.\n",
            "In contrast to gradient-based feature attribution methods, we propose a simple\n",
            "and effective Layer-wise Attention Tracing (LAT) method to analyze structured\n",
            "attention weights. We apply our method to Transformer models trained on two\n",
            "tasks that have surface dissimilarities, but share common semantics---sentiment\n",
            "analysis of movie reviews and time-series valence prediction in life story\n",
            "narratives. Across both tasks, words with high aggregated attention weights\n",
            "were rich in emotional semantics, as quantitatively validated by an emotion\n",
            "lexicon labeled by human annotators. Our results show that structured attention\n",
            "weights encode rich semantics in sentiment analysis, and match human\n",
            "interpretations of semantics.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.07523 \n",
            "Title :Mapping and Describing Geospatial Data to Generalize Complex Mapping and\n",
            "  Describing Geospatial Data to Generalize Complex Models: The Case of\n",
            "  LittoSIM-GEN Models\n",
            "  For some scientific questions, empirical data are essential to develop\n",
            "reliable simulation models. These data usually come from different sources with\n",
            "diverse and heterogeneous formats. The design of complex data-driven models is\n",
            "often shaped by the structure of the data available in research projects.\n",
            "Hence, applying such models to other case studies requires either to get\n",
            "similar data or to transform new data to fit the model inputs. It is the case\n",
            "of agent-based models (ABMs) that use advanced data structures such as\n",
            "Geographic Information Systems data. We faced this problem in the LittoSIM-GEN\n",
            "project when generalizing our participatory flooding model (LittoSIM) to new\n",
            "territories. From this experience, we provide a mapping approach to structure,\n",
            "describe, and automatize the integration of geospatial data into ABMs.\n",
            "\n",
            "**Paper Id :1905.04232 \n",
            "Title :Automatic Programming of Cellular Automata and Artificial Neural\n",
            "  Networks Guided by Philosophy\n",
            "  Many computer models such as cellular automata and artificial neural networks\n",
            "have been developed and successfully applied. However, in some cases, these\n",
            "models might be restrictive on the possible solutions or their solutions might\n",
            "be difficult to interpret. To overcome this problem, we outline a new approach,\n",
            "the so-called allagmatic method, that automatically programs and executes\n",
            "models with as little limitations as possible while maintaining human\n",
            "interpretability. Earlier we described a metamodel and its building blocks\n",
            "according to the philosophical concepts of structure (spatial dimension) and\n",
            "operation (temporal dimension). They are entity, milieu, and update function\n",
            "that together abstractly describe cellular automata, artificial neural\n",
            "networks, and possibly any kind of computer model. By automatically combining\n",
            "these building blocks in an evolutionary computation, interpretability might be\n",
            "increased by the relationship to the metamodel, and models might be translated\n",
            "into more interpretable models via the metamodel. We propose generic and\n",
            "object-oriented programming to implement the entities and their milieus as\n",
            "dynamic and generic arrays and the update function as a method. We show two\n",
            "experiments where a simple cellular automaton and an artificial neural network\n",
            "are automatically programmed, compiled, and executed. A target state is\n",
            "successfully evolved and learned in the cellular automaton and artificial\n",
            "neural network, respectively. We conclude that the allagmatic method can create\n",
            "and execute cellular automaton and artificial neural network models in an\n",
            "automated manner with the guidance of philosophy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.09799 \n",
            "Title :Online Memory Leak Detection in the Cloud-based Infrastructures\n",
            "  A memory leak in an application deployed on the cloud can affect the\n",
            "availability and reliability of the application. Therefore, to identify and\n",
            "ultimately resolve it quickly is highly important. However, in the production\n",
            "environment running on the cloud, memory leak detection is a challenge without\n",
            "the knowledge of the application or its internal object allocation details.\n",
            "  This paper addresses this challenge of online detection of memory leaks in\n",
            "cloud-based infrastructure without having any internal application knowledge by\n",
            "introducing a novel machine learning based algorithm Precog. This algorithm\n",
            "solely uses one metric i.e the system's memory utilization on which the\n",
            "application is deployed for the detection of a memory leak. The developed\n",
            "algorithm's accuracy was tested on 60 virtual machines manually labeled memory\n",
            "utilization data provided by our industry partner Huawei Munich Research Center\n",
            "and it was found that the proposed algorithm achieves the accuracy score of\n",
            "85\\% with less than half a second prediction time per virtual machine.\n",
            "\n",
            "**Paper Id :2010.08187 \n",
            "Title :PrivNet: Safeguarding Private Attributes in Transfer Learning for\n",
            "  Recommendation\n",
            "  Transfer learning is an effective technique to improve a target recommender\n",
            "system with the knowledge from a source domain. Existing research focuses on\n",
            "the recommendation performance of the target domain while ignores the privacy\n",
            "leakage of the source domain. The transferred knowledge, however, may\n",
            "unintendedly leak private information of the source domain. For example, an\n",
            "attacker can accurately infer user demographics from their historical purchase\n",
            "provided by a source domain data owner. This paper addresses the above\n",
            "privacy-preserving issue by learning a privacy-aware neural representation by\n",
            "improving target performance while protecting source privacy. The key idea is\n",
            "to simulate the attacks during the training for protecting unseen users'\n",
            "privacy in the future, modeled by an adversarial game, so that the transfer\n",
            "learning model becomes robust to attacks. Experiments show that the proposed\n",
            "PrivNet model can successfully disentangle the knowledge benefitting the\n",
            "transfer from leaking the privacy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.10215 \n",
            "Title :Analysis of Relation between Motor Activity and Imaginary EEG Records\n",
            "  Electroencephalography (EEG) signals signals are often used to learn about\n",
            "brain structure and to learn what thinking. EEG signals can be easily affected\n",
            "by external factors. For this reason, they should be applied various\n",
            "pre-process during their analysis. In this study, it is used the EEG signals\n",
            "received from 109 subjects when opening and closing their right or left fists\n",
            "and performing hand and foot movements and imagining the same movements. The\n",
            "relationship between motor activities and imaginary of that motor activities\n",
            "were investigated. Algorithms with high performance rates have been used for\n",
            "feature extraction , selection and classification using the nearest neighbour\n",
            "algorithm.\n",
            "\n",
            "**Paper Id :2005.11151 \n",
            "Title :Attention Patterns Detection using Brain Computer Interfaces\n",
            "  The human brain provides a range of functions such as expressing emotions,\n",
            "controlling the rate of breathing, etc., and its study has attracted the\n",
            "interest of scientists for many years. As machine learning models become more\n",
            "sophisticated, and bio-metric data becomes more readily available through new\n",
            "non-invasive technologies, it becomes increasingly possible to gain access to\n",
            "interesting biometric data that could revolutionize Human-Computer Interaction.\n",
            "In this research, we propose a method to assess and quantify human attention\n",
            "levels and their effects on learning. In our study, we employ a brain computer\n",
            "interface (BCI) capable of detecting brain wave activity and displaying the\n",
            "corresponding electroencephalograms (EEG). We train recurrent neural networks\n",
            "(RNNS) to identify the type of activity an individual is performing.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.10831 \n",
            "Title :Toxicity Detection in Drug Candidates using Simplified Molecular-Input\n",
            "  Line-Entry System\n",
            "  The need for analysis of toxicity in new drug candidates and the requirement\n",
            "of doing it fast have asked the consideration of scientists towards the use of\n",
            "artificial intelligence tools to examine toxicity levels and to develop models\n",
            "to a degree where they can be used commercially to measure toxicity levels\n",
            "efficiently in upcoming drugs. Artificial Intelligence based models can be used\n",
            "to predict the toxic nature of a chemical using Quantitative Structure Activity\n",
            "Relationship techniques. Convolutional Neural Network models have demonstrated\n",
            "great outcomes in predicting the qualitative analysis of chemicals in order to\n",
            "determine the toxicity. This paper goes for the study of Simplified Molecular\n",
            "Input Line-Entry System (SMILES) as a parameter to develop Long short term\n",
            "memory (LSTM) based models in order to examine the toxicity of a molecule and\n",
            "the degree to which the need can be fulfilled for practical use alongside its\n",
            "future outlooks for the purpose of real world applications.\n",
            "\n",
            "**Paper Id :2105.00375 \n",
            "Title :Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary\n",
            "  Results\n",
            "  Given an on-board diagnostics (OBD) dataset and a physics-based emissions\n",
            "prediction model, this paper aims to develop an accurate and\n",
            "computational-efficient AI (Artificial Intelligence) method that predicts\n",
            "vehicle emissions. The problem is of societal importance because vehicular\n",
            "emissions lead to climate change and impact human health. This problem is\n",
            "challenging because the OBD data does not contain enough parameters needed by\n",
            "high-order physics models. Conversely, related work has shown that low-order\n",
            "physics models have poor predictive accuracy when using available OBD data.\n",
            "This paper uses a divergent window co-occurrence pattern detection method to\n",
            "develop a spatiotemporal variability-aware AI model for predicting emission\n",
            "values from the OBD datasets. We conducted a case study using real-world OBD\n",
            "data from a local public transportation agency. Results show that the proposed\n",
            "AI method has approximately 65% improved predictive accuracy than a non-AI\n",
            "low-order physics model and is approximately 35% more accurate than a baseline\n",
            "model.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.10964 \n",
            "Title :Investment vs. reward in a competitive knapsack problem\n",
            "  Natural selection drives species to develop brains, with sizes that increase\n",
            "with the complexity of the tasks to be tackled. Our goal is to investigate the\n",
            "balance between the metabolic costs of larger brains compared to the advantage\n",
            "they provide in solving general and combinatorial problems. Defining advantage\n",
            "as the performance relative to competitors, a two-player game based on the\n",
            "knapsack problem is used. Within this framework, two opponents compete over\n",
            "shared resources, with the goal of collecting more resources than the opponent.\n",
            "Neural nets of varying sizes are trained using a variant of the AlphaGo Zero\n",
            "algorithm. A surprisingly simple relation, $N_A/(N_A+N_B)$, is found for the\n",
            "relative win rate of a net with $N_A$ neurons against one with $N_B$. Success\n",
            "increases linearly with investments in additional resources when the networks\n",
            "sizes are very different, i.e. when $N_A \\ll N_B$, with returns diminishing\n",
            "when both networks become comparable in size.\n",
            "\n",
            "**Paper Id :2010.08885 \n",
            "Title :A Game AI Competition to foster Collaborative AI research and\n",
            "  development\n",
            "  Game AI competitions are important to foster research and development on Game\n",
            "AI and AI in general. These competitions supply different challenging problems\n",
            "that can be translated into other contexts, virtual or real. They provide\n",
            "frameworks and tools to facilitate the research on their core topics and\n",
            "provide means for comparing and sharing results. A competition is also a way to\n",
            "motivate new researchers to study these challenges. In this document, we\n",
            "present the Geometry Friends Game AI Competition. Geometry Friends is a\n",
            "two-player cooperative physics-based puzzle platformer computer game. The\n",
            "concept of the game is simple, though its solving has proven to be difficult.\n",
            "While the main and apparent focus of the game is cooperation, it also relies on\n",
            "other AI-related problems such as planning, plan execution, and motion control,\n",
            "all connected to situational awareness. All of these must be solved in\n",
            "real-time. In this paper, we discuss the competition and the challenges it\n",
            "brings, and present an overview of the current solutions.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.11436 \n",
            "Title :Challenges Encountered in Turkish Natural Language Processing Studies\n",
            "  Natural language processing is a branch of computer science that combines\n",
            "artificial intelligence with linguistics. It aims to analyze a language element\n",
            "such as writing or speaking with software and convert it into information.\n",
            "Considering that each language has its own grammatical rules and vocabulary\n",
            "diversity, the complexity of the studies in this field is somewhat\n",
            "understandable. For instance, Turkish is a very interesting language in many\n",
            "ways. Examples of this are agglutinative word structure, consonant/vowel\n",
            "harmony, a large number of productive derivational morphemes (practically\n",
            "infinite vocabulary), derivation and syntactic relations, a complex emphasis on\n",
            "vocabulary and phonological rules. In this study, the interesting features of\n",
            "Turkish in terms of natural language processing are mentioned. In addition,\n",
            "summary info about natural language processing techniques, systems and various\n",
            "sources developed for Turkish are given.\n",
            "\n",
            "**Paper Id :2002.01365 \n",
            "Title :Compositional Languages Emerge in a Neural Iterated Learning Model\n",
            "  The principle of compositionality, which enables natural language to\n",
            "represent complex concepts via a structured combination of simpler ones, allows\n",
            "us to convey an open-ended set of messages using a limited vocabulary. If\n",
            "compositionality is indeed a natural property of language, we may expect it to\n",
            "appear in communication protocols that are created by neural agents in language\n",
            "games. In this paper, we propose an effective neural iterated learning (NIL)\n",
            "algorithm that, when applied to interacting neural agents, facilitates the\n",
            "emergence of a more structured type of language. Indeed, these languages\n",
            "provide learning speed advantages to neural agents during training, which can\n",
            "be incrementally amplified via NIL. We provide a probabilistic model of NIL and\n",
            "an explanation of why the advantage of compositional language exist. Our\n",
            "experiments confirm our analysis, and also demonstrate that the emerged\n",
            "languages largely improve the generalizing power of the neural agent\n",
            "communication.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2101.11844 \n",
            "Title :A Taxonomy of Explainable Bayesian Networks\n",
            "  Artificial Intelligence (AI), and in particular, the explainability thereof,\n",
            "has gained phenomenal attention over the last few years. Whilst we usually do\n",
            "not question the decision-making process of these systems in situations where\n",
            "only the outcome is of interest, we do however pay close attention when these\n",
            "systems are applied in areas where the decisions directly influence the lives\n",
            "of humans. It is especially noisy and uncertain observations close to the\n",
            "decision boundary which results in predictions which cannot necessarily be\n",
            "explained that may foster mistrust among end-users. This drew attention to AI\n",
            "methods for which the outcomes can be explained. Bayesian networks are\n",
            "probabilistic graphical models that can be used as a tool to manage\n",
            "uncertainty. The probabilistic framework of a Bayesian network allows for\n",
            "explainability in the model, reasoning and evidence. The use of these methods\n",
            "is mostly ad hoc and not as well organised as explainability methods in the\n",
            "wider AI research field. As such, we introduce a taxonomy of explainability in\n",
            "Bayesian networks. We extend the existing categorisation of explainability in\n",
            "the model, reasoning or evidence to include explanation of decisions. The\n",
            "explanations obtained from the explainability methods are illustrated by means\n",
            "of a simple medical diagnostic scenario. The taxonomy introduced in this paper\n",
            "has the potential not only to encourage end-users to efficiently communicate\n",
            "outcomes obtained, but also support their understanding of how and, more\n",
            "importantly, why certain predictions were made.\n",
            "\n",
            "**Paper Id :2010.04990 \n",
            "Title :The emergence of Explainability of Intelligent Systems: Delivering\n",
            "  Explainable and Personalised Recommendations for Energy Efficiency\n",
            "  The recent advances in artificial intelligence namely in machine learning and\n",
            "deep learning, have boosted the performance of intelligent systems in several\n",
            "ways. This gave rise to human expectations, but also created the need for a\n",
            "deeper understanding of how intelligent systems think and decide. The concept\n",
            "of explainability appeared, in the extent of explaining the internal system\n",
            "mechanics in human terms. Recommendation systems are intelligent systems that\n",
            "support human decision making, and as such, they have to be explainable in\n",
            "order to increase user trust and improve the acceptance of recommendations. In\n",
            "this work, we focus on a context-aware recommendation system for energy\n",
            "efficiency and develop a mechanism for explainable and persuasive\n",
            "recommendations, which are personalized to user preferences and habits. The\n",
            "persuasive facts either emphasize on the economical saving prospects (Econ) or\n",
            "on a positive ecological impact (Eco) and explanations provide the reason for\n",
            "recommending an energy saving action. Based on a study conducted using a\n",
            "Telegram bot, different scenarios have been validated with actual data and\n",
            "human feedback. Current results show a total increase of 19\\% on the\n",
            "recommendation acceptance ratio when both economical and ecological persuasive\n",
            "facts are employed. This revolutionary approach on recommendation systems,\n",
            "demonstrates how intelligent recommendations can effectively encourage energy\n",
            "saving behavior.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.01579 \n",
            "Title :Exploiting Raw Images for Real-Scene Super-Resolution\n",
            "  Super-resolution is a fundamental problem in computer vision which aims to\n",
            "overcome the spatial limitation of camera sensors. While significant progress\n",
            "has been made in single image super-resolution, most algorithms only perform\n",
            "well on synthetic data, which limits their applications in real scenarios. In\n",
            "this paper, we study the problem of real-scene single image super-resolution to\n",
            "bridge the gap between synthetic data and real captured images. We focus on two\n",
            "issues of existing super-resolution algorithms: lack of realistic training data\n",
            "and insufficient utilization of visual information obtained from cameras. To\n",
            "address the first issue, we propose a method to generate more realistic\n",
            "training data by mimicking the imaging process of digital cameras. For the\n",
            "second issue, we develop a two-branch convolutional neural network to exploit\n",
            "the radiance information originally-recorded in raw images. In addition, we\n",
            "propose a dense channel-attention block for better image restoration as well as\n",
            "a learning-based guided filter network for effective color correction. Our\n",
            "model is able to generalize to different cameras without deliberately training\n",
            "on images from specific camera types. Extensive experiments demonstrate that\n",
            "the proposed algorithm can recover fine details and clear structures, and\n",
            "achieve high-quality results for single image super-resolution in real scenes.\n",
            "\n",
            "**Paper Id :1909.03681 \n",
            "Title :Outlier Detection in High Dimensional Data\n",
            "  High-dimensional data poses unique challenges in outlier detection process.\n",
            "Most of the existing algorithms fail to properly address the issues stemming\n",
            "from a large number of features. In particular, outlier detection algorithms\n",
            "perform poorly on data set of small size with a large number of features. In\n",
            "this paper, we propose a novel outlier detection algorithm based on principal\n",
            "component analysis and kernel density estimation. The proposed method is\n",
            "designed to address the challenges of dealing with high-dimensional data by\n",
            "projecting the original data onto a smaller space and using the innate\n",
            "structure of the data to calculate anomaly scores for each data point.\n",
            "Numerical experiments on synthetic and real-life data show that our method\n",
            "performs well on high-dimensional data. In particular, the proposed method\n",
            "outperforms the benchmark methods as measured by the $F_1$-score. Our method\n",
            "also produces better-than-average execution times compared to the benchmark\n",
            "methods.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.03896 \n",
            "Title :Consequences of Misaligned AI\n",
            "  AI systems often rely on two key components: a specified goal or reward\n",
            "function and an optimization algorithm to compute the optimal behavior for that\n",
            "goal. This approach is intended to provide value for a principal: the user on\n",
            "whose behalf the agent acts. The objectives given to these agents often refer\n",
            "to a partial specification of the principal's goals. We consider the cost of\n",
            "this incompleteness by analyzing a model of a principal and an agent in a\n",
            "resource constrained world where the $L$ attributes of the state correspond to\n",
            "different sources of utility for the principal. We assume that the reward\n",
            "function given to the agent only has support on $J < L$ attributes. The\n",
            "contributions of our paper are as follows: 1) we propose a novel model of an\n",
            "incomplete principal-agent problem from artificial intelligence; 2) we provide\n",
            "necessary and sufficient conditions under which indefinitely optimizing for any\n",
            "incomplete proxy objective leads to arbitrarily low overall utility; and 3) we\n",
            "show how modifying the setup to allow reward functions that reference the full\n",
            "state or allowing the principal to update the proxy objective over time can\n",
            "lead to higher utility solutions. The results in this paper argue that we\n",
            "should view the design of reward functions as an interactive and dynamic\n",
            "process and identifies a theoretical scenario where some degree of\n",
            "interactivity is desirable.\n",
            "\n",
            "**Paper Id :2005.04912 \n",
            "Title :Maximizing Information Gain in Partially Observable Environments via\n",
            "  Prediction Reward\n",
            "  Information gathering in a partially observable environment can be formulated\n",
            "as a reinforcement learning (RL), problem where the reward depends on the\n",
            "agent's uncertainty. For example, the reward can be the negative entropy of the\n",
            "agent's belief over an unknown (or hidden) variable. Typically, the rewards of\n",
            "an RL agent are defined as a function of the state-action pairs and not as a\n",
            "function of the belief of the agent; this hinders the direct application of\n",
            "deep RL methods for such tasks. This paper tackles the challenge of using\n",
            "belief-based rewards for a deep RL agent, by offering a simple insight that\n",
            "maximizing any convex function of the belief of the agent can be approximated\n",
            "by instead maximizing a prediction reward: a reward based on prediction\n",
            "accuracy. In particular, we derive the exact error between negative entropy and\n",
            "the expected prediction reward. This insight provides theoretical motivation\n",
            "for several fields using prediction rewards---namely visual attention, question\n",
            "answering systems, and intrinsic motivation---and highlights their connection\n",
            "to the usually distinct fields of active perception, active sensing, and sensor\n",
            "placement. Based on this insight we present deep anticipatory networks (DANs),\n",
            "which enables an agent to take actions to reduce its uncertainty without\n",
            "performing explicit belief inference. We present two applications of DANs:\n",
            "building a sensor selection system for tracking people in a shopping mall and\n",
            "learning discrete models of attention on fashion MNIST and MNIST digit\n",
            "classification.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.05264 \n",
            "Title :Player Modeling via Multi-Armed Bandits\n",
            "  This paper focuses on building personalized player models solely from player\n",
            "behavior in the context of adaptive games. We present two main contributions:\n",
            "The first is a novel approach to player modeling based on multi-armed bandits\n",
            "(MABs). This approach addresses, at the same time and in a principled way, both\n",
            "the problem of collecting data to model the characteristics of interest for the\n",
            "current player and the problem of adapting the interactive experience based on\n",
            "this model. Second, we present an approach to evaluating and fine-tuning these\n",
            "algorithms prior to generating data in a user study. This is an important\n",
            "problem, because conducting user studies is an expensive and labor-intensive\n",
            "process; therefore, an ability to evaluate the algorithms beforehand can save a\n",
            "significant amount of resources. We evaluate our approach in the context of\n",
            "modeling players' social comparison orientation (SCO) and present empirical\n",
            "results from both simulations and real players.\n",
            "\n",
            "**Paper Id :1910.09713 \n",
            "Title :ALGAMES: A Fast Solver for Constrained Dynamic Games\n",
            "  Dynamic games are an effective paradigm for dealing with the control of\n",
            "multiple interacting actors. This paper introduces ALGAMES (Augmented\n",
            "Lagrangian GAME-theoretic Solver), a solver that handles trajectory\n",
            "optimization problems with multiple actors and general nonlinear state and\n",
            "input constraints. Its novelty resides in satisfying the first order optimality\n",
            "conditions with a quasi-Newton root-finding algorithm and rigorously enforcing\n",
            "constraints using an augmented Lagrangian formulation. We evaluate our solver\n",
            "in the context of autonomous driving on scenarios with a strong level of\n",
            "interactions between the vehicles. We assess the robustness of the solver using\n",
            "Monte Carlo simulations. It is able to reliably solve complex problems like\n",
            "ramp merging with three vehicles three times faster than a state-of-the-art\n",
            "DDP-based approach. A model predictive control (MPC) implementation of the\n",
            "algorithm demonstrates real-time performance on complex autonomous driving\n",
            "scenarios with an update frequency higher than 60 Hz.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.05424 \n",
            "Title :Doctor Imitator: Hand-Radiography-based Bone Age Assessment by Imitating\n",
            "  Scoring Methods\n",
            "  Bone age assessment is challenging in clinical practice due to the\n",
            "complicated bone age assessment process. Current automatic bone age assessment\n",
            "methods were designed with rare consideration of the diagnostic logistics and\n",
            "thus may yield certain uninterpretable hidden states and outputs. Consequently,\n",
            "doctors can find it hard to cooperate with such models harmoniously because it\n",
            "is difficult to check the correctness of the model predictions. In this work,\n",
            "we propose a new graph-based deep learning framework for bone age assessment\n",
            "with hand radiographs, called Doctor Imitator (DI). The architecture of DI is\n",
            "designed to learn the diagnostic logistics of doctors using the scoring methods\n",
            "(e.g., the Tanner-Whitehouse method) for bone age assessment. Specifically, the\n",
            "convolutions of DI capture the local features of the anatomical regions of\n",
            "interest (ROIs) on hand radiographs and predict the ROI scores by our proposed\n",
            "Anatomy-based Group Convolution, summing up for bone age prediction. Besides,\n",
            "we develop a novel Dual Graph-based Attention module to compute\n",
            "patient-specific attention for ROI features and context attention for ROI\n",
            "scores. As far as we know, DI is the first automatic bone age assessment\n",
            "framework following the scoring methods without fully supervised hand\n",
            "radiographs. Experiments on hand radiographs with only bone age supervision\n",
            "verify that DI can achieve excellent performance with sparse parameters and\n",
            "provide more interpretability.\n",
            "\n",
            "**Paper Id :1909.01136 \n",
            "Title :Pre-training A Neural Language Model Improves The Sample Efficiency of\n",
            "  an Emergency Room Classification Model\n",
            "  To build a French national electronic injury surveillance system based on\n",
            "emergency room visits, we aim to develop a coding system to classify their\n",
            "causes from clinical notes in free-text. Supervised learning techniques have\n",
            "shown good results in this area but require a large amount of expert annotated\n",
            "dataset which is time consuming and costly to obtain. We hypothesize that the\n",
            "Natural Language Processing Transformer model incorporating a generative\n",
            "self-supervised pre-training step can significantly reduce the required number\n",
            "of annotated samples for supervised fine-tuning. In this preliminary study, we\n",
            "test our hypothesis in the simplified problem of predicting whether a visit is\n",
            "the consequence of a traumatic event or not from free-text clinical notes.\n",
            "Using fully re-trained GPT-2 models (without OpenAI pre-trained weights), we\n",
            "assess the gain of applying a self-supervised pre-training phase with unlabeled\n",
            "notes prior to the supervised learning task. Results show that the number of\n",
            "data required to achieve a ginve level of performance (AUC>0.95) was reduced by\n",
            "a factor of 10 when applying pre-training. Namely, for 16 times more data, the\n",
            "fully-supervised model achieved an improvement <1% in AUC. To conclude, it is\n",
            "possible to adapt a multi-purpose neural language model such as the GPT-2 to\n",
            "create a powerful tool for classification of free-text notes with only a small\n",
            "number of labeled samples.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.06793 \n",
            "Title :Unanswerable Questions about Images and Texts\n",
            "  Questions about a text or an image that cannot be answered raise distinctive\n",
            "issues for an AI. This note discusses the problem of unanswerable questions in\n",
            "VQA (visual question answering), in QA (visual question answering), and in AI\n",
            "generally.\n",
            "\n",
            "**Paper Id :2003.00431 \n",
            "Title :A Study on Multimodal and Interactive Explanations for Visual Question\n",
            "  Answering\n",
            "  Explainability and interpretability of AI models is an essential factor\n",
            "affecting the safety of AI. While various explainable AI (XAI) approaches aim\n",
            "at mitigating the lack of transparency in deep networks, the evidence of the\n",
            "effectiveness of these approaches in improving usability, trust, and\n",
            "understanding of AI systems are still missing. We evaluate multimodal\n",
            "explanations in the setting of a Visual Question Answering (VQA) task, by\n",
            "asking users to predict the response accuracy of a VQA agent with and without\n",
            "explanations. We use between-subjects and within-subjects experiments to probe\n",
            "explanation effectiveness in terms of improving user prediction accuracy,\n",
            "confidence, and reliance, among other factors. The results indicate that the\n",
            "explanations help improve human prediction accuracy, especially in trials when\n",
            "the VQA system's answer is inaccurate. Furthermore, we introduce active\n",
            "attention, a novel method for evaluating causal attentional effects through\n",
            "intervention by editing attention maps. User explanation ratings are strongly\n",
            "correlated with human prediction accuracy and suggest the efficacy of these\n",
            "explanations in human-machine AI collaboration tasks.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.11085 \n",
            "Title :Comparative Fault Location Estimation by Using Image Processing in Mixed\n",
            "  Transmission Lines\n",
            "  The distance protection relays are used to determine the impedance based\n",
            "fault location according to the current and voltage magnitudes in the\n",
            "transmission lines. However, the fault location cannot be correctly detected in\n",
            "mixed transmission lines due to different characteristic impedance per unit\n",
            "length because the characteristic impedance of high voltage cable line is\n",
            "significantly different from overhead line. Thus, determinations of the fault\n",
            "section and location with the distance protection relays are difficult in the\n",
            "mixed transmission lines. In this study, 154 kV overhead transmission line and\n",
            "underground cable line are examined as the mixed transmission line for the\n",
            "distance protection relays. Phase to ground faults are created in the mixed\n",
            "transmission line. overhead line section and underground cable section are\n",
            "simulated by using PSCAD-EMTDC.The short circuit fault images are generated in\n",
            "the distance protection relay for the overhead transmission line and\n",
            "underground cable transmission line faults. The images include the R-X\n",
            "impedance diagram of the fault, and the R-X impedance diagram have been\n",
            "detected by applying image processing steps. Artificial neural network (ANN)\n",
            "and the regression methods are used for prediction of the fault location, and\n",
            "the results of image processing are used as the input parameters for the\n",
            "training process of ANN and the regression methods. The results of ANN and\n",
            "regression methods are compared to select the most suitable method at the end\n",
            "of this study for forecasting of the fault location in transmission lines.\n",
            "\n",
            "**Paper Id :2106.10685 \n",
            "Title :MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant\n",
            "  placements of relay nodes in mission critical wireless networks\n",
            "  In critical infrastructures like airports, much care has to be devoted in\n",
            "protecting radio communication networks from external electromagnetic\n",
            "interference. Protection of such mission-critical radio communication networks\n",
            "is usually tackled by exploiting radiogoniometers: at least three suitably\n",
            "deployed radiogoniometers, and a gateway gathering information from them,\n",
            "permit to monitor and localise sources of electromagnetic emissions that are\n",
            "not supposed to be present in the monitored area. Typically, radiogoniometers\n",
            "are connected to the gateway through relay nodes. As a result, some degree of\n",
            "fault-tolerance for the network of relay nodes is essential in order to offer a\n",
            "reliable monitoring. On the other hand, deployment of relay nodes is typically\n",
            "quite expensive. As a result, we have two conflicting requirements: minimise\n",
            "costs while guaranteeing a given fault-tolerance. In this paper, we address the\n",
            "problem of computing a deployment for relay nodes that minimises the relay node\n",
            "network cost while at the same time guaranteeing proper working of the network\n",
            "even when some of the relay nodes (up to a given maximum number) become faulty\n",
            "(fault-tolerance). We show that, by means of a computation-intensive\n",
            "pre-processing on a HPC infrastructure, the above optimisation problem can be\n",
            "encoded as a 0/1 Linear Program, becoming suitable to be approached with\n",
            "standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT\n",
            "solvers. Our problem formulation enables us to present experimental results\n",
            "comparing the performance of these three solving technologies on a real case\n",
            "study of a relay node network deployment in areas of the Leonardo da Vinci\n",
            "Airport in Rome, Italy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2102.11506 \n",
            "Title :Comparative evaluation of CNN architectures for Image Caption Generation\n",
            "  Aided by recent advances in Deep Learning, Image Caption Generation has seen\n",
            "tremendous progress over the last few years. Most methods use transfer learning\n",
            "to extract visual information, in the form of image features, with the help of\n",
            "pre-trained Convolutional Neural Network models followed by transformation of\n",
            "the visual information using a Caption Generator module to generate the output\n",
            "sentences. Different methods have used different Convolutional Neural Network\n",
            "Architectures and, to the best of our knowledge, there is no systematic study\n",
            "which compares the relative efficacy of different Convolutional Neural Network\n",
            "architectures for extracting the visual information. In this work, we have\n",
            "evaluated 17 different Convolutional Neural Networks on two popular Image\n",
            "Caption Generation frameworks: the first based on Neural Image Caption (NIC)\n",
            "generation model and the second based on Soft-Attention framework. We observe\n",
            "that model complexity of Convolutional Neural Network, as measured by number of\n",
            "parameters, and the accuracy of the model on Object Recognition task does not\n",
            "necessarily co-relate with its efficacy on feature extraction for Image Caption\n",
            "Generation task.\n",
            "\n",
            "**Paper Id :2008.03209 \n",
            "Title :Investigating maximum likelihood based training of infinite mixtures for\n",
            "  uncertainty quantification\n",
            "  Uncertainty quantification in neural networks gained a lot of attention in\n",
            "the past years. The most popular approaches, Bayesian neural networks (BNNs),\n",
            "Monte Carlo dropout, and deep ensembles have one thing in common: they are all\n",
            "based on some kind of mixture model. While the BNNs build infinite mixture\n",
            "models and are derived via variational inference, the latter two build finite\n",
            "mixtures trained with the maximum likelihood method. In this work we\n",
            "investigate the effect of training an infinite mixture distribution with the\n",
            "maximum likelihood method instead of variational inference. We find that the\n",
            "proposed objective leads to stochastic networks with an increased predictive\n",
            "variance, which improves uncertainty based identification of\n",
            "miss-classification and robustness against adversarial attacks in comparison to\n",
            "a standard BNN with equivalent network structure. The new model also displays\n",
            "higher entropy on out-of-distribution data.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.04364 \n",
            "Title :Coverage based testing for V&V and Safety Assurance of Self-driving\n",
            "  Autonomous Vehicles: A Systematic Literature Review\n",
            "  Self-driving Autonomous Vehicles (SAVs) are gaining more interest each\n",
            "passing day by the industry as well as the general public. Tech and automobile\n",
            "companies are investing huge amounts of capital in research and development of\n",
            "SAVs to make sure they have a head start in the SAV market in the future. One\n",
            "of the major hurdles in the way of SAVs making it to the public roads is the\n",
            "lack of confidence of public in the safety aspect of SAVs. In order to assure\n",
            "safety and provide confidence to the public in the safety of SAVs, researchers\n",
            "around the world have used coverage-based testing for Verification and\n",
            "Validation (V&V) and safety assurance of SAVs. The objective of this paper is\n",
            "to investigate the coverage criteria proposed and coverage maximizing\n",
            "techniques used by researchers in the last decade up till now, to assure safety\n",
            "of SAVs. We conduct a Systematic Literature Review (SLR) for this investigation\n",
            "in our paper. We present a classification of existing research based on the\n",
            "coverage criteria used. Several research gaps and research directions are also\n",
            "provided in this SLR to enable further research in this domain. This paper\n",
            "provides a body of knowledge in the domain of safety assurance of SAVs. We\n",
            "believe the results of this SLR will be helpful in the progression of V&V and\n",
            "safety assurance of SAVs.\n",
            "\n",
            "**Paper Id :2007.11218 \n",
            "Title :Regulating human control over autonomous systems\n",
            "  In recent years, many sectors have experienced significant progress in\n",
            "automation, associated with the growing advances in artificial intelligence and\n",
            "machine learning. There are already automated robotic weapons, which are able\n",
            "to evaluate and engage with targets on their own, and there are already\n",
            "autonomous vehicles that do not need a human driver. It is argued that the use\n",
            "of increasingly autonomous systems (AS) should be guided by the policy of human\n",
            "control, according to which humans should execute a certain significant level\n",
            "of judgment over AS. While in the military sector there is a fear that AS could\n",
            "mean that humans lose control over life and death decisions, in the\n",
            "transportation domain, on the contrary, there is a strongly held view that\n",
            "autonomy could bring significant operational benefits by removing the need for\n",
            "a human driver. This article explores the notion of human control in the United\n",
            "States in the two domains of defense and transportation. The operationalization\n",
            "of emerging policies of human control results in the typology of direct and\n",
            "indirect human controls exercised over the use of AS. The typology helps to\n",
            "steer the debate away from the linguistic complexities of the term autonomy. It\n",
            "identifies instead where human factors are undergoing important changes and\n",
            "ultimately informs about more detailed rules and standards formulation, which\n",
            "differ across domains, applications, and sectors.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.04826 \n",
            "Title :Exact and heuristic approaches for multi-objective garbage accumulation\n",
            "  points location in real scenarios\n",
            "  Municipal solid waste management is a major challenge for nowadays urban\n",
            "societies, because it accounts for a large proportion of public budget and,\n",
            "when mishandled, it can lead to environmental and social problems. This work\n",
            "focuses on the problem of locating waste bins in an urban area, which is\n",
            "considered to have a strong influence in the overall efficiency of the reverse\n",
            "logistic chain. This article contributes with an exact multiobjective approach\n",
            "to solve the waste bin location in which the optimization criteria that are\n",
            "considered are: the accessibility to the system (as quality of service\n",
            "measure), the investment cost, and the required frequency of waste removal from\n",
            "the bins (as a proxy of the posterior routing costs). In this approach,\n",
            "different methods to obtain the objectives ideal and nadir values over the\n",
            "Pareto front are proposed and compared. Then, a family of heuristic methods\n",
            "based on the PageRank algorithm is proposed which aims to optimize the\n",
            "accessibility to the system, the amount of collected waste and the installation\n",
            "cost. The experimental evaluation was performed on real-world scenarios of the\n",
            "cities of Montevideo, Uruguay, and Bah\\'ia Blanca, Argentina. The obtained\n",
            "results show the competitiveness of the proposed approaches for constructing a\n",
            "set of candidate solutions that considers the different trade-offs between the\n",
            "optimization criteria.\n",
            "\n",
            "**Paper Id :1912.05748 \n",
            "Title :Multi-Agent Task Allocation in Complementary Teams: A Hunter and\n",
            "  Gatherer Approach\n",
            "  Consider a dynamic task allocation problem, where tasks are unknowingly\n",
            "distributed over an environment. This paper considers each task comprised of\n",
            "two sequential subtasks: detection and completion, where each subtask can only\n",
            "be carried out by a certain type of agent. We address this problem using a\n",
            "novel nature-inspired approach called \"hunter and gatherer\". The proposed\n",
            "method employs two complementary teams of agents: one agile in detecting\n",
            "(hunters) and another skillful in completing (gatherers) the tasks. To minimize\n",
            "the collective cost of task accomplishments in a distributed manner, a\n",
            "game-theoretic solution is introduced to couple agents from complementary\n",
            "teams. We utilize market-based negotiation models to develop incentive-based\n",
            "decision-making algorithms relying on innovative notions of \"certainty and\n",
            "uncertainty profit margins\". The simulation results demonstrate that employing\n",
            "two complementary teams of hunters and gatherers can effectually improve the\n",
            "number of tasks completed by agents compared to conventional methods, while the\n",
            "collective cost of accomplishments is minimized. In addition, the stability and\n",
            "efficacy of the proposed solutions are studied using Nash equilibrium analysis\n",
            "and statistical analysis respectively. It is also numerically shown that the\n",
            "proposed solutions function fairly, i.e. for each type of agent, the overall\n",
            "workload is distributed equally.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.05481 \n",
            "Title :From Classical to Hierarchical: benchmarks for the HTN Track of the\n",
            "  International Planning Competition\n",
            "  In this short paper, we outline nine classical benchmarks submitted to the\n",
            "first hierarchical planning track of the International Planning competition in\n",
            "2020. All of these benchmarks are based on the HDDL language. The choice of the\n",
            "benchmarks was based on a questionnaire sent to the HTN community. They are the\n",
            "following: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper,\n",
            "and Hiking. In the rest of the paper we give a short description of these\n",
            "benchmarks. All are totally ordered.\n",
            "\n",
            "**Paper Id :2011.13297 \n",
            "Title :Totally and Partially Ordered Hierarchical Planners in PDDL4J Library\n",
            "  In this paper, we outline the implementation of the TFD (Totally Ordered Fast\n",
            "Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners\n",
            "that participated in the first HTN IPC competition in 2020. These two planners\n",
            "are based on forward-chaining task decomposition coupled with a compact\n",
            "grounding of actions, methods, tasks and HTN problems.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.11276 \n",
            "Title :High precision control and deep learning-based corn stand counting\n",
            "  algorithms for agricultural robot\n",
            "  This paper presents high precision control and deep learning-based corn stand\n",
            "counting algorithms for a low-cost, ultra-compact 3D printed and autonomous\n",
            "field robot for agricultural operations. Currently, plant traits, such as\n",
            "emergence rate, biomass, vigor, and stand counting, are measured manually. This\n",
            "is highly labor-intensive and prone to errors. The robot, termed TerraSentia,\n",
            "is designed to automate the measurement of plant traits for efficient\n",
            "phenotyping as an alternative to manual measurements. In this paper, we\n",
            "formulate a Nonlinear Moving Horizon Estimator (NMHE) that identifies key\n",
            "terrain parameters using onboard robot sensors and a learning-based Nonlinear\n",
            "Model Predictive Control (NMPC) that ensures high precision path tracking in\n",
            "the presence of unknown wheel-terrain interaction. Moreover, we develop a\n",
            "machine vision algorithm designed to enable an ultra-compact ground robot to\n",
            "count corn stands by driving through the fields autonomously. The algorithm\n",
            "leverages a deep network to detect corn plants in images, and a visual tracking\n",
            "model to re-identify detected objects at different time steps. We collected\n",
            "data from 53 corn plots in various fields for corn plants around 14 days after\n",
            "emergence (stage V3 - V4). The robot predictions have agreed well with the\n",
            "ground truth with $C_{robot}=1.02 \\times C_{human}-0.86$ and a correlation\n",
            "coefficient $R=0.96$. The mean relative error given by the algorithm is\n",
            "$-3.78\\%$, and the standard deviation is $6.76\\%$. These results indicate a\n",
            "first and significant step towards autonomous robot-based real-time phenotyping\n",
            "using low-cost, ultra-compact ground robots for corn and potentially other\n",
            "crops.\n",
            "\n",
            "**Paper Id :2006.05241 \n",
            "Title :New Fusion Algorithm provides an alternative approach to Robotic Path\n",
            "  planning\n",
            "  For rapid growth in technology and automation, human tasks are being taken\n",
            "over by robots as robots have proven to be better with both speed and\n",
            "precision. One of the major and widespread usages of these robots is in the\n",
            "industrial businesses, where they are employed to carry massive loads in and\n",
            "around work areas. As these working environments might not be completely\n",
            "localized and could be dynamically changing, new approaches must be evaluated\n",
            "to guarantee a crash-free way of performing duties. This paper presents a new\n",
            "and efficient fusion algorithm for solving the path planning problem in a\n",
            "custom 2D environment. This fusion algorithm integrates an improved and\n",
            "optimized version of both, A* algorithm and the Artificial potential field\n",
            "method. Firstly, an initial or preliminary path is planned in the environmental\n",
            "model by adopting the A* algorithm. The heuristic function of this A* algorithm\n",
            "is optimized and improved according to the environmental model. This is\n",
            "followed by selecting and saving the key nodes in the initial path. Lastly, on\n",
            "the basis of these saved key nodes, path smoothing is done by artificial\n",
            "potential field method. Our simulation results carried out using Python viz.\n",
            "libraries indicate that the new fusion algorithm is feasible and superior in\n",
            "smoothness performance and can satisfy as a time-efficient and cheaper\n",
            "alternative to conventional A* strategies of path planning.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.11388 \n",
            "Title :Collaborative Agent Gameplay in the Pandemic Board Game\n",
            "  While artificial intelligence has been applied to control players' decisions\n",
            "in board games for over half a century, little attention is given to games with\n",
            "no player competition. Pandemic is an exemplar collaborative board game where\n",
            "all players coordinate to overcome challenges posed by events occurring during\n",
            "the game's progression. This paper proposes an artificial agent which controls\n",
            "all players' actions and balances chances of winning versus risk of losing in\n",
            "this highly stochastic environment. The agent applies a Rolling Horizon\n",
            "Evolutionary Algorithm on an abstraction of the game-state that lowers the\n",
            "branching factor and simulates the game's stochasticity. Results show that the\n",
            "proposed algorithm can find winning strategies more consistently in different\n",
            "games of varying difficulty. The impact of a number of state evaluation metrics\n",
            "is explored, balancing between optimistic strategies that favor winning and\n",
            "pessimistic strategies that guard against losing.\n",
            "\n",
            "**Paper Id :1906.01470 \n",
            "Title :Options as responses: Grounding behavioural hierarchies in multi-agent\n",
            "  RL\n",
            "  This paper investigates generalisation in multi-agent games, where the\n",
            "generality of the agent can be evaluated by playing against opponents it hasn't\n",
            "seen during training. We propose two new games with concealed information and\n",
            "complex, non-transitive reward structure (think rock/paper/scissors). It turns\n",
            "out that most current deep reinforcement learning methods fail to efficiently\n",
            "explore the strategy space, thus learning policies that generalise poorly to\n",
            "unseen opponents. We then propose a novel hierarchical agent architecture,\n",
            "where the hierarchy is grounded in the game-theoretic structure of the game --\n",
            "the top level chooses strategic responses to opponents, while the low level\n",
            "implements them into policy over primitive actions. This grounding facilitates\n",
            "credit assignment across the levels of hierarchy. Our experiments show that the\n",
            "proposed hierarchical agent is capable of generalisation to unseen opponents,\n",
            "while conventional baselines fail to generalise whatsoever.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.13823 \n",
            "Title :A Novel Adaptive Minority Oversampling Technique for Improved\n",
            "  Classification in Data Imbalanced Scenarios\n",
            "  Imbalance in the proportion of training samples belonging to different\n",
            "classes often poses performance degradation of conventional classifiers. This\n",
            "is primarily due to the tendency of the classifier to be biased towards the\n",
            "majority classes in the imbalanced dataset. In this paper, we propose a novel\n",
            "three step technique to address imbalanced data. As a first step we\n",
            "significantly oversample the minority class distribution by employing the\n",
            "traditional Synthetic Minority OverSampling Technique (SMOTE) algorithm using\n",
            "the neighborhood of the minority class samples and in the next step we\n",
            "partition the generated samples using a Gaussian-Mixture Model based clustering\n",
            "algorithm. In the final step synthetic data samples are chosen based on the\n",
            "weight associated with the cluster, the weight itself being determined by the\n",
            "distribution of the majority class samples. Extensive experiments on several\n",
            "standard datasets from diverse domains shows the usefulness of the proposed\n",
            "technique in comparison with the original SMOTE and its state-of-the-art\n",
            "variants algorithms.\n",
            "\n",
            "**Paper Id :1909.03681 \n",
            "Title :Outlier Detection in High Dimensional Data\n",
            "  High-dimensional data poses unique challenges in outlier detection process.\n",
            "Most of the existing algorithms fail to properly address the issues stemming\n",
            "from a large number of features. In particular, outlier detection algorithms\n",
            "perform poorly on data set of small size with a large number of features. In\n",
            "this paper, we propose a novel outlier detection algorithm based on principal\n",
            "component analysis and kernel density estimation. The proposed method is\n",
            "designed to address the challenges of dealing with high-dimensional data by\n",
            "projecting the original data onto a smaller space and using the innate\n",
            "structure of the data to calculate anomaly scores for each data point.\n",
            "Numerical experiments on synthetic and real-life data show that our method\n",
            "performs well on high-dimensional data. In particular, the proposed method\n",
            "outperforms the benchmark methods as measured by the $F_1$-score. Our method\n",
            "also produces better-than-average execution times compared to the benchmark\n",
            "methods.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.14950 \n",
            "Title :The AI Settlement Generation Challenge in Minecraft: First Year Report\n",
            "  This article outlines what we learned from the first year of the AI\n",
            "Settlement Generation Competition in Minecraft, a competition about producing\n",
            "AI programs that can generate interesting settlements in Minecraft for an\n",
            "unseen map. This challenge seeks to focus research into adaptive and holistic\n",
            "procedural content generation. Generating Minecraft towns and villages given\n",
            "existing maps is a suitable task for this, as it requires the generated content\n",
            "to be adaptive, functional, evocative and aesthetic at the same time. Here, we\n",
            "present the results from the first iteration of the competition. We discuss the\n",
            "evaluation methodology, present the different technical approaches by the\n",
            "competitors, and outline the open problems.\n",
            "\n",
            "**Paper Id :2010.05967 \n",
            "Title :The Zero Resource Speech Challenge 2020: Discovering discrete subword\n",
            "  and word units\n",
            "  We present the Zero Resource Speech Challenge 2020, which aims at learning\n",
            "speech representations from raw audio signals without any labels. It combines\n",
            "the data sets and metrics from two previous benchmarks (2017 and 2019) and\n",
            "features two tasks which tap into two levels of speech representation. The\n",
            "first task is to discover low bit-rate subword representations that optimize\n",
            "the quality of speech synthesis; the second one is to discover word-like units\n",
            "from unsegmented raw speech. We present the results of the twenty submitted\n",
            "models and discuss the implications of the main findings for unsupervised\n",
            "speech learning.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2103.16215 \n",
            "Title :Convolutional Neural Networks for Sleep Stage Scoring on a Two-Channel\n",
            "  EEG Signal\n",
            "  Sleeping problems have become one of the major diseases all over the world.\n",
            "To tackle this issue, the basic tool used by specialists is the Polysomnogram,\n",
            "which is a collection of different signals recorded during sleep. After its\n",
            "recording, the specialists have to score the different signals according to one\n",
            "of the standard guidelines. This process is carried out manually, which can be\n",
            "highly time-consuming and very prone to annotation errors. Therefore, over the\n",
            "years, many approaches have been explored in an attempt to support the\n",
            "specialists in this task. In this paper, an approach based on convolutional\n",
            "neural networks is presented, where an in-depth comparison is performed in\n",
            "order to determine the convenience of using more than one signal simultaneously\n",
            "as input. Additionally, the models were also used as parts of an ensemble model\n",
            "to check whether any useful information can be extracted from signal processing\n",
            "a single signal at a time which the dual-signal model cannot identify. Tests\n",
            "have been performed by using a well-known dataset called expanded sleep-EDF,\n",
            "which is the most commonly used dataset as the benchmark for this problem. The\n",
            "tests were carried out with a leave-one-out cross-validation over the patients,\n",
            "which ensures that there is no possible contamination between training and\n",
            "testing. The resulting proposal is a network smaller than previously published\n",
            "ones, but which overcomes the results of any previous models on the same\n",
            "dataset. The best result shows an accuracy of 92.67\\% and a Cohen's Kappa value\n",
            "over 0.84 compared to human experts.\n",
            "\n",
            "**Paper Id :2012.14794 \n",
            "Title :A Deep Reinforcement Learning Based Multi-Criteria Decision Support\n",
            "  System for Textile Manufacturing Process Optimization\n",
            "  Textile manufacturing is a typical traditional industry involving high\n",
            "complexity in interconnected processes with limited capacity on the application\n",
            "of modern technologies. Decision-making in this domain generally takes multiple\n",
            "criteria into consideration, which usually arouses more complexity. To address\n",
            "this issue, the present paper proposes a decision support system that combines\n",
            "the intelligent data-based random forest (RF) models and a human knowledge\n",
            "based analytical hierarchical process (AHP) multi-criteria structure in\n",
            "accordance to the objective and the subjective factors of the textile\n",
            "manufacturing process. More importantly, the textile manufacturing process is\n",
            "described as the Markov decision process (MDP) paradigm, and a deep\n",
            "reinforcement learning scheme, the Deep Q-networks (DQN), is employed to\n",
            "optimize it. The effectiveness of this system has been validated in a case\n",
            "study of optimizing a textile ozonation process, showing that it can better\n",
            "master the challenging decision-making tasks in textile manufacturing\n",
            "processes.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.02245 \n",
            "Title :Multi-Scale Context Aggregation Network with Attention-Guided for Crowd\n",
            "  Counting\n",
            "  Crowd counting aims to predict the number of people and generate the density\n",
            "map in the image. There are many challenges, including varying head scales, the\n",
            "diversity of crowd distribution across images and cluttered backgrounds. In\n",
            "this paper, we propose a multi-scale context aggregation network (MSCANet)\n",
            "based on single-column encoder-decoder architecture for crowd counting, which\n",
            "consists of an encoder based on a dense context-aware module (DCAM) and a\n",
            "hierarchical attention-guided decoder. To handle the issue of scale variation,\n",
            "we construct the DCAM to aggregate multi-scale contextual information by\n",
            "densely connecting the dilated convolution with varying receptive fields. The\n",
            "proposed DCAM can capture rich contextual information of crowd areas due to its\n",
            "long-range receptive fields and dense scale sampling. Moreover, to suppress the\n",
            "background noise and generate a high-quality density map, we adopt a\n",
            "hierarchical attention-guided mechanism in the decoder. This helps to integrate\n",
            "more useful spatial information from shallow feature maps of the encoder by\n",
            "introducing multiple supervision based on semantic attention module (SAM).\n",
            "Extensive experiments demonstrate that the proposed approach achieves better\n",
            "performance than other similar state-of-the-art methods on three challenging\n",
            "benchmark datasets for crowd counting. The code is available at\n",
            "https://github.com/KingMV/MSCANet\n",
            "\n",
            "**Paper Id :2008.03787 \n",
            "Title :Neural Manipulation Planning on Constraint Manifolds\n",
            "  The presence of task constraints imposes a significant challenge to motion\n",
            "planning. Despite all recent advancements, existing algorithms are still\n",
            "computationally expensive for most planning problems. In this paper, we present\n",
            "Constrained Motion Planning Networks (CoMPNet), the first neural planner for\n",
            "multimodal kinematic constraints. Our approach comprises the following\n",
            "components: i) constraint and environment perception encoders; ii) neural robot\n",
            "configuration generator that outputs configurations on/near the constraint\n",
            "manifold(s), and iii) a bidirectional planning algorithm that takes the\n",
            "generated configurations to create a feasible robot motion trajectory. We show\n",
            "that CoMPNet solves practical motion planning tasks involving both\n",
            "unconstrained and constrained problems. Furthermore, it generalizes to new\n",
            "unseen locations of the objects, i.e., not seen during training, in the given\n",
            "environments with high success rates. When compared to the state-of-the-art\n",
            "constrained motion planning algorithms, CoMPNet outperforms by order of\n",
            "magnitude improvement in computational speed with a significantly lower\n",
            "variance.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.02756 \n",
            "Title :Efficient transfer learning for NLP with ELECTRA\n",
            "  Clark et al. [2020] claims that the ELECTRA approach is highly efficient in\n",
            "NLP performances relative to computation budget. As such, this reproducibility\n",
            "study focus on this claim, summarized by the following question: Can we use\n",
            "ELECTRA to achieve close to SOTA performances for NLP in low-resource settings,\n",
            "in term of compute cost?\n",
            "\n",
            "**Paper Id :2002.04335 \n",
            "Title :Static and Dynamic Values of Computation in MCTS\n",
            "  Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for\n",
            "planning, and has powered many recent advances in artificial intelligence. In\n",
            "MCTS, one typically performs computations (i.e., simulations) to collect\n",
            "statistics about the possible future consequences of actions, and then chooses\n",
            "accordingly. Many popular MCTS methods such as UCT and its variants decide\n",
            "which computations to perform by trading-off exploration and exploitation. In\n",
            "this work, we take a more direct approach, and explicitly quantify the value of\n",
            "a computation based on its expected impact on the quality of the action\n",
            "eventually chosen. Our approach goes beyond the \"myopic\" limitations of\n",
            "existing computation-value-based methods in two senses: (I) we are able to\n",
            "account for the impact of non-immediate (ie, future) computations (II) on\n",
            "non-immediate actions. We show that policies that greedily optimize computation\n",
            "values are optimal under certain assumptions and obtain results that are\n",
            "competitive with the state-of-the-art.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.05742 \n",
            "Title :A Novel Visualization System of Using Augmented Reality in Knee\n",
            "  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm\n",
            "  Background and aim: Image registration and alignment are the main limitations\n",
            "of augmented reality-based knee replacement surgery. This research aims to\n",
            "decrease the registration error, eliminate outcomes that are trapped in local\n",
            "minima to improve the alignment problems, handle the occlusion, and maximize\n",
            "the overlapping parts. Methodology: markerless image registration method was\n",
            "used for Augmented reality-based knee replacement surgery to guide and\n",
            "visualize the surgical operation. While weight least square algorithm was used\n",
            "to enhance stereo camera-based tracking by filling border occlusion in right to\n",
            "left direction and non-border occlusion from left to right direction. Results:\n",
            "This study has improved video precision to 0.57 mm~0.61 mm alignment error.\n",
            "Furthermore, with the use of bidirectional points, for example, forwards and\n",
            "backwards directional cloud point, the iteration on image registration was\n",
            "decreased. This has led to improve the processing time as well. The processing\n",
            "time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear\n",
            "that this proposed system has focused on overcoming the misalignment difficulty\n",
            "caused by movement of patient and enhancing the AR visualization during knee\n",
            "replacement surgery. The proposed system was reliable and favorable which helps\n",
            "in eliminating alignment error by ascertaining the optimal rigid transformation\n",
            "between two cloud points and removing the outliers and non-Gaussian noise. The\n",
            "proposed augmented reality system helps in accurate visualization and\n",
            "navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,\n",
            "etc.\n",
            "\n",
            "**Paper Id :1904.07073 \n",
            "Title :A deep learning framework for quality assessment and restoration in\n",
            "  video endoscopy\n",
            "  Endoscopy is a routine imaging technique used for both diagnosis and\n",
            "minimally invasive surgical treatment. Artifacts such as motion blur, bubbles,\n",
            "specular reflections, floating objects and pixel saturation impede the visual\n",
            "interpretation and the automated analysis of endoscopy videos. Given the\n",
            "widespread use of endoscopy in different clinical applications, we contend that\n",
            "the robust and reliable identification of such artifacts and the automated\n",
            "restoration of corrupted video frames is a fundamental medical imaging problem.\n",
            "Existing state-of-the-art methods only deal with the detection and restoration\n",
            "of selected artifacts. However, typically endoscopy videos contain numerous\n",
            "artifacts which motivates to establish a comprehensive solution.\n",
            "  We propose a fully automatic framework that can: 1) detect and classify six\n",
            "different primary artifacts, 2) provide a quality score for each frame and 3)\n",
            "restore mildly corrupted frames. To detect different artifacts our framework\n",
            "exploits fast multi-scale, single stage convolutional neural network detector.\n",
            "We introduce a quality metric to assess frame quality and predict image\n",
            "restoration success. Generative adversarial networks with carefully chosen\n",
            "regularization are finally used to restore corrupted frames.\n",
            "  Our detector yields the highest mean average precision (mAP at 5% threshold)\n",
            "of 49.0 and the lowest computational time of 88 ms allowing for accurate\n",
            "real-time processing. Our restoration models for blind deblurring, saturation\n",
            "correction and inpainting demonstrate significant improvements over previous\n",
            "methods. On a set of 10 test videos we show that our approach preserves an\n",
            "average of 68.7% which is 25% more frames than that retained from the raw\n",
            "videos.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.09557 \n",
            "Title :Learning to Communicate with Strangers via Channel Randomisation Methods\n",
            "  We introduce two methods for improving the performance of agents meeting for\n",
            "the first time to accomplish a communicative task. The methods are: (1)\n",
            "`message mutation' during the generation of the communication protocol; and (2)\n",
            "random permutations of the communication channel. These proposals are tested\n",
            "using a simple two-player game involving a `teacher' who generates a\n",
            "communication protocol and sends a message, and a `student' who interprets the\n",
            "message. After training multiple agents via self-play we analyse the\n",
            "performance of these agents when they are matched with a stranger, i.e. their\n",
            "zero-shot communication performance. We find that both message mutation and\n",
            "channel permutation positively influence performance, and we discuss their\n",
            "effects.\n",
            "\n",
            "**Paper Id :1901.03887 \n",
            "Title :Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n",
            "  Learning through Memory-driven Communication\n",
            "  Deep reinforcement learning algorithms have recently been used to train\n",
            "multiple interacting agents in a centralised manner whilst keeping their\n",
            "execution decentralised. When the agents can only acquire partial observations\n",
            "and are faced with tasks requiring coordination and synchronisation skills,\n",
            "inter-agent communication plays an essential role. In this work, we propose a\n",
            "framework for multi-agent training using deep deterministic policy gradients\n",
            "that enables concurrent, end-to-end learning of an explicit communication\n",
            "protocol through a memory device. During training, the agents learn to perform\n",
            "read and write operations enabling them to infer a shared representation of the\n",
            "world. We empirically demonstrate that concurrent learning of the communication\n",
            "device and individual policies can improve inter-agent coordination and\n",
            "performance in small-scale systems. Our experimental results show that the\n",
            "proposed method achieves superior performance in scenarios with up to six\n",
            "agents. We illustrate how different communication patterns can emerge on six\n",
            "different tasks of increasing complexity. Furthermore, we study the effects of\n",
            "corrupting the communication channel, provide a visualisation of the\n",
            "time-varying memory content as the underlying task is being solved and validate\n",
            "the building blocks of the proposed memory device through ablation studies.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.11037 \n",
            "Title :10 Years of the PCG workshop: Past and Future Trends\n",
            "  As of 2020, the international workshop on Procedural Content Generation\n",
            "enters its second decade. The annual workshop, hosted by the international\n",
            "conference on the Foundations of Digital Games, has collected a corpus of 95\n",
            "papers published in its first 10 years. This paper provides an overview of the\n",
            "workshop's activities and surveys the prevalent research topics emerging over\n",
            "the years.\n",
            "\n",
            "**Paper Id :2009.12521 \n",
            "Title :Proceedings of the Sixteenth International Workshop on the ACL2 Theorem\n",
            "  Prover and its Applications\n",
            "  This volume contains a selection of papers presented at the 16th\n",
            "International Workshop on the ACL2 Theorem Prover and its Applications\n",
            "(ACL2-2020). The workshops are the premier technical forum for presenting\n",
            "research and experiences related to ACL2.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.11165 \n",
            "Title :Hierarchical growing grid networks for skeleton based action recognition\n",
            "  In this paper, a novel cognitive architecture for action recognition is\n",
            "developed by applying layers of growing grid neural networks.Using these layers\n",
            "makes the system capable of automatically arranging its representational\n",
            "structure. In addition to the expansion of the neural map during the growth\n",
            "phase, the system is provided with a prior knowledge of the input space, which\n",
            "increases the processing speed of the learning phase. Apart from two layers of\n",
            "growing grid networks the architecture is composed of a preprocessing layer, an\n",
            "ordered vector representation layer and a one-layer supervised neural network.\n",
            "These layers are designed to solve the action recognition problem. The\n",
            "first-layer growing grid receives the input data of human actions and the\n",
            "neural map generates an action pattern vector representing each action sequence\n",
            "by connecting the elicited activation of the trained map. The pattern vectors\n",
            "are then sent to the ordered vector representation layer to build the\n",
            "time-invariant input vectors of key activations for the second-layer growing\n",
            "grid. The second-layer growing grid categorizes the input vectors to the\n",
            "corresponding action clusters/sub-clusters and finally the one-layer supervised\n",
            "neural network labels the shaped clusters with action labels. Three experiments\n",
            "using different datasets of actions show that the system is capable of learning\n",
            "to categorize the actions quickly and efficiently. The performance of the\n",
            "growing grid architecture is com-pared with the results from a system based on\n",
            "Self-Organizing Maps, showing that the growing grid architecture performs\n",
            "significantly superior on the action recognition tasks.\n",
            "\n",
            "**Paper Id :2005.05137 \n",
            "Title :New Ideas for Brain Modelling 6\n",
            "  This paper describes implementation details for a 3-level cognitive model,\n",
            "described in the paper series. The whole architecture is now modular, with\n",
            "different levels using different types of information. The ensemble-hierarchy\n",
            "relationship is maintained and placed in the bottom optimising and middle\n",
            "aggregating levels, to store memory objects and their relations. The top-level\n",
            "cognitive layer has been re-designed to model the Cognitive Process Language\n",
            "(CPL) of an earlier paper, by refactoring it into a network structure with a\n",
            "light scheduler. The cortex brain region is thought to be hierarchical -\n",
            "clustering from simple to more complex features. The refactored network might\n",
            "therefore challenge conventional thinking on that brain region. It is also\n",
            "argued that the function and structure in particular, of the new top level, is\n",
            "similar to the psychology theory of chunking. The model is still only a\n",
            "framework and does not have enough information for real intelligence. But a\n",
            "framework is now implemented over the whole design and so can give a more\n",
            "complete picture about the potential for results.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.11683 \n",
            "Title :Establishing phone-pair co-usage by comparing mobility patterns\n",
            "  In forensic investigations it is often of value to establish whether two\n",
            "phones were used by the same person during a given time period. We present a\n",
            "method that uses time and location of cell tower registrations of mobile phones\n",
            "to assess the strength of evidence that any pair of phones were used by the\n",
            "same person. The method is transparent as it uses logistic regression to\n",
            "discriminate between the hypotheses of same and different user, and a standard\n",
            "kernel density estimation to quantify the weight of evidence in terms of a\n",
            "likelihood ratio. We further add to previous theoretical work by training and\n",
            "validating our method on real world data, paving the way for application in\n",
            "practice. The method shows good performance under different modeling choices\n",
            "and robustness under lower quantity or quality of data. We discuss practical\n",
            "usage in court.\n",
            "\n",
            "**Paper Id :1910.09153 \n",
            "Title :Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n",
            "  Series Analysis\n",
            "  In this work, we develop a novel framework to measure the similarity between\n",
            "dynamic financial networks, i.e., time-varying financial networks.\n",
            "Particularly, we explore whether the proposed similarity measure can be\n",
            "employed to understand the structural evolution of the financial networks with\n",
            "time. For a set of time-varying financial networks with each vertex\n",
            "representing the individual time series of a different stock and each edge\n",
            "between a pair of time series representing the absolute value of their Pearson\n",
            "correlation, our start point is to compute the commute time matrix associated\n",
            "with the weighted adjacency matrix of the network structures, where each\n",
            "element of the matrix can be seen as the enhanced correlation value between\n",
            "pairwise stocks. For each network, we show how the commute time matrix allows\n",
            "us to identify a reliable set of dominant correlated time series as well as an\n",
            "associated dominant probability distribution of the stock belonging to this\n",
            "set. Furthermore, we represent each original network as a discrete dominant\n",
            "Shannon entropy time series computed from the dominant probability\n",
            "distribution. With the dominant entropy time series for each pair of financial\n",
            "networks to hand, we develop a similarity measure based on the classical\n",
            "dynamic time warping framework, for analyzing the financial time-varying\n",
            "networks. We show that the proposed similarity measure is positive definite and\n",
            "thus corresponds to a kernel measure on graphs. The proposed kernel bridges the\n",
            "gap between graph kernels and the classical dynamic time warping framework for\n",
            "multiple financial time series analysis. Experiments on time-varying networks\n",
            "extracted through New York Stock Exchange (NYSE) database demonstrate the\n",
            "effectiveness of the proposed approach.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2104.13780 \n",
            "Title :Semantic Consistency and Identity Mapping Multi-Component Generative\n",
            "  Adversarial Network for Person Re-Identification\n",
            "  In a real world environment, person re-identification (Re-ID) is a\n",
            "challenging task due to variations in lighting conditions, viewing angles, pose\n",
            "and occlusions. Despite recent performance gains, current person Re-ID\n",
            "algorithms still suffer heavily when encountering these variations. To address\n",
            "this problem, we propose a semantic consistency and identity mapping\n",
            "multi-component generative adversarial network (SC-IMGAN) which provides style\n",
            "adaptation from one to many domains. To ensure that transformed images are as\n",
            "realistic as possible, we propose novel identity mapping and semantic\n",
            "consistency losses to maintain identity across the diverse domains. For the\n",
            "Re-ID task, we propose a joint verification-identification quartet network\n",
            "which is trained with generated and real images, followed by an effective\n",
            "quartet loss for verification. Our proposed method outperforms state-of-the-art\n",
            "techniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR,\n",
            "PRID2011, iLIDS and Market-1501.\n",
            "\n",
            "**Paper Id :1901.07733 \n",
            "Title :Deep-Learning Inversion of Seismic Data\n",
            "  We propose a new method to tackle the mapping challenge from time-series data\n",
            "to spatial image in the field of seismic exploration, i.e., reconstructing the\n",
            "velocity model directly from seismic data by deep neural networks (DNNs). The\n",
            "conventional way of addressing this ill-posed inversion problem is through\n",
            "iterative algorithms, which suffer from poor nonlinear mapping and strong\n",
            "nonuniqueness. Other attempts may either import human intervention errors or\n",
            "underuse seismic data. The challenge for DNNs mainly lies in the weak spatial\n",
            "correspondence, the uncertain reflection-reception relationship between seismic\n",
            "data and velocity model, as well as the time-varying property of seismic data.\n",
            "To tackle these challenges, we propose end-to-end seismic inversion networks\n",
            "(SeisInvNets) with novel components to make the best use of all seismic data.\n",
            "Specifically, we start with every seismic trace and enhance it with its\n",
            "neighborhood information, its observation setup, and the global context of its\n",
            "corresponding seismic profile. From the enhanced seismic traces, the spatially\n",
            "aligned feature maps can be learned and further concatenated to reconstruct a\n",
            "velocity model. In general, we let every seismic trace contribute to the\n",
            "reconstruction of the whole velocity model by finding spatial correspondence.\n",
            "The proposed SeisInvNet consistently produces improvements over the baselines\n",
            "and achieves promising performance on our synthesized and proposed SeisInv data\n",
            "set according to various evaluation metrics. The inversion results are more\n",
            "consistent with the target from the aspects of velocity values, subsurface\n",
            "structures, and geological interfaces. Moreover, the mechanism and the\n",
            "generalization of the proposed method are discussed and verified. Nevertheless,\n",
            "the generalization of deep-learning-based inversion methods on real data is\n",
            "still challenging and considering physics may be one potential solution.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2105.00375 \n",
            "Title :Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary\n",
            "  Results\n",
            "  Given an on-board diagnostics (OBD) dataset and a physics-based emissions\n",
            "prediction model, this paper aims to develop an accurate and\n",
            "computational-efficient AI (Artificial Intelligence) method that predicts\n",
            "vehicle emissions. The problem is of societal importance because vehicular\n",
            "emissions lead to climate change and impact human health. This problem is\n",
            "challenging because the OBD data does not contain enough parameters needed by\n",
            "high-order physics models. Conversely, related work has shown that low-order\n",
            "physics models have poor predictive accuracy when using available OBD data.\n",
            "This paper uses a divergent window co-occurrence pattern detection method to\n",
            "develop a spatiotemporal variability-aware AI model for predicting emission\n",
            "values from the OBD datasets. We conducted a case study using real-world OBD\n",
            "data from a local public transportation agency. Results show that the proposed\n",
            "AI method has approximately 65% improved predictive accuracy than a non-AI\n",
            "low-order physics model and is approximately 35% more accurate than a baseline\n",
            "model.\n",
            "\n",
            "**Paper Id :2101.10831 \n",
            "Title :Toxicity Detection in Drug Candidates using Simplified Molecular-Input\n",
            "  Line-Entry System\n",
            "  The need for analysis of toxicity in new drug candidates and the requirement\n",
            "of doing it fast have asked the consideration of scientists towards the use of\n",
            "artificial intelligence tools to examine toxicity levels and to develop models\n",
            "to a degree where they can be used commercially to measure toxicity levels\n",
            "efficiently in upcoming drugs. Artificial Intelligence based models can be used\n",
            "to predict the toxic nature of a chemical using Quantitative Structure Activity\n",
            "Relationship techniques. Convolutional Neural Network models have demonstrated\n",
            "great outcomes in predicting the qualitative analysis of chemicals in order to\n",
            "determine the toxicity. This paper goes for the study of Simplified Molecular\n",
            "Input Line-Entry System (SMILES) as a parameter to develop Long short term\n",
            "memory (LSTM) based models in order to examine the toxicity of a molecule and\n",
            "the degree to which the need can be fulfilled for practical use alongside its\n",
            "future outlooks for the purpose of real world applications.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2105.03092 \n",
            "Title :An Influence-based Approach for Root Cause Alarm Discovery in Telecom\n",
            "  Networks\n",
            "  Alarm root cause analysis is a significant component in the day-to-day\n",
            "telecommunication network maintenance, and it is critical for efficient and\n",
            "accurate fault localization and failure recovery. In practice, accurate and\n",
            "self-adjustable alarm root cause analysis is a great challenge due to network\n",
            "complexity and vast amounts of alarms. A popular approach for failure root\n",
            "cause identification is to construct a graph with approximate edges, commonly\n",
            "based on either event co-occurrences or conditional independence tests.\n",
            "However, considerable expert knowledge is typically required for edge pruning.\n",
            "We propose a novel data-driven framework for root cause alarm localization,\n",
            "combining both causal inference and network embedding techniques. In this\n",
            "framework, we design a hybrid causal graph learning method (HPCI), which\n",
            "combines Hawkes Process with Conditional Independence tests, as well as propose\n",
            "a novel Causal Propagation-Based Embedding algorithm (CPBE) to infer edge\n",
            "weights. We subsequently discover root cause alarms in a real-time data stream\n",
            "by applying an influence maximization algorithm on the weighted graph. We\n",
            "evaluate our method on artificial data and real-world telecom data, showing a\n",
            "significant improvement over the best baselines.\n",
            "\n",
            "**Paper Id :2110.15385 \n",
            "Title :Data-driven Residual Generation for Early Fault Detection with Limited\n",
            "  Data\n",
            "  Traditionally, fault detection and isolation community has used system\n",
            "dynamic equations to generate diagnosers and to analyze detectability and\n",
            "isolability of the dynamic systems. Model-based fault detection and isolation\n",
            "methods use system model to generate a set of residuals as the bases for fault\n",
            "detection and isolation. However, in many complex systems it is not feasible to\n",
            "develop highly accurate models for the systems and to keep the models updated\n",
            "during the system lifetime. Recently, data-driven solutions have received an\n",
            "immense attention in the industries systems for several practical reasons.\n",
            "First, these methods do not require the initial investment and expertise for\n",
            "developing accurate models. Moreover, it is possible to automatically update\n",
            "and retrain the diagnosers as the system or the environment change over time.\n",
            "Finally, unlike the model-based methods it is straight forward to combine time\n",
            "series measurements such as pressure and voltage with other sources of\n",
            "information such as system operating hours to achieve a higher accuracy. In\n",
            "this paper, we extend the traditional model-based fault detection and isolation\n",
            "concepts such as residuals, and detectable and isolable faults to the\n",
            "data-driven domain. We then propose an algorithm to automatically generate\n",
            "residuals from the normal operating data. We present the performance of our\n",
            "proposed approach through a comparative case study.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2105.10606 \n",
            "Title :CEREC: A Corpus for Entity Resolution in Email Conversations\n",
            "  We present the first large scale corpus for entity resolution in email\n",
            "conversations (CEREC). The corpus consists of 6001 email threads from the Enron\n",
            "Email Corpus containing 36,448 email messages and 60,383 entity coreference\n",
            "chains. The annotation is carried out as a two-step process with minimal manual\n",
            "effort. Experiments are carried out for evaluating different features and\n",
            "performance of four baselines on the created corpus. For the task of mention\n",
            "identification and coreference resolution, a best performance of 59.2 F1 is\n",
            "reported, highlighting the room for improvement. An in-depth qualitative and\n",
            "quantitative error analysis is presented to understand the limitations of the\n",
            "baselines considered.\n",
            "\n",
            "**Paper Id :2008.02837 \n",
            "Title :aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n",
            "  and Transfer Learning\n",
            "  We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\n",
            "Techniques in News Articles. We developed ensemble models using RoBERTa-based\n",
            "neural architectures, additional CRF layers, transfer learning between the two\n",
            "subtasks, and advanced post-processing to handle the multi-label nature of the\n",
            "task, the consistency between nested spans, repetitions, and labels from\n",
            "similar spans in training. We achieved sizable improvements over baseline\n",
            "fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n",
            "(almost tied with the 2nd) out of 36 teams on the span identification subtask\n",
            "with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\n",
            "on the technique classification subtask with an F1 score of 0.62.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2105.13854 \n",
            "Title :Neonatal seizure detection from raw multi-channel EEG using a fully\n",
            "  convolutional architecture\n",
            "  A deep learning classifier for detecting seizures in neonates is proposed.\n",
            "This architecture is designed to detect seizure events from raw\n",
            "electroencephalogram (EEG) signals as opposed to the state-of-the-art hand\n",
            "engineered feature-based representation employed in traditional machine\n",
            "learning based solutions. The seizure detection system utilises only\n",
            "convolutional layers in order to process the multichannel time domain signal\n",
            "and is designed to exploit the large amount of weakly labelled data in the\n",
            "training stage. The system performance is assessed on a large database of\n",
            "continuous EEG recordings of 834h in duration; this is further validated on a\n",
            "held-out publicly available dataset and compared with two baseline SVM based\n",
            "systems.\n",
            "  The developed system achieves a 56% relative improvement with respect to a\n",
            "feature-based state-of-the art baseline, reaching an AUC of 98.5%; this also\n",
            "compares favourably both in terms of performance and run-time. The effect of\n",
            "varying architectural parameters is thoroughly studied. The performance\n",
            "improvement is achieved through novel architecture design which allows more\n",
            "efficient usage of available training data and end-to-end optimisation from the\n",
            "front-end feature extraction to the back-end classification. The proposed\n",
            "architecture opens new avenues for the application of deep learning to neonatal\n",
            "EEG, where the performance becomes a function of the amount of training data\n",
            "with less dependency on the availability of precise clinical labels.\n",
            "\n",
            "**Paper Id :1911.09032 \n",
            "Title :Outside the Box: Abstraction-Based Monitoring of Neural Networks\n",
            "  Neural networks have demonstrated unmatched performance in a range of\n",
            "classification tasks. Despite numerous efforts of the research community,\n",
            "novelty detection remains one of the significant limitations of neural\n",
            "networks. The ability to identify previously unseen inputs as novel is crucial\n",
            "for our understanding of the decisions made by neural networks. At runtime,\n",
            "inputs not falling into any of the categories learned during training cannot be\n",
            "classified correctly by the neural network. Existing approaches treat the\n",
            "neural network as a black box and try to detect novel inputs based on the\n",
            "confidence of the output predictions. However, neural networks are not trained\n",
            "to reduce their confidence for novel inputs, which limits the effectiveness of\n",
            "these approaches. We propose a framework to monitor a neural network by\n",
            "observing the hidden layers. We employ a common abstraction from program\n",
            "analysis - boxes - to identify novel behaviors in the monitored layers, i.e.,\n",
            "inputs that cause behaviors outside the box. For each neuron, the boxes range\n",
            "over the values seen in training. The framework is efficient and flexible to\n",
            "achieve a desired trade-off between raising false warnings and detecting novel\n",
            "inputs. We illustrate the performance and the robustness to variability in the\n",
            "unknown classes on popular image-classification benchmarks.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2106.09564 \n",
            "Title :Knowledge distillation from multi-modal to mono-modal segmentation\n",
            "  networks\n",
            "  The joint use of multiple imaging modalities for medical image segmentation\n",
            "has been widely studied in recent years. The fusion of information from\n",
            "different modalities has demonstrated to improve the segmentation accuracy,\n",
            "with respect to mono-modal segmentations, in several applications. However,\n",
            "acquiring multiple modalities is usually not possible in a clinical setting due\n",
            "to a limited number of physicians and scanners, and to limit costs and scan\n",
            "time. Most of the time, only one modality is acquired. In this paper, we\n",
            "propose KD-Net, a framework to transfer knowledge from a trained multi-modal\n",
            "network (teacher) to a mono-modal one (student). The proposed method is an\n",
            "adaptation of the generalized distillation framework where the student network\n",
            "is trained on a subset (1 modality) of the teacher's inputs (n modalities). We\n",
            "illustrate the effectiveness of the proposed framework in brain tumor\n",
            "segmentation with the BraTS 2018 dataset. Using different architectures, we\n",
            "show that the student network effectively learns from the teacher and always\n",
            "outperforms the baseline mono-modal network in terms of segmentation accuracy.\n",
            "\n",
            "**Paper Id :2112.10017 \n",
            "Title :Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks\n",
            "  Existing research on continual learning of a sequence of tasks focused on\n",
            "dealing with catastrophic forgetting, where the tasks are assumed to be\n",
            "dissimilar and have little shared knowledge. Some work has also been done to\n",
            "transfer previously learned knowledge to the new task when the tasks are\n",
            "similar and have shared knowledge. To the best of our knowledge, no technique\n",
            "has been proposed to learn a sequence of mixed similar and dissimilar tasks\n",
            "that can deal with forgetting and also transfer knowledge forward and backward.\n",
            "This paper proposes such a technique to learn both types of tasks in the same\n",
            "network. For dissimilar tasks, the algorithm focuses on dealing with\n",
            "forgetting, and for similar tasks, the algorithm focuses on selectively\n",
            "transferring the knowledge learned from some similar previous tasks to improve\n",
            "the new task learning. Additionally, the algorithm automatically detects\n",
            "whether a new task is similar to any previous tasks. Empirical evaluation using\n",
            "sequences of mixed tasks demonstrates the effectiveness of the proposed model.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2106.10684 \n",
            "Title :Optimal personalised treatment computation through in silico clinical\n",
            "  trials on patient digital twins\n",
            "  In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns\n",
            "carried out by means of computer simulations, hold the promise to decrease time\n",
            "and cost for the safety and efficacy assessment of pharmacological treatments,\n",
            "reduce the need for animal and human testing, and enable precision medicine. In\n",
            "this paper we present methods and an algorithm that, by means of extensive\n",
            "computer simulation--based experimental campaigns (ISTC) guided by intelligent\n",
            "search, optimise a pharmacological treatment for an individual patient\n",
            "(precision medicine). e show the effectiveness of our approach on a case study\n",
            "involving a real pharmacological treatment, namely the downregulation phase of\n",
            "a complex clinical protocol for assisted reproduction in humans.\n",
            "\n",
            "**Paper Id :1909.01136 \n",
            "Title :Pre-training A Neural Language Model Improves The Sample Efficiency of\n",
            "  an Emergency Room Classification Model\n",
            "  To build a French national electronic injury surveillance system based on\n",
            "emergency room visits, we aim to develop a coding system to classify their\n",
            "causes from clinical notes in free-text. Supervised learning techniques have\n",
            "shown good results in this area but require a large amount of expert annotated\n",
            "dataset which is time consuming and costly to obtain. We hypothesize that the\n",
            "Natural Language Processing Transformer model incorporating a generative\n",
            "self-supervised pre-training step can significantly reduce the required number\n",
            "of annotated samples for supervised fine-tuning. In this preliminary study, we\n",
            "test our hypothesis in the simplified problem of predicting whether a visit is\n",
            "the consequence of a traumatic event or not from free-text clinical notes.\n",
            "Using fully re-trained GPT-2 models (without OpenAI pre-trained weights), we\n",
            "assess the gain of applying a self-supervised pre-training phase with unlabeled\n",
            "notes prior to the supervised learning task. Results show that the number of\n",
            "data required to achieve a ginve level of performance (AUC>0.95) was reduced by\n",
            "a factor of 10 when applying pre-training. Namely, for 16 times more data, the\n",
            "fully-supervised model achieved an improvement <1% in AUC. To conclude, it is\n",
            "possible to adapt a multi-purpose neural language model such as the GPT-2 to\n",
            "create a powerful tool for classification of free-text notes with only a small\n",
            "number of labeled samples.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2106.10685 \n",
            "Title :MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant\n",
            "  placements of relay nodes in mission critical wireless networks\n",
            "  In critical infrastructures like airports, much care has to be devoted in\n",
            "protecting radio communication networks from external electromagnetic\n",
            "interference. Protection of such mission-critical radio communication networks\n",
            "is usually tackled by exploiting radiogoniometers: at least three suitably\n",
            "deployed radiogoniometers, and a gateway gathering information from them,\n",
            "permit to monitor and localise sources of electromagnetic emissions that are\n",
            "not supposed to be present in the monitored area. Typically, radiogoniometers\n",
            "are connected to the gateway through relay nodes. As a result, some degree of\n",
            "fault-tolerance for the network of relay nodes is essential in order to offer a\n",
            "reliable monitoring. On the other hand, deployment of relay nodes is typically\n",
            "quite expensive. As a result, we have two conflicting requirements: minimise\n",
            "costs while guaranteeing a given fault-tolerance. In this paper, we address the\n",
            "problem of computing a deployment for relay nodes that minimises the relay node\n",
            "network cost while at the same time guaranteeing proper working of the network\n",
            "even when some of the relay nodes (up to a given maximum number) become faulty\n",
            "(fault-tolerance). We show that, by means of a computation-intensive\n",
            "pre-processing on a HPC infrastructure, the above optimisation problem can be\n",
            "encoded as a 0/1 Linear Program, becoming suitable to be approached with\n",
            "standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT\n",
            "solvers. Our problem formulation enables us to present experimental results\n",
            "comparing the performance of these three solving technologies on a real case\n",
            "study of a relay node network deployment in areas of the Leonardo da Vinci\n",
            "Airport in Rome, Italy.\n",
            "\n",
            "**Paper Id :1901.06949 \n",
            "Title :Differential Privacy for Power Grid Obfuscation\n",
            "  The availability of high-fidelity energy networks brings significant value to\n",
            "academic and commercial research. However, such releases also raise fundamental\n",
            "concerns related to privacy and security as they can reveal sensitive\n",
            "commercial information and expose system vulnerabilities. This paper\n",
            "investigates how to release power networks where the parameters of transmission\n",
            "lines and transformers are obfuscated. It does so by using the framework of\n",
            "Differential Privacy (DP), that provides strong privacy guarantees and has\n",
            "attracted significant attention in recent years. Unfortunately, simple DP\n",
            "mechanisms often result in AC-infeasible networks. To address these concerns,\n",
            "this paper presents a novel differential privacy mechanism that guarantees\n",
            "AC-feasibility and largely preserves the fidelity of the obfuscated network.\n",
            "Experimental results also show that the obfuscation significantly reduces the\n",
            "potential damage of an attacker exploiting the release of the dataset.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2106.12614 \n",
            "Title :Handwritten Digit Recognition using Machine and Deep Learning Algorithms\n",
            "  The reliance of humans over machines has never been so high such that from\n",
            "object classification in photographs to adding sound to silent movies\n",
            "everything can be performed with the help of deep learning and machine learning\n",
            "algorithms. Likewise, Handwritten text recognition is one of the significant\n",
            "areas of research and development with a streaming number of possibilities that\n",
            "could be attained. Handwriting recognition (HWR), also known as Handwritten\n",
            "Text Recognition (HTR), is the ability of a computer to receive and interpret\n",
            "intelligible handwritten input from sources such as paper documents,\n",
            "photographs, touch-screens and other devices [1]. Apparently, in this paper, we\n",
            "have performed handwritten digit recognition with the help of MNIST datasets\n",
            "using Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\n",
            "Convolution Neural Network (CNN) models. Our main objective is to compare the\n",
            "accuracy of the models stated above along with their execution time to get the\n",
            "best possible model for digit recognition.\n",
            "\n",
            "**Paper Id :2002.06306 \n",
            "Title :Jelly Bean World: A Testbed for Never-Ending Learning\n",
            "  Machine learning has shown growing success in recent years. However, current\n",
            "machine learning systems are highly specialized, trained for particular\n",
            "problems or domains, and typically on a single narrow dataset. Human learning,\n",
            "on the other hand, is highly general and adaptable. Never-ending learning is a\n",
            "machine learning paradigm that aims to bridge this gap, with the goal of\n",
            "encouraging researchers to design machine learning systems that can learn to\n",
            "perform a wider variety of inter-related tasks in more complex environments. To\n",
            "date, there is no environment or testbed to facilitate the development and\n",
            "evaluation of never-ending learning systems. To this end, we propose the Jelly\n",
            "Bean World testbed. The Jelly Bean World allows experimentation over\n",
            "two-dimensional grid worlds which are filled with items and in which agents can\n",
            "navigate. This testbed provides environments that are sufficiently complex and\n",
            "where more generally intelligent algorithms ought to perform better than\n",
            "current state-of-the-art reinforcement learning approaches. It does so by\n",
            "producing non-stationary environments and facilitating experimentation with\n",
            "multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope\n",
            "that this new freely-available software will prompt new research and interest\n",
            "in the development and evaluation of never-ending learning systems and more\n",
            "broadly, general intelligence systems.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2106.13552 \n",
            "Title :Graph Pattern Loss based Diversified Attention Network for Cross-Modal\n",
            "  Retrieval\n",
            "  Cross-modal retrieval aims to enable flexible retrieval experience by\n",
            "combining multimedia data such as image, video, text, and audio. One core of\n",
            "unsupervised approaches is to dig the correlations among different object\n",
            "representations to complete satisfied retrieval performance without requiring\n",
            "expensive labels. In this paper, we propose a Graph Pattern Loss based\n",
            "Diversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to\n",
            "deeply analyze correlations among representations. First, we propose a\n",
            "diversified attention feature projector by considering the interaction between\n",
            "different representations to generate multiple representations of an instance.\n",
            "Then, we design a novel graph pattern loss to explore the correlations among\n",
            "different representations, in this graph all possible distances between\n",
            "different representations are considered. In addition, a modality classifier is\n",
            "added to explicitly declare the corresponding modalities of features before\n",
            "fusion and guide the network to enhance discrimination ability. We test GPLDAN\n",
            "on four public datasets. Compared with the state-of-the-art cross-modal\n",
            "retrieval methods, the experimental results demonstrate the performance and\n",
            "competitiveness of GPLDAN.\n",
            "\n",
            "**Paper Id :2009.13862 \n",
            "Title :Where is the Model Looking At?--Concentrate and Explain the Network\n",
            "  Attention\n",
            "  Image classification models have achieved satisfactory performance on many\n",
            "datasets, sometimes even better than human. However, The model attention is\n",
            "unclear since the lack of interpretability. This paper investigates the\n",
            "fidelity and interpretability of model attention. We propose an Explainable\n",
            "Attribute-based Multi-task (EAT) framework to concentrate the model attention\n",
            "on the discriminative image area and make the attention interpretable. We\n",
            "introduce attributes prediction to the multi-task learning network, helping the\n",
            "network to concentrate attention on the foreground objects. We generate\n",
            "attribute-based textual explanations for the network and ground the attributes\n",
            "on the image to show visual explanations. The multi-model explanation can not\n",
            "only improve user trust but also help to find the weakness of network and\n",
            "dataset. Our framework can be generalized to any basic model. We perform\n",
            "experiments on three datasets and five basic models. Results indicate that the\n",
            "EAT framework can give multi-modal explanations that interpret the network\n",
            "decision. The performance of several recognition approaches is improved by\n",
            "guiding network attention.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2107.00528 \n",
            "Title :Visualising Argumentation Graphs with Graph Embeddings and t-SNE\n",
            "  This paper applies t-SNE, a visualisation technique familiar from Deep Neural\n",
            "Network research to argumentation graphs by applying it to the output of graph\n",
            "embeddings generated using several different methods. It shows that such a\n",
            "visualisation approach can work for argumentation and show interesting\n",
            "structural properties of argumentation graphs, opening up paths for further\n",
            "research in the area.\n",
            "\n",
            "**Paper Id :1709.05360 \n",
            "Title :Embedding Deep Networks into Visual Explanations\n",
            "  In this paper, we propose a novel Explanation Neural Network (XNN) to explain\n",
            "the predictions made by a deep network. The XNN works by learning a nonlinear\n",
            "embedding of a high-dimensional activation vector of a deep network layer into\n",
            "a low-dimensional explanation space while retaining faithfulness i.e., the\n",
            "original deep learning predictions can be constructed from the few concepts\n",
            "extracted by our explanation network. We then visualize such concepts for human\n",
            "to learn about the high-level concepts that the deep network is using to make\n",
            "decisions. We propose an algorithm called Sparse Reconstruction Autoencoder\n",
            "(SRAE) for learning the embedding to the explanation space. SRAE aims to\n",
            "reconstruct part of the original feature space while retaining faithfulness. A\n",
            "pull-away term is applied to SRAE to make the bases of the explanation space\n",
            "more orthogonal to each other. A visualization system is then introduced for\n",
            "human understanding of the features in the explanation space. The proposed\n",
            "method is applied to explain CNN models in image classification tasks. We\n",
            "conducted a human study, which shows that the proposed approach outperforms\n",
            "single saliency map baselines, and improves human performance on a difficult\n",
            "classification tasks. Also, several novel metrics are introduced to evaluate\n",
            "the performance of explanations quantitatively without human involvement.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2107.02175 \n",
            "Title :Identifying negativity factors from social media text corpus using\n",
            "  sentiment analysis method\n",
            "  Automatic sentiment analysis play vital role in decision making. Many\n",
            "organizations spend a lot of budget to understand their customer satisfaction\n",
            "by manually going over their feedback/comments or tweets. Automatic sentiment\n",
            "analysis can give overall picture of the comments received against any event,\n",
            "product, or activity. Usually, the comments/tweets are classified into two main\n",
            "classes that are negative or positive. However, the negative comments are too\n",
            "abstract to understand the basic reason or the context. organizations are\n",
            "interested to identify the exact reason for the negativity. In this research\n",
            "study, we hierarchically goes down into negative comments, and link them with\n",
            "more classes. Tweets are extracted from social media sites such as Twitter and\n",
            "Facebook. If the sentiment analysis classifies any tweet into negative class,\n",
            "then we further try to associates that negative comments with more possible\n",
            "negative classes. Based on expert opinions, the negative comments/tweets are\n",
            "further classified into 8 classes. Different machine learning algorithms are\n",
            "evaluated and their accuracy are reported.\n",
            "\n",
            "**Paper Id :1905.04071 \n",
            "Title :Survey on Evaluation Methods for Dialogue Systems\n",
            "  In this paper we survey the methods and concepts developed for the evaluation\n",
            "of dialogue systems. Evaluation is a crucial part during the development\n",
            "process. Often, dialogue systems are evaluated by means of human evaluations\n",
            "and questionnaires. However, this tends to be very cost and time intensive.\n",
            "Thus, much work has been put into finding methods, which allow to reduce the\n",
            "involvement of human labour. In this survey, we present the main concepts and\n",
            "methods. For this, we differentiate between the various classes of dialogue\n",
            "systems (task-oriented dialogue systems, conversational dialogue systems, and\n",
            "question-answering dialogue systems). We cover each class by introducing the\n",
            "main technologies developed for the dialogue systems and then by presenting the\n",
            "evaluation methods regarding this class.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2107.02466 \n",
            "Title :On-edge Multi-task Transfer Learning: Model and Practice with\n",
            "  Data-driven Task Allocation\n",
            "  On edge devices, data scarcity occurs as a common problem where transfer\n",
            "learning serves as a widely-suggested remedy. Nevertheless, transfer learning\n",
            "imposes a heavy computation burden to resource-constrained edge devices.\n",
            "Existing task allocation works usually assume all submitted tasks are equally\n",
            "important, leading to inefficient resource allocation at a task level when\n",
            "directly applied in Multi-task Transfer Learning (MTL). To address these\n",
            "issues, we first reveal that it is crucial to measure the impact of tasks on\n",
            "overall decision performance improvement and quantify \\emph{task importance}.\n",
            "We then show that task allocation with task importance for MTL (TATIM) is a\n",
            "variant of the NP-complete Knapsack problem, where the complicated computation\n",
            "to solve this problem needs to be conducted repeatedly under varying contexts.\n",
            "To solve TATIM with high computational efficiency, we propose a Data-driven\n",
            "Cooperative Task Allocation (DCTA) approach. Finally, we evaluate the\n",
            "performance of DCTA by not only a trace-driven simulation, but also a new\n",
            "comprehensive real-world AIOps case study that bridges model and practice via a\n",
            "new architecture and main components design within the AIOps system. Extensive\n",
            "experiments show that our DCTA reduces 3.24 times of processing time, and saves\n",
            "48.4\\% energy consumption compared with the state-of-the-art when solving\n",
            "TATIM.\n",
            "\n",
            "**Paper Id :1912.05748 \n",
            "Title :Multi-Agent Task Allocation in Complementary Teams: A Hunter and\n",
            "  Gatherer Approach\n",
            "  Consider a dynamic task allocation problem, where tasks are unknowingly\n",
            "distributed over an environment. This paper considers each task comprised of\n",
            "two sequential subtasks: detection and completion, where each subtask can only\n",
            "be carried out by a certain type of agent. We address this problem using a\n",
            "novel nature-inspired approach called \"hunter and gatherer\". The proposed\n",
            "method employs two complementary teams of agents: one agile in detecting\n",
            "(hunters) and another skillful in completing (gatherers) the tasks. To minimize\n",
            "the collective cost of task accomplishments in a distributed manner, a\n",
            "game-theoretic solution is introduced to couple agents from complementary\n",
            "teams. We utilize market-based negotiation models to develop incentive-based\n",
            "decision-making algorithms relying on innovative notions of \"certainty and\n",
            "uncertainty profit margins\". The simulation results demonstrate that employing\n",
            "two complementary teams of hunters and gatherers can effectually improve the\n",
            "number of tasks completed by agents compared to conventional methods, while the\n",
            "collective cost of accomplishments is minimized. In addition, the stability and\n",
            "efficacy of the proposed solutions are studied using Nash equilibrium analysis\n",
            "and statistical analysis respectively. It is also numerically shown that the\n",
            "proposed solutions function fairly, i.e. for each type of agent, the overall\n",
            "workload is distributed equally.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2107.11820 \n",
            "Title :A Survey of Monte Carlo Methods for Parameter Estimation\n",
            "  Statistical signal processing applications usually require the estimation of\n",
            "some parameters of interest given a set of observed data. These estimates are\n",
            "typically obtained either by solving a multi-variate optimization problem, as\n",
            "in the maximum likelihood (ML) or maximum a posteriori (MAP) estimators, or by\n",
            "performing a multi-dimensional integration, as in the minimum mean squared\n",
            "error (MMSE) estimators. Unfortunately, analytical expressions for these\n",
            "estimators cannot be found in most real-world applications, and the Monte Carlo\n",
            "(MC) methodology is one feasible approach. MC methods proceed by drawing random\n",
            "samples, either from the desired distribution or from a simpler one, and using\n",
            "them to compute consistent estimators. The most important families of MC\n",
            "algorithms are Markov chain MC (MCMC) and importance sampling (IS). On the one\n",
            "hand, MCMC methods draw samples from a proposal density, building then an\n",
            "ergodic Markov chain whose stationary distribution is the desired distribution\n",
            "by accepting or rejecting those candidate samples as the new state of the\n",
            "chain. On the other hand, IS techniques draw samples from a simple proposal\n",
            "density, and then assign them suitable weights that measure their quality in\n",
            "some appropriate way. In this paper, we perform a thorough review of MC methods\n",
            "for the estimation of static parameters in signal processing applications. A\n",
            "historical note on the development of MC schemes is also provided, followed by\n",
            "the basic MC method and a brief description of the rejection sampling (RS)\n",
            "algorithm, as well as three sections describing many of the most relevant MCMC\n",
            "and IS algorithms, and their combined use.\n",
            "\n",
            "**Paper Id :2006.08150 \n",
            "Title :A VIKOR and TOPSIS focused reanalysis of the MADM methods based on\n",
            "  logarithmic normalization\n",
            "  Decision and policy-makers in multi-criteria decision-making analysis take\n",
            "into account some strategies in order to analyze outcomes and to finally make\n",
            "an effective and more precise decision. Among those strategies, the\n",
            "modification of the normalization process in the multiple-criteria\n",
            "decision-making algorithm is still a question due to the confrontation of many\n",
            "normalization tools. Normalization is the basic action in defining and solving\n",
            "a MADM problem and a MADM model. Normalization is the first, also necessary,\n",
            "step in solving, i.e. the application of a MADM method. It is a fact that the\n",
            "selection of normalization methods has a direct effect on the results. One of\n",
            "the latest normalization methods introduced is the Logarithmic Normalization\n",
            "(LN) method. This new method has a distinguished advantage, reflecting in that\n",
            "a sum of the normalized values of criteria always equals 1. This normalization\n",
            "method had never been applied in any MADM methods before. This research study\n",
            "is focused on the analysis of the classical MADM methods based on logarithmic\n",
            "normalization. VIKOR and TOPSIS, as the two famous MADM methods, were selected\n",
            "for this reanalysis research study. Two numerical examples were checked in both\n",
            "methods, based on both the classical and the novel ways based on the LN. The\n",
            "results indicate that there are differences between the two approaches.\n",
            "Eventually, a sensitivity analysis is also designed to illustrate the\n",
            "reliability of the final results.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2108.08009 \n",
            "Title :XAI Methods for Neural Time Series Classification: A Brief Review\n",
            "  Deep learning models have recently demonstrated remarkable results in a\n",
            "variety of tasks, which is why they are being increasingly applied in\n",
            "high-stake domains, such as industry, medicine, and finance. Considering that\n",
            "automatic predictions in these domains might have a substantial impact on the\n",
            "well-being of a person, as well as considerable financial and legal\n",
            "consequences to an individual or a company, all actions and decisions that\n",
            "result from applying these models have to be accountable. Given that a\n",
            "substantial amount of data that is collected in high-stake domains are in the\n",
            "form of time series, in this paper we examine the current state of eXplainable\n",
            "AI (XAI) methods with a focus on approaches for opening up deep learning black\n",
            "boxes for the task of time series classification. Finally, our contribution\n",
            "also aims at deriving promising directions for future work, to advance XAI for\n",
            "deep learning on time series data.\n",
            "\n",
            "**Paper Id :2001.01126 \n",
            "Title :Can x2vec Save Lives? Integrating Graph and Language Embeddings for\n",
            "  Automatic Mental Health Classification\n",
            "  Graph and language embedding models are becoming commonplace in large scale\n",
            "analyses given their ability to represent complex sparse data densely in\n",
            "low-dimensional space. Integrating these models' complementary relational and\n",
            "communicative data may be especially helpful if predicting rare events or\n",
            "classifying members of hidden populations - tasks requiring huge and sparse\n",
            "datasets for generalizable analyses. For example, due to social stigma and\n",
            "comorbidities, mental health support groups often form in amorphous online\n",
            "groups. Predicting suicidality among individuals in these settings using\n",
            "standard network analyses is prohibitive due to resource limits (e.g., memory),\n",
            "and adding auxiliary data like text to such models exacerbates complexity- and\n",
            "sparsity-related issues. Here, I show how merging graph and language embedding\n",
            "models (metapath2vec and doc2vec) avoids these limits and extracts unsupervised\n",
            "clustering data without domain expertise or feature engineering. Graph and\n",
            "language distances to a suicide support group have little correlation (\\r{ho} <\n",
            "0.23), implying the two models are not embedding redundant information. When\n",
            "used separately to predict suicidality among individuals, graph and language\n",
            "data generate relatively accurate results (69% and 76%, respectively); however,\n",
            "when integrated, both data produce highly accurate predictions (90%, with 10%\n",
            "false-positives and 12% false-negatives). Visualizing graph embeddings\n",
            "annotated with predictions of potentially suicidal individuals shows the\n",
            "integrated model could classify such individuals even if they are positioned\n",
            "far from the support group. These results extend research on the importance of\n",
            "simultaneously analyzing behavior and language in massive networks and efforts\n",
            "to integrate embedding models for different kinds of data when predicting and\n",
            "classifying, particularly when they involve rare events.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2110.07235 \n",
            "Title :HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive\n",
            "  Media\n",
            "  We introduce HUMAN4D, a large and multimodal 4D dataset that contains a\n",
            "variety of human activities simultaneously captured by a professional\n",
            "marker-based MoCap, a volumetric capture and an audio recording system. By\n",
            "capturing 2 female and $2$ male professional actors performing various\n",
            "full-body movements and expressions, HUMAN4D provides a diverse set of motions\n",
            "and poses encountered as part of single- and multi-person daily, physical and\n",
            "social activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),\n",
            "volumetric and audio data. Despite the existence of multi-view color datasets\n",
            "captured with the use of hardware (HW) synchronization, to the best of our\n",
            "knowledge, HUMAN4D is the first and only public resource that provides\n",
            "volumetric depth maps with high synchronization precision due to the use of\n",
            "intra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned\n",
            "and rigged 3D character complements HUMAN4D to enable joint research on\n",
            "time-varying and high-quality dynamic meshes. We provide evaluation baselines\n",
            "by benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D\n",
            "compression methods. For the former, we apply 2D and 3D pose estimation\n",
            "algorithms both on single- and multi-view data cues. For the latter, we\n",
            "benchmark open-source 3D codecs on volumetric data respecting online volumetric\n",
            "video encoding and steady bit-rates. Furthermore, qualitative and quantitative\n",
            "visual comparison between mesh-based volumetric data reconstructed in different\n",
            "qualities showcases the available options with respect to 4D representations.\n",
            "HUMAN4D is introduced to the computer vision and graphics research communities\n",
            "to enable joint research on spatio-temporally aligned pose, volumetric, mRGBD\n",
            "and audio data cues. The dataset and its code are available\n",
            "https://tofis.github.io/myurls/human4d.\n",
            "\n",
            "**Paper Id :2004.11204 \n",
            "Title :Classification using Hyperdimensional Computing: A Review\n",
            "  Hyperdimensional (HD) computing is built upon its unique data type referred\n",
            "to as hypervectors. The dimension of these hypervectors is typically in the\n",
            "range of tens of thousands. Proposed to solve cognitive tasks, HD computing\n",
            "aims at calculating similarity among its data. Data transformation is realized\n",
            "by three operations, including addition, multiplication and permutation. Its\n",
            "ultra-wide data representation introduces redundancy against noise. Since\n",
            "information is evenly distributed over every bit of the hypervectors, HD\n",
            "computing is inherently robust. Additionally, due to the nature of those three\n",
            "operations, HD computing leads to fast learning ability, high energy efficiency\n",
            "and acceptable accuracy in learning and classification tasks. This paper\n",
            "introduces the background of HD computing, and reviews the data representation,\n",
            "data transformation, and similarity measurement. The orthogonality in high\n",
            "dimensions presents opportunities for flexible computing. To balance the\n",
            "tradeoff between accuracy and efficiency, strategies include but are not\n",
            "limited to encoding, retraining, binarization and hardware acceleration.\n",
            "Evaluations indicate that HD computing shows great potential in addressing\n",
            "problems using data in the form of letters, signals and images. HD computing\n",
            "especially shows significant promise to replace machine learning algorithms as\n",
            "a light-weight classifier in the field of internet of things (IoTs).\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2110.15385 \n",
            "Title :Data-driven Residual Generation for Early Fault Detection with Limited\n",
            "  Data\n",
            "  Traditionally, fault detection and isolation community has used system\n",
            "dynamic equations to generate diagnosers and to analyze detectability and\n",
            "isolability of the dynamic systems. Model-based fault detection and isolation\n",
            "methods use system model to generate a set of residuals as the bases for fault\n",
            "detection and isolation. However, in many complex systems it is not feasible to\n",
            "develop highly accurate models for the systems and to keep the models updated\n",
            "during the system lifetime. Recently, data-driven solutions have received an\n",
            "immense attention in the industries systems for several practical reasons.\n",
            "First, these methods do not require the initial investment and expertise for\n",
            "developing accurate models. Moreover, it is possible to automatically update\n",
            "and retrain the diagnosers as the system or the environment change over time.\n",
            "Finally, unlike the model-based methods it is straight forward to combine time\n",
            "series measurements such as pressure and voltage with other sources of\n",
            "information such as system operating hours to achieve a higher accuracy. In\n",
            "this paper, we extend the traditional model-based fault detection and isolation\n",
            "concepts such as residuals, and detectable and isolable faults to the\n",
            "data-driven domain. We then propose an algorithm to automatically generate\n",
            "residuals from the normal operating data. We present the performance of our\n",
            "proposed approach through a comparative case study.\n",
            "\n",
            "**Paper Id :2003.01668 \n",
            "Title :Model Assertions for Monitoring and Improving ML Models\n",
            "  ML models are increasingly deployed in settings with real world interactions\n",
            "such as vehicles, but unfortunately, these models can fail in systematic ways.\n",
            "To prevent errors, ML engineering teams monitor and continuously improve these\n",
            "models. We propose a new abstraction, model assertions, that adapts the\n",
            "classical use of program assertions as a way to monitor and improve ML models.\n",
            "Model assertions are arbitrary functions over a model's input and output that\n",
            "indicate when errors may be occurring, e.g., a function that triggers if an\n",
            "object rapidly changes its class in a video. We propose methods of using model\n",
            "assertions at all stages of ML system deployment, including runtime monitoring,\n",
            "validating labels, and continuously improving ML models. For runtime\n",
            "monitoring, we show that model assertions can find high confidence errors,\n",
            "where a model returns the wrong output with high confidence, which\n",
            "uncertainty-based monitoring techniques would not detect. For training, we\n",
            "propose two methods of using model assertions. First, we propose a bandit-based\n",
            "active learning algorithm that can sample from data flagged by assertions and\n",
            "show that it can reduce labeling costs by up to 40% over traditional\n",
            "uncertainty-based methods. Second, we propose an API for generating\n",
            "\"consistency assertions\" (e.g., the class change example) and weak labels for\n",
            "inputs where the consistency assertions fail, and show that these weak labels\n",
            "can improve relative model quality by up to 46%. We evaluate model assertions\n",
            "on four real-world tasks with video, LIDAR, and ECG data.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2111.04689 \n",
            "Title :A Comparison of Model-Free and Model Predictive Control for Price\n",
            "  Responsive Water Heaters\n",
            "  We present a careful comparison of two model-free control algorithms,\n",
            "Evolution Strategies (ES) and Proximal Policy Optimization (PPO), with receding\n",
            "horizon model predictive control (MPC) for operating simulated, price\n",
            "responsive water heaters. Four MPC variants are considered: a one-shot\n",
            "controller with perfect forecasting yielding optimal control; a limited-horizon\n",
            "controller with perfect forecasting; a mean forecasting-based controller; and a\n",
            "two-stage stochastic programming controller using historical scenarios. In all\n",
            "cases, the MPC model for water temperature and electricity price are exact;\n",
            "only water demand is uncertain. For comparison, both ES and PPO learn neural\n",
            "network-based policies by directly interacting with the simulated environment\n",
            "under the same scenarios used by MPC. All methods are then evaluated on a\n",
            "separate one-week continuation of the demand time series. We demonstrate that\n",
            "optimal control for this problem is challenging, requiring more than 8-hour\n",
            "lookahead for MPC with perfect forecasting to attain the minimum cost. Despite\n",
            "this challenge, both ES and PPO learn good general purpose policies that\n",
            "outperform mean forecast and two-stage stochastic MPC controllers in terms of\n",
            "average cost and are more than two orders of magnitude faster at computing\n",
            "actions. We show that ES in particular can leverage parallelism to learn a\n",
            "policy in under 90 seconds using 1150 CPU cores.\n",
            "\n",
            "**Paper Id :1906.04595 \n",
            "Title :Evaluating aleatoric and epistemic uncertainties of time series deep\n",
            "  learning models for soil moisture predictions\n",
            "  Soil moisture is an important variable that determines floods, vegetation\n",
            "health, agriculture productivity, and land surface feedbacks to the atmosphere,\n",
            "etc. Accurately modeling soil moisture has important implications in both\n",
            "weather and climate models. The recently available satellite-based observations\n",
            "give us a unique opportunity to build data-driven models to predict soil\n",
            "moisture instead of using land surface models, but previously there was no\n",
            "uncertainty estimate. We tested Monte Carlo dropout (MCD) with an aleatoric\n",
            "term for our long short-term memory models for this problem, and asked if the\n",
            "uncertainty terms behave as they were argued to. We show that the method\n",
            "successfully captures the predictive error after tuning a hyperparameter on a\n",
            "representative training dataset. We show the MCD uncertainty estimate, as\n",
            "previously argued, does detect dissimilarity.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2111.10871 \n",
            "Title :A Software Tool for Evaluating Unmanned Autonomous Systems\n",
            "  The North Carolina Agriculture and Technical State University (NC A&T) in\n",
            "collaboration with Georgia Tech Research Institute (GTRI) has developed\n",
            "methodologies for creating simulation-based technology tools that are capable\n",
            "of inferring the perceptions and behavioral states of autonomous systems. These\n",
            "methodologies have the potential to provide the Test and Evaluation (T&E)\n",
            "community at the Department of Defense (DoD) with a greater insight into the\n",
            "internal processes of these systems. The methodologies use only external\n",
            "observations and do not require complete knowledge of the internal processing\n",
            "of and/or any modifications to the system under test. This paper presents an\n",
            "example of one such simulation-based technology tool, named as the Data-Driven\n",
            "Intelligent Prediction Tool (DIPT). DIPT was developed for testing a\n",
            "multi-platform Unmanned Aerial Vehicle (UAV) system capable of conducting\n",
            "collaborative search missions. DIPT's Graphical User Interface (GUI) enables\n",
            "the testers to view the aircraft's current operating state, predicts its\n",
            "current target-detection status, and provides reasoning for exhibiting a\n",
            "particular behavior along with an explanation of assigning a particular task to\n",
            "it.\n",
            "\n",
            "**Paper Id :2012.01176 \n",
            "Title :Proceedings Second Workshop on Formal Methods for Autonomous Systems\n",
            "  Autonomous systems are highly complex and present unique challenges for the\n",
            "application of formal methods. Autonomous systems act without human\n",
            "intervention, and are often embedded in a robotic system, so that they can\n",
            "interact with the real world. As such, they exhibit the properties of\n",
            "safety-critical, cyber-physical, hybrid, and real-time systems.\n",
            "  The goal of FMAS is to bring together leading researchers who are tackling\n",
            "the unique challenges of autonomous systems using formal methods, to present\n",
            "recent and ongoing work. We are interested in the use of formal methods to\n",
            "specify, model, or verify autonomous or robotic systems; in whole or in part.\n",
            "We are also interested in successful industrial applications and potential\n",
            "future directions for this emerging application of formal methods.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2112.05575 \n",
            "Title :Paradigms of Computational Agency\n",
            "  Agent-based models have emerged as a promising paradigm for addressing ever\n",
            "increasing complexity of information systems. In its initial days in the 1990s\n",
            "when object-oriented modeling was at its peak, an agent was treated as a\n",
            "special kind of \"object\" that had a persistent state and its own independent\n",
            "thread of execution. Since then, agent-based models have diversified enormously\n",
            "to even open new conceptual insights about the nature of systems in general.\n",
            "This paper presents a perspective on the disparate ways in which our\n",
            "understanding of agency, as well as computational models of agency have\n",
            "evolved. Advances in hardware like GPUs, that brought neural networks back to\n",
            "life, may also similarly infuse new life into agent-based models, as well as\n",
            "pave the way for advancements in research on Artificial General Intelligence\n",
            "(AGI).\n",
            "\n",
            "**Paper Id :1905.04232 \n",
            "Title :Automatic Programming of Cellular Automata and Artificial Neural\n",
            "  Networks Guided by Philosophy\n",
            "  Many computer models such as cellular automata and artificial neural networks\n",
            "have been developed and successfully applied. However, in some cases, these\n",
            "models might be restrictive on the possible solutions or their solutions might\n",
            "be difficult to interpret. To overcome this problem, we outline a new approach,\n",
            "the so-called allagmatic method, that automatically programs and executes\n",
            "models with as little limitations as possible while maintaining human\n",
            "interpretability. Earlier we described a metamodel and its building blocks\n",
            "according to the philosophical concepts of structure (spatial dimension) and\n",
            "operation (temporal dimension). They are entity, milieu, and update function\n",
            "that together abstractly describe cellular automata, artificial neural\n",
            "networks, and possibly any kind of computer model. By automatically combining\n",
            "these building blocks in an evolutionary computation, interpretability might be\n",
            "increased by the relationship to the metamodel, and models might be translated\n",
            "into more interpretable models via the metamodel. We propose generic and\n",
            "object-oriented programming to implement the entities and their milieus as\n",
            "dynamic and generic arrays and the update function as a method. We show two\n",
            "experiments where a simple cellular automaton and an artificial neural network\n",
            "are automatically programmed, compiled, and executed. A target state is\n",
            "successfully evolved and learned in the cellular automaton and artificial\n",
            "neural network, respectively. We conclude that the allagmatic method can create\n",
            "and execute cellular automaton and artificial neural network models in an\n",
            "automated manner with the guidance of philosophy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2112.10017 \n",
            "Title :Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks\n",
            "  Existing research on continual learning of a sequence of tasks focused on\n",
            "dealing with catastrophic forgetting, where the tasks are assumed to be\n",
            "dissimilar and have little shared knowledge. Some work has also been done to\n",
            "transfer previously learned knowledge to the new task when the tasks are\n",
            "similar and have shared knowledge. To the best of our knowledge, no technique\n",
            "has been proposed to learn a sequence of mixed similar and dissimilar tasks\n",
            "that can deal with forgetting and also transfer knowledge forward and backward.\n",
            "This paper proposes such a technique to learn both types of tasks in the same\n",
            "network. For dissimilar tasks, the algorithm focuses on dealing with\n",
            "forgetting, and for similar tasks, the algorithm focuses on selectively\n",
            "transferring the knowledge learned from some similar previous tasks to improve\n",
            "the new task learning. Additionally, the algorithm automatically detects\n",
            "whether a new task is similar to any previous tasks. Empirical evaluation using\n",
            "sequences of mixed tasks demonstrates the effectiveness of the proposed model.\n",
            "\n",
            "**Paper Id :1912.05748 \n",
            "Title :Multi-Agent Task Allocation in Complementary Teams: A Hunter and\n",
            "  Gatherer Approach\n",
            "  Consider a dynamic task allocation problem, where tasks are unknowingly\n",
            "distributed over an environment. This paper considers each task comprised of\n",
            "two sequential subtasks: detection and completion, where each subtask can only\n",
            "be carried out by a certain type of agent. We address this problem using a\n",
            "novel nature-inspired approach called \"hunter and gatherer\". The proposed\n",
            "method employs two complementary teams of agents: one agile in detecting\n",
            "(hunters) and another skillful in completing (gatherers) the tasks. To minimize\n",
            "the collective cost of task accomplishments in a distributed manner, a\n",
            "game-theoretic solution is introduced to couple agents from complementary\n",
            "teams. We utilize market-based negotiation models to develop incentive-based\n",
            "decision-making algorithms relying on innovative notions of \"certainty and\n",
            "uncertainty profit margins\". The simulation results demonstrate that employing\n",
            "two complementary teams of hunters and gatherers can effectually improve the\n",
            "number of tasks completed by agents compared to conventional methods, while the\n",
            "collective cost of accomplishments is minimized. In addition, the stability and\n",
            "efficacy of the proposed solutions are studied using Nash equilibrium analysis\n",
            "and statistical analysis respectively. It is also numerically shown that the\n",
            "proposed solutions function fairly, i.e. for each type of agent, the overall\n",
            "workload is distributed equally.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2112.10021 \n",
            "Title :Continual Learning with Knowledge Transfer for Sentiment Classification\n",
            "  This paper studies continual learning (CL) for sentiment classification (SC).\n",
            "In this setting, the CL system learns a sequence of SC tasks incrementally in a\n",
            "neural network, where each task builds a classifier to classify the sentiment\n",
            "of reviews of a particular product category or domain. Two natural questions\n",
            "are: Can the system transfer the knowledge learned in the past from the\n",
            "previous tasks to the new task to help it learn a better model for the new\n",
            "task? And, can old models for previous tasks be improved in the process as\n",
            "well? This paper proposes a novel technique called KAN to achieve these\n",
            "objectives. KAN can markedly improve the SC accuracy of both the new task and\n",
            "the old tasks via forward and backward knowledge transfer. The effectiveness of\n",
            "KAN is demonstrated through extensive experiments.\n",
            "\n",
            "**Paper Id :1909.04719 \n",
            "Title :Neural Belief Reasoner\n",
            "  This paper proposes a new generative model called neural belief reasoner\n",
            "(NBR). It differs from previous models in that it specifies a belief function\n",
            "rather than a probability distribution. Its implementation consists of neural\n",
            "networks, fuzzy-set operations and belief-function operations, and\n",
            "query-answering, sample-generation and training algorithms are presented. This\n",
            "paper studies NBR in two tasks. The first is a synthetic unsupervised-learning\n",
            "task, which demonstrates NBR's ability to perform multi-hop reasoning,\n",
            "reasoning with uncertainty and reasoning about conflicting information. The\n",
            "second is supervised learning: a robust MNIST classifier for 4 and 9, which is\n",
            "the most challenging pair of digits. This classifier needs no adversarial\n",
            "training, and it substantially exceeds the state of the art in adversarial\n",
            "robustness as measured by the L2 metric, while at the same time maintains 99.1%\n",
            "accuracy on natural images.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2201.07489 \n",
            "Title :Development of Fake News Model using Machine Learning through Natural\n",
            "  Language Processing\n",
            "  Fake news detection research is still in the early stage as this is a\n",
            "relatively new phenomenon in the interest raised by society. Machine learning\n",
            "helps to solve complex problems and to build AI systems nowadays and especially\n",
            "in those cases where we have tacit knowledge or the knowledge that is not\n",
            "known. We used machine learning algorithms and for identification of fake news;\n",
            "we applied three classifiers; Passive Aggressive, Na\\\"ive Bayes, and Support\n",
            "Vector Machine. Simple classification is not completely correct in fake news\n",
            "detection because classification methods are not specialized for fake news.\n",
            "With the integration of machine learning and text-based processing, we can\n",
            "detect fake news and build classifiers that can classify the news data. Text\n",
            "classification mainly focuses on extracting various features of text and after\n",
            "that incorporating those features into classification. The big challenge in\n",
            "this area is the lack of an efficient way to differentiate between fake and\n",
            "non-fake due to the unavailability of corpora. We applied three different\n",
            "machine learning classifiers on two publicly available datasets. Experimental\n",
            "analysis based on the existing dataset indicates a very encouraging and\n",
            "improved performance.\n",
            "\n",
            "**Paper Id :2205.00210 \n",
            "Title :Software Testing for Machine Learning\n",
            "  Machine learning has become prevalent across a wide variety of applications.\n",
            "Unfortunately, machine learning has also shown to be susceptible to deception,\n",
            "leading to errors, and even fatal failures. This circumstance calls into\n",
            "question the widespread use of machine learning, especially in safety-critical\n",
            "applications, unless we are able to assure its correctness and trustworthiness\n",
            "properties. Software verification and testing are established technique for\n",
            "assuring such properties, for example by detecting errors. However, software\n",
            "testing challenges for machine learning are vast and profuse - yet critical to\n",
            "address. This summary talk discusses the current state-of-the-art of software\n",
            "testing for machine learning. More specifically, it discusses six key challenge\n",
            "areas for software testing of machine learning systems, examines current\n",
            "approaches to these challenges and highlights their limitations. The paper\n",
            "provides a research agenda with elaborated directions for making progress\n",
            "toward advancing the state-of-the-art on testing of machine learning.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2201.08167 \n",
            "Title :Chatbot Based Solution for Supporting Software Incident Management\n",
            "  Process\n",
            "  A set of steps for implementing a chatbot, to support decision-making\n",
            "activities in the software incident management process is proposed and\n",
            "discussed in this article. Each step is presented independently of the platform\n",
            "used for the construction of chatbots and are detailed with their respective\n",
            "activities. The proposed steps can be carried out in a continuous and adaptable\n",
            "way, favoring the constant training of a chatbot and allowing the increasingly\n",
            "cohesive interpretatin of the intentions of the specialists who work in the\n",
            "Software Incident Management Process. The software incident resolution process\n",
            "accordingly to the ITIL framework, is considered for the experiment. The\n",
            "results of the work present the steps for the chatbot construction, the\n",
            "solution based on DialogFlow platform and some conclusions based on the\n",
            "experiment.\n",
            "\n",
            "**Paper Id :2005.11014 \n",
            "Title :Intent Mining from past conversations for conversational agent\n",
            "  Conversational systems are of primary interest in the AI community. Chatbots\n",
            "are increasingly being deployed to provide round-the-clock support and to\n",
            "increase customer engagement. Many of the commercial bot building frameworks\n",
            "follow a standard approach that requires one to build and train an intent model\n",
            "to recognize a user input. Intent models are trained in a supervised setting\n",
            "with a collection of textual utterance and intent label pairs. Gathering a\n",
            "substantial and wide coverage of training data for different intent is a\n",
            "bottleneck in the bot building process. Moreover, the cost of labeling a\n",
            "hundred to thousands of conversations with intent is a time consuming and\n",
            "laborious job. In this paper, we present an intent discovery framework that\n",
            "involves 4 primary steps: Extraction of textual utterances from a conversation\n",
            "using a pre-trained domain agnostic Dialog Act Classifier (Data Extraction),\n",
            "automatic clustering of similar user utterances (Clustering), manual annotation\n",
            "of clusters with an intent label (Labeling) and propagation of intent labels to\n",
            "the utterances from the previous step, which are not mapped to any cluster\n",
            "(Label Propagation); to generate intent training data from raw conversations.\n",
            "We have introduced a novel density-based clustering algorithm ITER-DBSCAN for\n",
            "unbalanced data clustering. Subject Matter Expert (Annotators with domain\n",
            "expertise) manually looks into the clustered user utterances and provides an\n",
            "intent label for discovery. We conducted user studies to validate the\n",
            "effectiveness of the trained intent model generated in terms of coverage of\n",
            "intents, accuracy and time saving concerning manual annotation. Although the\n",
            "system is developed for building an intent model for the conversational system,\n",
            "this framework can also be used for a short text clustering or as a labeling\n",
            "framework.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2202.01660 \n",
            "Title :Computational Aspects of Conditional Minisum Approval Voting in\n",
            "  Elections with Interdependent Issues\n",
            "  Approval voting provides a simple, practical framework for multi-issue\n",
            "elections, and the most representative example among such election rules is the\n",
            "classic Minisum approval voting rule. We consider a generalization of Minisum,\n",
            "introduced by the work of Barrot and Lang [2016], referred to as Conditional\n",
            "Minisum, where voters are also allowed to express dependencies between issues.\n",
            "The price we have to pay when we move to this higher level of expressiveness is\n",
            "that we end up with a computationally hard rule. Motivated by this, we focus on\n",
            "the computational aspects of Conditional Minisum, where progress has been\n",
            "rather scarce so far. We identify restrictions that concern the voters'\n",
            "dependencies and the value of an optimal solution, under which we provide the\n",
            "first multiplicative approximation algorithms for the problem. At the same\n",
            "time, by additionally requiring certain structural properties for the union of\n",
            "dependencies cast by the whole electorate, we obtain optimal efficient\n",
            "algorithms for well-motivated special cases. Overall, our work provides a\n",
            "better understanding on the complexity implications introduced by conditional\n",
            "voting.\n",
            "\n",
            "**Paper Id :2005.10716 \n",
            "Title :Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for\n",
            "  Automatic Dialog Evaluation\n",
            "  Open Domain dialog system evaluation is one of the most important challenges\n",
            "in dialog research. Existing automatic evaluation metrics, such as BLEU are\n",
            "mostly reference-based. They calculate the difference between the generated\n",
            "response and a limited number of available references. Likert-score based\n",
            "self-reported user rating is widely adopted by social conversational systems,\n",
            "such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\n",
            "from bias and variance among different users. To alleviate this problem, we\n",
            "formulate dialog evaluation as a comparison task. We also propose an automatic\n",
            "evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\n",
            "automatically cleans self-reported user ratings as it trains on them.\n",
            "Specifically, we first use a self-supervised method to learn better dialog\n",
            "feature representation, and then use KNN and Shapley to remove confusing\n",
            "samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\n",
            "comparison task.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2202.08372 \n",
            "Title :Fuzzy Pooling\n",
            "  Convolutional Neural Networks (CNNs) are artificial learning systems\n",
            "typically based on two operations: convolution, which implements feature\n",
            "extraction through filtering, and pooling, which implements dimensionality\n",
            "reduction. The impact of pooling in the classification performance of the CNNs\n",
            "has been highlighted in several previous works, and a variety of alternative\n",
            "pooling operators have been proposed. However, only a few of them tackle with\n",
            "the uncertainty that is naturally propagated from the input layer to the\n",
            "feature maps of the hidden layers through convolutions. In this paper we\n",
            "present a novel pooling operation based on (type-1) fuzzy sets to cope with the\n",
            "local imprecision of the feature maps, and we investigate its performance in\n",
            "the context of image classification. Fuzzy pooling is performed by\n",
            "fuzzification, aggregation and defuzzification of feature map neighborhoods. It\n",
            "is used for the construction of a fuzzy pooling layer that can be applied as a\n",
            "drop-in replacement of the current, crisp, pooling layers of CNN architectures.\n",
            "Several experiments using publicly available datasets show that the proposed\n",
            "approach can enhance the classification performance of a CNN. A comparative\n",
            "evaluation shows that it outperforms state-of-the-art pooling approaches.\n",
            "\n",
            "**Paper Id :1709.05360 \n",
            "Title :Embedding Deep Networks into Visual Explanations\n",
            "  In this paper, we propose a novel Explanation Neural Network (XNN) to explain\n",
            "the predictions made by a deep network. The XNN works by learning a nonlinear\n",
            "embedding of a high-dimensional activation vector of a deep network layer into\n",
            "a low-dimensional explanation space while retaining faithfulness i.e., the\n",
            "original deep learning predictions can be constructed from the few concepts\n",
            "extracted by our explanation network. We then visualize such concepts for human\n",
            "to learn about the high-level concepts that the deep network is using to make\n",
            "decisions. We propose an algorithm called Sparse Reconstruction Autoencoder\n",
            "(SRAE) for learning the embedding to the explanation space. SRAE aims to\n",
            "reconstruct part of the original feature space while retaining faithfulness. A\n",
            "pull-away term is applied to SRAE to make the bases of the explanation space\n",
            "more orthogonal to each other. A visualization system is then introduced for\n",
            "human understanding of the features in the explanation space. The proposed\n",
            "method is applied to explain CNN models in image classification tasks. We\n",
            "conducted a human study, which shows that the proposed approach outperforms\n",
            "single saliency map baselines, and improves human performance on a difficult\n",
            "classification tasks. Also, several novel metrics are introduced to evaluate\n",
            "the performance of explanations quantitatively without human involvement.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2203.00397 \n",
            "Title :A Theory of Abstraction in Reinforcement Learning\n",
            "  Reinforcement learning defines the problem facing agents that learn to make\n",
            "good decisions through action and observation alone. To be effective problem\n",
            "solvers, such agents must efficiently explore vast worlds, assign credit from\n",
            "delayed feedback, and generalize to new experiences, all while making use of\n",
            "limited data, computational resources, and perceptual bandwidth. Abstraction is\n",
            "essential to all of these endeavors. Through abstraction, agents can form\n",
            "concise models of their environment that support the many practices required of\n",
            "a rational, adaptive decision maker. In this dissertation, I present a theory\n",
            "of abstraction in reinforcement learning. I first offer three desiderata for\n",
            "functions that carry out the process of abstraction: they should 1) preserve\n",
            "representation of near-optimal behavior, 2) be learned and constructed\n",
            "efficiently, and 3) lower planning or learning time. I then present a suite of\n",
            "new algorithms and analysis that clarify how agents can learn to abstract\n",
            "according to these desiderata. Collectively, these results provide a partial\n",
            "path toward the discovery and use of abstraction that minimizes the complexity\n",
            "of effective reinforcement learning.\n",
            "\n",
            "**Paper Id :2005.11895 \n",
            "Title :Reinforcement Learning with Iterative Reasoning for Merging in Dense\n",
            "  Traffic\n",
            "  Maneuvering in dense traffic is a challenging task for autonomous vehicles\n",
            "because it requires reasoning about the stochastic behaviors of many other\n",
            "participants. In addition, the agent must achieve the maneuver within a limited\n",
            "time and distance. In this work, we propose a combination of reinforcement\n",
            "learning and game theory to learn merging behaviors. We design a training\n",
            "curriculum for a reinforcement learning agent using the concept of level-$k$\n",
            "behavior. This approach exposes the agent to a broad variety of behaviors\n",
            "during training, which promotes learning policies that are robust to model\n",
            "discrepancies. We show that our approach learns more efficient policies than\n",
            "traditional training methods.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2203.02192 \n",
            "Title :Optimal Keywords Grouping in Sponsored Search Advertising under\n",
            "  Uncertain Environments\n",
            "  In sponsored search advertising, advertisers need to make a series of keyword\n",
            "decisions. Among them, how to group these keywords to form several adgroups\n",
            "within a campaign is a challenging task, due to the highly uncertain\n",
            "environment of search advertising. This paper proposes a stochastic programming\n",
            "model for keywords grouping, taking click-through rate and conversion rate as\n",
            "random variables, with consideration of budget constraints and advertisers'\n",
            "risk-tolerance. A branch-and-bound algorithm is developed to solve our model.\n",
            "Furthermore, we conduct computational experiments to evaluate the effectiveness\n",
            "of our model and solution, with two real-world datasets collected from reports\n",
            "and logs of search advertising campaigns. Experimental results illustrated that\n",
            "our keywords grouping approach outperforms five baselines, and it can\n",
            "approximately approach the optimum in a steady way. This research generates\n",
            "several interesting findings that illuminate critical managerial insights for\n",
            "advertisers in sponsored search advertising. First, keywords grouping does\n",
            "matter for advertisers, especially in the situation with a large number of\n",
            "keywords. Second, in keyword grouping decisions, the marginal profit does not\n",
            "necessarily show the marginal diminishing phenomenon as the budget increases.\n",
            "Such that, it's a worthy try for advertisers to increase their budget in\n",
            "keywords grouping decisions, in order to obtain additional profit. Third, the\n",
            "optimal keywords grouping solution is a result of multifaceted trade-off among\n",
            "various advertising factors. In particular, assigning more keywords into\n",
            "adgroups or having more budget won't certainly lead to higher profits. This\n",
            "suggests a warning for advertisers that it's not wise to take the number of\n",
            "keywords as the criterion for keywords grouping decisions.\n",
            "\n",
            "**Paper Id :2010.04124 \n",
            "Title :Extending the Hint Factory for the assistance dilemma: A novel,\n",
            "  data-driven HelpNeed Predictor for proactive problem-solving help\n",
            "  Determining when and whether to provide personalized support is a well-known\n",
            "challenge called the assistance dilemma. A core problem in solving the\n",
            "assistance dilemma is the need to discover when students are unproductive so\n",
            "that the tutor can intervene. Such a task is particularly challenging for\n",
            "open-ended domains, even those that are well-structured with defined principles\n",
            "and goals. In this paper, we present a set of data-driven methods to classify,\n",
            "predict, and prevent unproductive problem-solving steps in the well-structured\n",
            "open-ended domain of logic. This approach leverages and extends the Hint\n",
            "Factory, a set of methods that leverages prior student solution attempts to\n",
            "build data-driven intelligent tutors. We present a HelpNeed classification,\n",
            "that uses prior student data to determine when students are likely to be\n",
            "unproductive and need help learning optimal problem-solving strategies. We\n",
            "present a controlled study to determine the impact of an Adaptive pedagogical\n",
            "policy that provides proactive hints at the start of each step based on the\n",
            "outcomes of our HelpNeed predictor: productive vs. unproductive. Our results\n",
            "show that the students in the Adaptive condition exhibited better training\n",
            "behaviors, with lower help avoidance, and higher help appropriateness (a higher\n",
            "chance of receiving help when it was likely to be needed), as measured using\n",
            "the HelpNeed classifier, when compared to the Control. Furthermore, the results\n",
            "show that the students who received Adaptive hints based on HelpNeed\n",
            "predictions during training significantly outperform their Control peers on the\n",
            "posttest, with the former producing shorter, more optimal solutions in less\n",
            "time. We conclude with suggestions on how these HelpNeed methods could be\n",
            "applied in other well-structured open-ended domains.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2204.05944 \n",
            "Title :Uncertainty-Aware Search Framework for Multi-Objective Bayesian\n",
            "  Optimization\n",
            "  We consider the problem of multi-objective (MO) blackbox optimization using\n",
            "expensive function evaluations, where the goal is to approximate the true\n",
            "Pareto set of solutions while minimizing the number of function evaluations.\n",
            "For example, in hardware design optimization, we need to find the designs that\n",
            "trade-off performance, energy, and area overhead using expensive simulations.\n",
            "We propose a novel uncertainty-aware search framework referred to as USeMO to\n",
            "efficiently select the sequence of inputs for evaluation to solve this problem.\n",
            "The selection method of USeMO consists of solving a cheap MO optimization\n",
            "problem via surrogate models of the true functions to identify the most\n",
            "promising candidates and picking the best candidate based on a measure of\n",
            "uncertainty. We also provide theoretical analysis to characterize the efficacy\n",
            "of our approach. Our experiments on several synthetic and six diverse\n",
            "real-world benchmark problems show that USeMO consistently outperforms the\n",
            "state-of-the-art algorithms.\n",
            "\n",
            "**Paper Id :2009.01721 \n",
            "Title :Max-value Entropy Search for Multi-Objective Bayesian Optimization with\n",
            "  Constraints\n",
            "  We consider the problem of constrained multi-objective blackbox optimization\n",
            "using expensive function evaluations, where the goal is to approximate the true\n",
            "Pareto set of solutions satisfying a set of constraints while minimizing the\n",
            "number of function evaluations. For example, in aviation power system design\n",
            "applications, we need to find the designs that trade-off total energy and the\n",
            "mass while satisfying specific thresholds for motor temperature and voltage of\n",
            "cells. This optimization requires performing expensive computational\n",
            "simulations to evaluate designs. In this paper, we propose a new approach\n",
            "referred as {\\em Max-value Entropy Search for Multi-objective Optimization with\n",
            "Constraints (MESMOC)} to solve this problem. MESMOC employs an output-space\n",
            "entropy based acquisition function to efficiently select the sequence of inputs\n",
            "for evaluation to uncover high-quality pareto-set solutions while satisfying\n",
            "constraints.\n",
            "  We apply MESMOC to two real-world engineering design applications to\n",
            "demonstrate its effectiveness over state-of-the-art algorithms.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2205.00210 \n",
            "Title :Software Testing for Machine Learning\n",
            "  Machine learning has become prevalent across a wide variety of applications.\n",
            "Unfortunately, machine learning has also shown to be susceptible to deception,\n",
            "leading to errors, and even fatal failures. This circumstance calls into\n",
            "question the widespread use of machine learning, especially in safety-critical\n",
            "applications, unless we are able to assure its correctness and trustworthiness\n",
            "properties. Software verification and testing are established technique for\n",
            "assuring such properties, for example by detecting errors. However, software\n",
            "testing challenges for machine learning are vast and profuse - yet critical to\n",
            "address. This summary talk discusses the current state-of-the-art of software\n",
            "testing for machine learning. More specifically, it discusses six key challenge\n",
            "areas for software testing of machine learning systems, examines current\n",
            "approaches to these challenges and highlights their limitations. The paper\n",
            "provides a research agenda with elaborated directions for making progress\n",
            "toward advancing the state-of-the-art on testing of machine learning.\n",
            "\n",
            "**Paper Id :2002.06306 \n",
            "Title :Jelly Bean World: A Testbed for Never-Ending Learning\n",
            "  Machine learning has shown growing success in recent years. However, current\n",
            "machine learning systems are highly specialized, trained for particular\n",
            "problems or domains, and typically on a single narrow dataset. Human learning,\n",
            "on the other hand, is highly general and adaptable. Never-ending learning is a\n",
            "machine learning paradigm that aims to bridge this gap, with the goal of\n",
            "encouraging researchers to design machine learning systems that can learn to\n",
            "perform a wider variety of inter-related tasks in more complex environments. To\n",
            "date, there is no environment or testbed to facilitate the development and\n",
            "evaluation of never-ending learning systems. To this end, we propose the Jelly\n",
            "Bean World testbed. The Jelly Bean World allows experimentation over\n",
            "two-dimensional grid worlds which are filled with items and in which agents can\n",
            "navigate. This testbed provides environments that are sufficiently complex and\n",
            "where more generally intelligent algorithms ought to perform better than\n",
            "current state-of-the-art reinforcement learning approaches. It does so by\n",
            "producing non-stationary environments and facilitating experimentation with\n",
            "multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope\n",
            "that this new freely-available software will prompt new research and interest\n",
            "in the development and evaluation of never-ending learning systems and more\n",
            "broadly, general intelligence systems.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2206.01889 \n",
            "Title :Initial Study into Application of Feature Density and\n",
            "  Linguistically-backed Embedding to Improve Machine Learning-based\n",
            "  Cyberbullying Detection\n",
            "  In this research, we study the change in the performance of machine learning\n",
            "(ML) classifiers when various linguistic preprocessing methods of a dataset\n",
            "were used, with the specific focus on linguistically-backed embeddings in\n",
            "Convolutional Neural Networks (CNN). Moreover, we study the concept of Feature\n",
            "Density and confirm its potential to comparatively predict the performance of\n",
            "ML classifiers, including CNN. The research was conducted on a Formspring\n",
            "dataset provided in a Kaggle competition on automatic cyberbullying detection.\n",
            "The dataset was re-annotated by objective experts (psychologists), as the\n",
            "importance of professional annotation in cyberbullying research has been\n",
            "indicated multiple times. The study confirmed the effectiveness of Neural\n",
            "Networks in cyberbullying detection and the correlation between classifier\n",
            "performance and Feature Density while also proposing a new approach of training\n",
            "various linguistically-backed embeddings for Convolutional Neural Networks.\n",
            "\n",
            "**Paper Id :2007.02040 \n",
            "Title :Discount Factor as a Regularizer in Reinforcement Learning\n",
            "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\n",
            "planning horizon, which is typically modeled by a discount factor. It is known\n",
            "that applying RL algorithms with a lower discount factor can act as a\n",
            "regularizer, improving performance in the limited data regime. Yet the exact\n",
            "nature of this regularizer has not been investigated. In this work, we fill in\n",
            "this gap. For several Temporal-Difference (TD) learning methods, we show an\n",
            "explicit equivalence between using a reduced discount factor and adding an\n",
            "explicit regularization term to the algorithm's loss. Motivated by the\n",
            "equivalence, we empirically study this technique compared to standard $L_2$\n",
            "regularization by extensive experiments in discrete and continuous domains,\n",
            "using tabular and functional representations. Our experiments suggest the\n",
            "regularization effectiveness is strongly related to properties of the available\n",
            "data, such as size, distribution, and mixing rate.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2206.02419 \n",
            "Title :Towards Responsible AI for Financial Transactions\n",
            "  The application of AI in finance is increasingly dependent on the principles\n",
            "of responsible AI. These principles - explainability, fairness, privacy,\n",
            "accountability, transparency and soundness form the basis for trust in future\n",
            "AI systems. In this study, we address the first principle by providing an\n",
            "explanation for a deep neural network that is trained on a mixture of\n",
            "numerical, categorical and textual inputs for financial transaction\n",
            "classification. The explanation is achieved through (1) a feature importance\n",
            "analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach\n",
            "of text clustering and decision tree classifiers. We then test the robustness\n",
            "of the model by exposing it to a targeted evasion attack, leveraging the\n",
            "knowledge we gained about the model through the extracted explanation.\n",
            "\n",
            "**Paper Id :2010.01479 \n",
            "Title :Explanation Ontology: A Model of Explanations for User-Centered AI\n",
            "  Explainability has been a goal for Artificial Intelligence (AI) systems since\n",
            "their conception, with the need for explainability growing as more complex AI\n",
            "models are increasingly used in critical, high-stakes settings such as\n",
            "healthcare. Explanations have often added to an AI system in a non-principled,\n",
            "post-hoc manner. With greater adoption of these systems and emphasis on\n",
            "user-centric explainability, there is a need for a structured representation\n",
            "that treats explainability as a primary consideration, mapping end user needs\n",
            "to specific explanation types and the system's AI capabilities. We design an\n",
            "explanation ontology to model both the role of explanations, accounting for the\n",
            "system and user attributes in the process, and the range of different\n",
            "literature-derived explanation types. We indicate how the ontology can support\n",
            "user requirements for explanations in the domain of healthcare. We evaluate our\n",
            "ontology with a set of competency questions geared towards a system designer\n",
            "who might use our ontology to decide which explanation types to include, given\n",
            "a combination of users' needs and a system's capabilities, both in system\n",
            "design settings and in real-time operations. Through the use of this ontology,\n",
            "system designers will be able to make informed choices on which explanations AI\n",
            "systems can and should provide.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2209.03805 \n",
            "Title :FAT Forensics: A Python Toolbox for Implementing and Deploying Fairness,\n",
            "  Accountability and Transparency Algorithms in Predictive Systems\n",
            "  Predictive systems, in particular machine learning algorithms, can take\n",
            "important, and sometimes legally binding, decisions about our everyday life. In\n",
            "most cases, however, these systems and decisions are neither regulated nor\n",
            "certified. Given the potential harm that these algorithms can cause, their\n",
            "qualities such as fairness, accountability and transparency (FAT) are of\n",
            "paramount importance. To ensure high-quality, fair, transparent and reliable\n",
            "predictive systems, we developed an open source Python package called FAT\n",
            "Forensics. It can inspect important fairness, accountability and transparency\n",
            "aspects of predictive algorithms to automatically and objectively report them\n",
            "back to engineers and users of such systems. Our toolbox can evaluate all\n",
            "elements of a predictive pipeline: data (and their features), models and\n",
            "predictions. Published under the BSD 3-Clause open source licence, FAT\n",
            "Forensics is opened up for personal and commercial usage.\n",
            "\n",
            "**Paper Id :2005.04949 \n",
            "Title :Designing for Human Rights in AI\n",
            "  In the age of big data, companies and governments are increasingly using\n",
            "algorithms to inform hiring decisions, employee management, policing, credit\n",
            "scoring, insurance pricing, and many more aspects of our lives. AI systems can\n",
            "help us make evidence-driven, efficient decisions, but can also confront us\n",
            "with unjustified, discriminatory decisions wrongly assumed to be accurate\n",
            "because they are made automatically and quantitatively. It is becoming evident\n",
            "that these technological developments are consequential to people's fundamental\n",
            "human rights. Despite increasing attention to these urgent challenges in recent\n",
            "years, technical solutions to these complex socio-ethical problems are often\n",
            "developed without empirical study of societal context and the critical input of\n",
            "societal stakeholders who are impacted by the technology. On the other hand,\n",
            "calls for more ethically- and socially-aware AI often fail to provide answers\n",
            "for how to proceed beyond stressing the importance of transparency,\n",
            "explainability, and fairness. Bridging these socio-technical gaps and the deep\n",
            "divide between abstract value language and design requirements is essential to\n",
            "facilitate nuanced, context-dependent design choices that will support moral\n",
            "and social values. In this paper, we bridge this divide through the framework\n",
            "of Design for Values, drawing on methodologies of Value Sensitive Design and\n",
            "Participatory Design to present a roadmap for proactively engaging societal\n",
            "stakeholders to translate fundamental human rights into context-dependent\n",
            "design requirements through a structured, inclusive, and transparent process.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2209.13965 \n",
            "Title :Anomaly detection optimization using big data and deep learning to\n",
            "  reduce false-positive\n",
            "  Anomaly-based Intrusion Detection System (IDS) has been a hot research topic\n",
            "because of its ability to detect new threats rather than only memorized\n",
            "signatures threats of signature-based IDS. Especially after the availability of\n",
            "advanced technologies that increase the number of hacking tools and increase\n",
            "the risk impact of an attack. The problem of any anomaly-based model is its\n",
            "high false-positive rate. The high false-positive rate is the reason why\n",
            "anomaly IDS is not commonly applied in practice. Because anomaly-based models\n",
            "classify an unseen pattern as a threat where it may be normal but not included\n",
            "in the training dataset. This type of problem is called overfitting where the\n",
            "model is not able to generalize. Optimizing Anomaly-based models by having a\n",
            "big training dataset that includes all possible normal cases may be an optimal\n",
            "solution but could not be applied in practice. Although we can increase the\n",
            "number of training samples to include much more normal cases, still we need a\n",
            "model that has more ability to generalize. In this research paper, we propose\n",
            "applying deep model instead of traditional models because it has more ability\n",
            "to generalize. Thus, we will obtain less false-positive by using big data and\n",
            "deep model. We made a comparison between machine learning and deep learning\n",
            "algorithms in the optimization of anomaly-based IDS by decreasing the\n",
            "false-positive rate. We did an experiment on the NSL-KDD benchmark and compared\n",
            "our results with one of the best used classifiers in traditional learning in\n",
            "IDS optimization. The experiment shows 10% lower false-positive by using deep\n",
            "learning instead of traditional learning.\n",
            "\n",
            "**Paper Id :2003.04984 \n",
            "Title :Securing of Unmanned Aerial Systems (UAS) against security threats using\n",
            "  human immune system\n",
            "  UASs form a large part of the fighting ability of the advanced military\n",
            "forces. In particular, these systems that carry confidential information are\n",
            "subject to security attacks. Accordingly, an Intrusion Detection System (IDS)\n",
            "has been proposed in the proposed design to protect against the security\n",
            "problems using the human immune system (HIS). The IDSs are used to detect and\n",
            "respond to attempts to compromise the target system. Since the UASs operate in\n",
            "the real world, the testing and validation of these systems with a variety of\n",
            "sensors is confronted with problems. This design is inspired by HIS. In the\n",
            "mapping, insecure signals are equivalent to an antigen that are detected by\n",
            "antibody-based training patterns and removed from the operation cycle. Among\n",
            "the main uses of the proposed design are the quick detection of intrusive\n",
            "signals and quarantining their activity. Moreover, SUAS-HIS method is evaluated\n",
            "here via extensive simulations carried out in NS-3 environment. The simulation\n",
            "results indicate that the UAS network performance metrics are improved in terms\n",
            "of false positive rate, false negative rate, detection rate, and packet\n",
            "delivery rate.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2211.08430 \n",
            "Title :Power-law Scaling to Assist with Key Challenges in Artificial\n",
            "  Intelligence\n",
            "  Power-law scaling, a central concept in critical phenomena, is found to be\n",
            "useful in deep learning, where optimized test errors on handwritten digit\n",
            "examples converge as a power-law to zero with database size. For rapid decision\n",
            "making with one training epoch, each example is presented only once to the\n",
            "trained network, the power-law exponent increased with the number of hidden\n",
            "layers. For the largest dataset, the obtained test error was estimated to be in\n",
            "the proximity of state-of-the-art algorithms for large epoch numbers. Power-law\n",
            "scaling assists with key challenges found in current artificial intelligence\n",
            "applications and facilitates an a priori dataset size estimation to achieve a\n",
            "desired test accuracy. It establishes a benchmark for measuring training\n",
            "complexity and a quantitative hierarchy of machine learning tasks and\n",
            "algorithms.\n",
            "\n",
            "**Paper Id :2007.02040 \n",
            "Title :Discount Factor as a Regularizer in Reinforcement Learning\n",
            "  Specifying a Reinforcement Learning (RL) task involves choosing a suitable\n",
            "planning horizon, which is typically modeled by a discount factor. It is known\n",
            "that applying RL algorithms with a lower discount factor can act as a\n",
            "regularizer, improving performance in the limited data regime. Yet the exact\n",
            "nature of this regularizer has not been investigated. In this work, we fill in\n",
            "this gap. For several Temporal-Difference (TD) learning methods, we show an\n",
            "explicit equivalence between using a reduced discount factor and adding an\n",
            "explicit regularization term to the algorithm's loss. Motivated by the\n",
            "equivalence, we empirically study this technique compared to standard $L_2$\n",
            "regularization by extensive experiments in discrete and continuous domains,\n",
            "using tabular and functional representations. Our experiments suggest the\n",
            "regularization effectiveness is strongly related to properties of the available\n",
            "data, such as size, distribution, and mixing rate.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2303.01403 \n",
            "Title :iART: Learning from Demonstration for Assisted Robotic Therapy Using\n",
            "  LSTM\n",
            "  In this paper, we present an intelligent Assistant for Robotic Therapy\n",
            "(iART), that provides robotic assistance during 3D trajectory tracking tasks.\n",
            "We propose a novel LSTM-based robot learning from demonstration (LfD) paradigm\n",
            "to mimic a therapist's assistance behavior. iART presents a trajectory agnostic\n",
            "LfD routine that can generalize learned behavior from a single trajectory to\n",
            "any 3D shape. Once the therapist's behavior has been learned, iART enables the\n",
            "patient to modify this behavior as per their preference. The system requires\n",
            "only a single demonstration of 2 minutes and exhibits a mean accuracy of 91.41%\n",
            "in predicting, and hence mimicking a therapist's assistance behavior. The\n",
            "system delivers stable assistance in realtime and successfully reproduces\n",
            "different types of assistance behaviors.\n",
            "\n",
            "**Paper Id :1909.10707 \n",
            "Title :Invariant Transform Experience Replay: Data Augmentation for Deep\n",
            "  Reinforcement Learning\n",
            "  Deep Reinforcement Learning (RL) is a promising approach for adaptive robot\n",
            "control, but its current application to robotics is currently hindered by high\n",
            "sample requirements. To alleviate this issue, we propose to exploit the\n",
            "symmetries present in robotic tasks. Intuitively, symmetries from observed\n",
            "trajectories define transformations that leave the space of feasible RL\n",
            "trajectories invariant and can be used to generate new feasible trajectories,\n",
            "which could be used for training. Based on this data augmentation idea, we\n",
            "formulate a general framework, called Invariant Transform Experience Replay\n",
            "that we present with two techniques: (i) Kaleidoscope Experience Replay\n",
            "exploits reflectional symmetries and (ii) Goal-augmented Experience Replay\n",
            "which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI\n",
            "Gym, our experimental results show significant increases in learning rates and\n",
            "success rates. Particularly, we attain a 13, 3, and 5 times speedup in the\n",
            "pushing, sliding, and pick-and-place tasks respectively in the multi-goal\n",
            "setting. Performance gains are also observed in similar tasks with obstacles\n",
            "and we successfully deployed a trained policy on a real Baxter robot. Our work\n",
            "demonstrates that invariant transformations on RL trajectories are a promising\n",
            "methodology to speed up learning in deep RL.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2305.02326 \n",
            "Title :Cybernetic Environment: A Historical Reflection on System, Design, and\n",
            "  Machine Intelligence\n",
            "  Taking on a historical lens, this paper traces the development of cybernetics\n",
            "and systems thinking back to the 1950s, when a group of interdisciplinary\n",
            "scholars converged to create a new theoretical model based on machines and\n",
            "systems for understanding matters of meaning, information, consciousness, and\n",
            "life. By presenting a genealogy of research in the landscape architecture\n",
            "discipline, the paper argues that landscape architects have been an important\n",
            "part of the development of cybernetics by materializing systems based on\n",
            "cybernetic principles in the environment through ecologically based landscape\n",
            "design. The landscape discipline has developed a design framework that provides\n",
            "transformative insights into understanding machine intelligence. The paper\n",
            "calls for a new paradigm of environmental engagement to understand matters of\n",
            "design and machine intelligence.\n",
            "\n",
            "**Paper Id :2005.02863 \n",
            "Title :The computerization of archaeology: survey on AI techniques\n",
            "  This paper analyses the application of artificial intelligence techniques to\n",
            "various areas of archaeology and more specifically: a) The use of software\n",
            "tools as a creative stimulus for the organization of exhibitions; the use of\n",
            "humanoid robots and holographic displays as guides that interact and involve\n",
            "museum visitors; b) The analysis of methods for the classification of fragments\n",
            "found in archaeological excavations and for the reconstruction of ceramics,\n",
            "with the recomposition of the parts of text missing from historical documents\n",
            "and epigraphs; c) The cataloguing and study of human remains to understand the\n",
            "social and historical context of belonging with the demonstration of the\n",
            "effectiveness of the AI techniques used; d) The detection of particularly\n",
            "difficult terrestrial archaeological sites with the analysis of the\n",
            "architectures of the Artificial Neural Networks most suitable for solving the\n",
            "problems presented by the site; the design of a study for the exploration of\n",
            "marine archaeological sites, located at depths that cannot be reached by man,\n",
            "through the construction of a freely explorable 3D version.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2305.09782 \n",
            "Title :Analysis of Visual Question Answering Algorithms with attention model\n",
            "  Visual question answering (VQA) usesimage processing algorithms to process\n",
            "the image and natural language processing methods to understand and answer the\n",
            "question. VQA is helpful to a visually impaired person, can be used for the\n",
            "security surveillance system and online chatbots that learn from the web. It\n",
            "uses NLP methods to learn the semantic of the question and to derive the\n",
            "textual features. Computer vision techniques are used for generating image\n",
            "representation in such a way that they can identify the objects about which\n",
            "question is asked. The Attention model tries to mimic the human behavior of\n",
            "giving attention to a different region of an image according to our\n",
            "understanding of its context. This paper critically examines and reviews\n",
            "methods of VQA algorithm such as generation of semantics of text,\n",
            "identification of objects and answer classification techniques that use the\n",
            "co-attention approach.\n",
            "\n",
            "**Paper Id :2003.00431 \n",
            "Title :A Study on Multimodal and Interactive Explanations for Visual Question\n",
            "  Answering\n",
            "  Explainability and interpretability of AI models is an essential factor\n",
            "affecting the safety of AI. While various explainable AI (XAI) approaches aim\n",
            "at mitigating the lack of transparency in deep networks, the evidence of the\n",
            "effectiveness of these approaches in improving usability, trust, and\n",
            "understanding of AI systems are still missing. We evaluate multimodal\n",
            "explanations in the setting of a Visual Question Answering (VQA) task, by\n",
            "asking users to predict the response accuracy of a VQA agent with and without\n",
            "explanations. We use between-subjects and within-subjects experiments to probe\n",
            "explanation effectiveness in terms of improving user prediction accuracy,\n",
            "confidence, and reliance, among other factors. The results indicate that the\n",
            "explanations help improve human prediction accuracy, especially in trials when\n",
            "the VQA system's answer is inaccurate. Furthermore, we introduce active\n",
            "attention, a novel method for evaluating causal attentional effects through\n",
            "intervention by editing attention maps. User explanation ratings are strongly\n",
            "correlated with human prediction accuracy and suggest the efficacy of these\n",
            "explanations in human-machine AI collaboration tasks.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2306.04887 \n",
            "Title :Big-data-driven and AI-based framework to enable personalization in\n",
            "  wireless networks\n",
            "  Current communication networks use design methodologies that prevent the\n",
            "realization of maximum network efficiency. In the first place, while users'\n",
            "perception of satisfactory service diverges widely, current networks are\n",
            "designed to be a \"universal fit,\" where they are generally over-engineered to\n",
            "deliver services appealing to all types of users. Also, current networks lack\n",
            "user-level data cognitive intelligence that would enable fast personalized\n",
            "network decisions and actions through automation. Thus, in this article, we\n",
            "propose the utilization of AI, big data analytics, and real-time non-intrusive\n",
            "user feedback in order to enable the personalization of wireless networks.\n",
            "Based on each user's actual QoS requirements and context, a multi-objective\n",
            "formulation enables the network to micro-manage and optimize the provided QoS\n",
            "and user satisfaction levels simultaneously. Moreover, in order to enable user\n",
            "feedback tracking and measurement, we propose a user satisfaction model based\n",
            "on the zone of tolerance concept. Furthermore, we propose a big-data-driven and\n",
            "AI-based personalization framework to integrate personalization into wireless\n",
            "networks. Finally, we implement a personalized network prototype to demonstrate\n",
            "the proposed personalization concept and its potential benefits through a case\n",
            "study. The case study shows how personalization can be realized to enable the\n",
            "efficient optimization of network resources such that certain requirement\n",
            "levels of user satisfaction and revenue in the form of saved resources are\n",
            "achieved.\n",
            "\n",
            "**Paper Id :2307.03867 \n",
            "Title :Personalized Resource Allocation in Wireless Networks: An AI-Enabled and\n",
            "  Big Data-Driven Multi-Objective Optimization\n",
            "  The design and optimization of wireless networks have mostly been based on\n",
            "strong mathematical and theoretical modeling. Nonetheless, as novel\n",
            "applications emerge in the era of 5G and beyond, unprecedented levels of\n",
            "complexity will be encountered in the design and optimization of the network.\n",
            "As a result, the use of Artificial Intelligence (AI) is envisioned for wireless\n",
            "network design and optimization due to the flexibility and adaptability it\n",
            "offers in solving extremely complex problems in real-time. One of the main\n",
            "future applications of AI is enabling user-level personalization for numerous\n",
            "use cases. AI will revolutionize the way we interact with computers in which\n",
            "computers will be able to sense commands and emotions from humans in a\n",
            "non-intrusive manner, making the entire process transparent to users. By\n",
            "leveraging this capability, and accelerated by the advances in computing\n",
            "technologies, wireless networks can be redesigned to enable the personalization\n",
            "of network services to the user level in real-time. While current wireless\n",
            "networks are being optimized to achieve a predefined set of quality\n",
            "requirements, the personalization technology advocated in this article is\n",
            "supported by an intelligent big data-driven layer designed to micro-manage the\n",
            "scarce network resources. This layer provides the intelligence required to\n",
            "decide the necessary service quality that achieves the target satisfaction\n",
            "level for each user. Due to its dynamic and flexible design, personalized\n",
            "networks are expected to achieve unprecedented improvements in optimizing two\n",
            "contradicting objectives in wireless networks: saving resources and improving\n",
            "user satisfaction levels.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2306.14551 \n",
            "Title :Creating user stereotypes for persona development from qualitative data\n",
            "  through semi-automatic subspace clustering\n",
            "  Personas are models of users that incorporate motivations, wishes, and\n",
            "objectives; These models are employed in user-centred design to help design\n",
            "better user experiences and have recently been employed in adaptive systems to\n",
            "help tailor the personalized user experience. Designing with personas involves\n",
            "the production of descriptions of fictitious users, which are often based on\n",
            "data from real users. The majority of data-driven persona development performed\n",
            "today is based on qualitative data from a limited set of interviewees and\n",
            "transformed into personas using labour-intensive manual techniques. In this\n",
            "study, we propose a method that employs the modelling of user stereotypes to\n",
            "automate part of the persona creation process and addresses the drawbacks of\n",
            "the existing semi-automated methods for persona development. The description of\n",
            "the method is accompanied by an empirical comparison with a manual technique\n",
            "and a semi-automated alternative (multiple correspondence analysis). The\n",
            "results of the comparison show that manual techniques differ between human\n",
            "persona designers leading to different results. The proposed algorithm provides\n",
            "similar results based on parameter input, but was more rigorous and will find\n",
            "optimal clusters, while lowering the labour associated with finding the\n",
            "clusters in the dataset. The output of the method also represents the largest\n",
            "variances in the dataset identified by the multiple correspondence analysis.\n",
            "\n",
            "**Paper Id :2008.02311 \n",
            "Title :Conceptual Metaphors Impact Perceptions of Human-AI Collaboration\n",
            "  With the emergence of conversational artificial intelligence (AI) agents, it\n",
            "is important to understand the mechanisms that influence users' experiences of\n",
            "these agents. We study a common tool in the designer's toolkit: conceptual\n",
            "metaphors. Metaphors can present an agent as akin to a wry teenager, a toddler,\n",
            "or an experienced butler. How might a choice of metaphor influence our\n",
            "experience of the AI agent? Sampling metaphors along the dimensions of warmth\n",
            "and competence---defined by psychological theories as the primary axes of\n",
            "variation for human social perception---we perform a study (N=260) where we\n",
            "manipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational\n",
            "agent. Following the experience, participants are surveyed about their\n",
            "intention to use the agent, their desire to cooperate with the agent, and the\n",
            "agent's usability. Contrary to the current tendency of designers to use high\n",
            "competence metaphors to describe AI products, we find that metaphors that\n",
            "signal low competence lead to better evaluations of the agent than metaphors\n",
            "that signal high competence. This effect persists despite both high and low\n",
            "competence agents featuring human-level performance and the wizards being blind\n",
            "to condition. A second study confirms that intention to adopt decreases rapidly\n",
            "as competence projected by the metaphor increases. In a third study, we assess\n",
            "effects of metaphor choices on potential users' desire to try out the system\n",
            "and find that users are drawn to systems that project higher competence and\n",
            "warmth. These results suggest that projecting competence may help attract new\n",
            "users, but those users may discard the agent unless it can quickly correct with\n",
            "a lower competence metaphor. We close with a retrospective analysis that finds\n",
            "similar patterns between metaphors and user attitudes towards past\n",
            "conversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2307.03867 \n",
            "Title :Personalized Resource Allocation in Wireless Networks: An AI-Enabled and\n",
            "  Big Data-Driven Multi-Objective Optimization\n",
            "  The design and optimization of wireless networks have mostly been based on\n",
            "strong mathematical and theoretical modeling. Nonetheless, as novel\n",
            "applications emerge in the era of 5G and beyond, unprecedented levels of\n",
            "complexity will be encountered in the design and optimization of the network.\n",
            "As a result, the use of Artificial Intelligence (AI) is envisioned for wireless\n",
            "network design and optimization due to the flexibility and adaptability it\n",
            "offers in solving extremely complex problems in real-time. One of the main\n",
            "future applications of AI is enabling user-level personalization for numerous\n",
            "use cases. AI will revolutionize the way we interact with computers in which\n",
            "computers will be able to sense commands and emotions from humans in a\n",
            "non-intrusive manner, making the entire process transparent to users. By\n",
            "leveraging this capability, and accelerated by the advances in computing\n",
            "technologies, wireless networks can be redesigned to enable the personalization\n",
            "of network services to the user level in real-time. While current wireless\n",
            "networks are being optimized to achieve a predefined set of quality\n",
            "requirements, the personalization technology advocated in this article is\n",
            "supported by an intelligent big data-driven layer designed to micro-manage the\n",
            "scarce network resources. This layer provides the intelligence required to\n",
            "decide the necessary service quality that achieves the target satisfaction\n",
            "level for each user. Due to its dynamic and flexible design, personalized\n",
            "networks are expected to achieve unprecedented improvements in optimizing two\n",
            "contradicting objectives in wireless networks: saving resources and improving\n",
            "user satisfaction levels.\n",
            "\n",
            "**Paper Id :1901.03775 \n",
            "Title :Creative AI Through Evolutionary Computation\n",
            "  The main power of artificial intelligence is not in modeling what we already\n",
            "know, but in creating solutions that are new. Such solutions exist in extremely\n",
            "large, high-dimensional, and complex search spaces. Population-based search\n",
            "techniques, i.e. variants of evolutionary computation, are well suited to\n",
            "finding them. These techniques are also well positioned to take advantage of\n",
            "large-scale parallel computing resources, making creative AI through\n",
            "evolutionary computation the likely \"next deep learning\".\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id :2308.01899 \n",
            "Title :How many preprints have actually been printed and why: a case study of\n",
            "  computer science preprints on arXiv\n",
            "  Preprints play an increasingly critical role in academic communities. There\n",
            "are many reasons driving researchers to post their manuscripts to preprint\n",
            "servers before formal submission to journals or conferences, but the use of\n",
            "preprints has also sparked considerable controversy, especially surrounding the\n",
            "claim of priority. In this paper, a case study of computer science preprints\n",
            "submitted to arXiv from 2008 to 2017 is conducted to quantify how many\n",
            "preprints have eventually been printed in peer-reviewed venues. Among those\n",
            "published manuscripts, some are published under different titles and without an\n",
            "update to their preprints on arXiv. In the case of these manuscripts, the\n",
            "traditional fuzzy matching method is incapable of mapping the preprint to the\n",
            "final published version. In view of this issue, we introduce a semantics-based\n",
            "mapping method with the employment of Bidirectional Encoder Representations\n",
            "from Transformers (BERT). With this new mapping method and a plurality of data\n",
            "sources, we find that 66% of all sampled preprints are published under\n",
            "unchanged titles and 11% are published under different titles and with other\n",
            "modifications. A further analysis was then performed to investigate why these\n",
            "preprints but not others were accepted for publication. Our comparison reveals\n",
            "that in the field of computer science, published preprints feature adequate\n",
            "revisions, multiple authorship, detailed abstract and introduction, extensive\n",
            "and authoritative references and available source code.\n",
            "\n",
            "**Paper Id :2010.00182 \n",
            "Title :Dual Attention Model for Citation Recommendation\n",
            "  Based on an exponentially increasing number of academic articles, discovering\n",
            "and citing comprehensive and appropriate resources has become a non-trivial\n",
            "task. Conventional citation recommender methods suffer from severe information\n",
            "loss. For example, they do not consider the section of the paper that the user\n",
            "is writing and for which they need to find a citation, the relatedness between\n",
            "the words in the local context (the text span that describes a citation), or\n",
            "the importance on each word from the local context. These shortcomings make\n",
            "such methods insufficient for recommending adequate citations to academic\n",
            "manuscripts. In this study, we propose a novel embedding-based neural network\n",
            "called \"dual attention model for citation recommendation (DACR)\" to recommend\n",
            "citations during manuscript preparation. Our method adapts embedding of three\n",
            "dimensions of semantic information: words in the local context, structural\n",
            "contexts, and the section on which a user is working. A neural network is\n",
            "designed to maximize the similarity between the embedding of the three input\n",
            "(local context words, section and structural contexts) and the target citation\n",
            "appearing in the context. The core of the neural network is composed of\n",
            "self-attention and additive attention, where the former aims to capture the\n",
            "relatedness between the contextual words and structural context, and the latter\n",
            "aims to learn the importance of them. The experiments on real-world datasets\n",
            "demonstrate the effectiveness of the proposed approach.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "threshold = 10  # Number of top pairs to print\n",
        "\n",
        "for _ in range(threshold):\n",
        "    max_elements, max_indices = torch.max(cosine_scores, dim=0)\n",
        "    max_index = max_indices[0].item()  # Get the maximum index from the tensor\n",
        "\n",
        "    print(\"\\n*********\\n\")\n",
        "    print(\"**Paper Id: \" + ai_df.iloc[count]['id'] + '\\nTitle: ' + ai_df.iloc[count]['title']\n",
        "          + '\\n' + ai_df.iloc[count]['abstract'] +\n",
        "          '\\n**Paper Id: ' + ai_df.iloc[max_index]['id'] + '\\nTitle: ' +\n",
        "          ai_df.iloc[max_index]['title'] + '\\n' +\n",
        "          ai_df.iloc[max_index]['abstract'])\n",
        "\n",
        "    cosine_scores[max_index] = 0.0  # Reset the current score to 0 to find the next maximum\n",
        "    count += 1\n",
        "\n",
        "    if count >= len(cosine_scores):\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWC7pI95pipE",
        "outputId": "377792ca-712a-4297-b527-255e6fe67ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1606.07282\n",
            "Title: A review of Gaussian Markov models for conditional independence\n",
            "  Markov models lie at the interface between statistical independence in a\n",
            "probability distribution and graph separation properties. We review model\n",
            "selection and estimation in directed and undirected Markov models with Gaussian\n",
            "parametrization, emphasizing the main similarities and differences. These two\n",
            "model classes are similar but not equivalent, although they share a common\n",
            "intersection. We present the existing results from a historical perspective,\n",
            "taking into account the amount of literature existing from both the artificial\n",
            "intelligence and statistics research communities, where these models were\n",
            "originated. We cover classical topics such as maximum likelihood estimation and\n",
            "model selection via hypothesis testing, but also more modern approaches like\n",
            "regularization and Bayesian methods. We also discuss how the Markov models\n",
            "reviewed fit in the rich hierarchy of other, higher level Markov model classes.\n",
            "Finally, we close the paper overviewing relaxations of the Gaussian assumption\n",
            "and pointing out the main areas of application where these Markov models are\n",
            "nowadays used.\n",
            "\n",
            "**Paper Id: 2004.12734\n",
            "Title: An Epistemic Approach to the Formal Specification of Statistical Machine\n",
            "  Learning\n",
            "  We propose an epistemic approach to formalizing statistical properties of\n",
            "machine learning. Specifically, we introduce a formal model for supervised\n",
            "learning based on a Kripke model where each possible world corresponds to a\n",
            "possible dataset and modal operators are interpreted as transformation and\n",
            "testing on datasets. Then we formalize various notions of the classification\n",
            "performance, robustness, and fairness of statistical classifiers by using our\n",
            "extension of statistical epistemic logic (StatEL). In this formalization, we\n",
            "show relationships among properties of classifiers, and relevance between\n",
            "classification performance and robustness. As far as we know, this is the first\n",
            "work that uses epistemic models and logical formulas to express statistical\n",
            "properties of machine learning, and would be a starting point to develop\n",
            "theories of formal specification of machine learning.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1611.10351\n",
            "Title: Joint Causal Inference from Multiple Contexts\n",
            "  The gold standard for discovering causal relations is by means of\n",
            "experimentation. Over the last decades, alternative methods have been proposed\n",
            "that can infer causal relations between variables from certain statistical\n",
            "patterns in purely observational data. We introduce Joint Causal Inference\n",
            "(JCI), a novel approach to causal discovery from multiple data sets from\n",
            "different contexts that elegantly unifies both approaches. JCI is a causal\n",
            "modeling framework rather than a specific algorithm, and it can be implemented\n",
            "using any causal discovery algorithm that can take into account certain\n",
            "background knowledge. JCI can deal with different types of interventions (e.g.,\n",
            "perfect, imperfect, stochastic, etc.) in a unified fashion, and does not\n",
            "require knowledge of intervention targets or types in case of interventional\n",
            "data. We explain how several well-known causal discovery algorithms can be seen\n",
            "as addressing special cases of the JCI framework, and we also propose novel\n",
            "implementations that extend existing causal discovery methods for purely\n",
            "observational data to the JCI setting. We evaluate different JCI\n",
            "implementations on synthetic data and on flow cytometry protein expression data\n",
            "and conclude that JCI implementations can considerably outperform\n",
            "state-of-the-art causal discovery algorithms.\n",
            "\n",
            "**Paper Id: 2011.04044\n",
            "Title: Exploring End-to-End Differentiable Natural Logic Modeling\n",
            "  We explore end-to-end trained differentiable models that integrate natural\n",
            "logic with neural networks, aiming to keep the backbone of natural language\n",
            "reasoning based on the natural logic formalism while introducing subsymbolic\n",
            "vector representations and neural components. The proposed model adapts module\n",
            "networks to model natural logic operations, which is enhanced with a memory\n",
            "component to model contextual information. Experiments show that the proposed\n",
            "framework can effectively model monotonicity-based reasoning, compared to the\n",
            "baseline neural network models without built-in inductive bias for\n",
            "monotonicity-based reasoning. Our proposed model shows to be robust when\n",
            "transferred from upward to downward inference. We perform further analyses on\n",
            "the performance of the proposed model on aggregation, showing the effectiveness\n",
            "of the proposed subcomponents on helping achieve better intermediate\n",
            "aggregation performance.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1703.10254\n",
            "Title: Bandit-Based Model Selection for Deformable Object Manipulation\n",
            "  We present a novel approach to deformable object manipulation that does not\n",
            "rely on highly-accurate modeling. The key contribution of this paper is to\n",
            "formulate the task as a Multi-Armed Bandit problem, with each arm representing\n",
            "a model of the deformable object. To \"pull\" an arm and evaluate its utility, we\n",
            "use the arm's model to generate a velocity command for the gripper(s) holding\n",
            "the object and execute it. As the task proceeds and the object deforms, the\n",
            "utility of each model can change. Our framework estimates these changes and\n",
            "balances exploration of the model set with exploitation of high-utility models.\n",
            "We also propose an approach based on Kalman Filtering for Non-stationary\n",
            "Multi-armed Normal Bandits (KF-MANB) to leverage the coupling between models to\n",
            "learn more from each arm pull. We demonstrate that our method outperforms\n",
            "previous methods on synthetic trials, and performs competitively on several\n",
            "manipulation tasks in simulation.\n",
            "\n",
            "**Paper Id: 2004.14174\n",
            "Title: Reevaluating Adversarial Examples in Natural Language\n",
            "  State-of-the-art attacks on NLP models lack a shared definition of a what\n",
            "constitutes a successful attack. We distill ideas from past work into a unified\n",
            "framework: a successful natural language adversarial example is a perturbation\n",
            "that fools the model and follows some linguistic constraints. We then analyze\n",
            "the outputs of two state-of-the-art synonym substitution attacks. We find that\n",
            "their perturbations often do not preserve semantics, and 38% introduce\n",
            "grammatical errors. Human surveys reveal that to successfully preserve\n",
            "semantics, we need to significantly increase the minimum cosine similarities\n",
            "between the embeddings of swapped words and between the sentence encodings of\n",
            "original and perturbed sentences.With constraints adjusted to better preserve\n",
            "semantics and grammaticality, the attack success rate drops by over 70\n",
            "percentage points.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1709.05360\n",
            "Title: Embedding Deep Networks into Visual Explanations\n",
            "  In this paper, we propose a novel Explanation Neural Network (XNN) to explain\n",
            "the predictions made by a deep network. The XNN works by learning a nonlinear\n",
            "embedding of a high-dimensional activation vector of a deep network layer into\n",
            "a low-dimensional explanation space while retaining faithfulness i.e., the\n",
            "original deep learning predictions can be constructed from the few concepts\n",
            "extracted by our explanation network. We then visualize such concepts for human\n",
            "to learn about the high-level concepts that the deep network is using to make\n",
            "decisions. We propose an algorithm called Sparse Reconstruction Autoencoder\n",
            "(SRAE) for learning the embedding to the explanation space. SRAE aims to\n",
            "reconstruct part of the original feature space while retaining faithfulness. A\n",
            "pull-away term is applied to SRAE to make the bases of the explanation space\n",
            "more orthogonal to each other. A visualization system is then introduced for\n",
            "human understanding of the features in the explanation space. The proposed\n",
            "method is applied to explain CNN models in image classification tasks. We\n",
            "conducted a human study, which shows that the proposed approach outperforms\n",
            "single saliency map baselines, and improves human performance on a difficult\n",
            "classification tasks. Also, several novel metrics are introduced to evaluate\n",
            "the performance of explanations quantitatively without human involvement.\n",
            "\n",
            "**Paper Id: 2101.07523\n",
            "Title: Mapping and Describing Geospatial Data to Generalize Complex Mapping and\n",
            "  Describing Geospatial Data to Generalize Complex Models: The Case of\n",
            "  LittoSIM-GEN Models\n",
            "  For some scientific questions, empirical data are essential to develop\n",
            "reliable simulation models. These data usually come from different sources with\n",
            "diverse and heterogeneous formats. The design of complex data-driven models is\n",
            "often shaped by the structure of the data available in research projects.\n",
            "Hence, applying such models to other case studies requires either to get\n",
            "similar data or to transform new data to fit the model inputs. It is the case\n",
            "of agent-based models (ABMs) that use advanced data structures such as\n",
            "Geographic Information Systems data. We faced this problem in the LittoSIM-GEN\n",
            "project when generalizing our participatory flooding model (LittoSIM) to new\n",
            "territories. From this experience, we provide a mapping approach to structure,\n",
            "describe, and automatize the integration of geospatial data into ABMs.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1801.03326\n",
            "Title: Expected Policy Gradients for Reinforcement Learning\n",
            "  We propose expected policy gradients (EPG), which unify stochastic policy\n",
            "gradients (SPG) and deterministic policy gradients (DPG) for reinforcement\n",
            "learning. Inspired by expected sarsa, EPG integrates (or sums) across actions\n",
            "when estimating the gradient, instead of relying only on the action in the\n",
            "sampled trajectory. For continuous action spaces, we first derive a practical\n",
            "result for Gaussian policies and quadratic critics and then extend it to a\n",
            "universal analytical method, covering a broad class of actors and critics,\n",
            "including Gaussian, exponential families, and policies with bounded support.\n",
            "For Gaussian policies, we introduce an exploration method that uses covariance\n",
            "proportional to the matrix exponential of the scaled Hessian of the critic with\n",
            "respect to the actions. For discrete action spaces, we derive a variant of EPG\n",
            "based on softmax policies. We also establish a new general policy gradient\n",
            "theorem, of which the stochastic and deterministic policy gradient theorems are\n",
            "special cases. Furthermore, we prove that EPG reduces the variance of the\n",
            "gradient estimates without requiring deterministic policies and with little\n",
            "computational overhead. Finally, we provide an extensive experimental\n",
            "evaluation of EPG and show that it outperforms existing approaches on multiple\n",
            "challenging control domains.\n",
            "\n",
            "**Paper Id: 2010.00711\n",
            "Title: A Survey of the State of Explainable AI for Natural Language Processing\n",
            "  Recent years have seen important advances in the quality of state-of-the-art\n",
            "models, but this has come at the expense of models becoming less interpretable.\n",
            "This survey presents an overview of the current state of Explainable AI (XAI),\n",
            "considered within the domain of Natural Language Processing (NLP). We discuss\n",
            "the main categorization of explanations, as well as the various ways\n",
            "explanations can be arrived at and visualized. We detail the operations and\n",
            "explainability techniques currently available for generating explanations for\n",
            "NLP model predictions, to serve as a resource for model developers in the\n",
            "community. Finally, we point out the current gaps and encourage directions for\n",
            "future work in this important research area.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1801.09061\n",
            "Title: SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to\n",
            "  object-oriented SPIN rules\n",
            "  Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language)\n",
            "ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family.\n",
            "Being supported by ontology editors, rule engines and ontology reasoners, it\n",
            "has become a very popular choice for developing rule-based applications on top\n",
            "of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium\n",
            "standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL\n",
            "Inferencing Notation) has become a de-facto industry standard to rep-resent\n",
            "SPARQL rules and constraints on Semantic Web models, building on the widespread\n",
            "acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper,\n",
            "we ar-gue that the life of existing SWRL rule-based ontology applications can\n",
            "be prolonged by con-verting them to SPIN. To this end, we have developed the\n",
            "SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules,\n",
            "considering the object-orientation of SPIN, i.e. linking rules to the\n",
            "appropriate ontology classes and optimizing them, as derived by analysing the\n",
            "rule conditions.\n",
            "\n",
            "**Paper Id: 1803.01316\n",
            "Title: On Cognitive Preferences and the Plausibility of Rule-based Models\n",
            "  It is conventional wisdom in machine learning and data mining that logical\n",
            "models such as rule sets are more interpretable than other models, and that\n",
            "among such rule-based models, simpler models are more interpretable than more\n",
            "complex ones. In this position paper, we question this latter assumption by\n",
            "focusing on one particular aspect of interpretability, namely the plausibility\n",
            "of models. Roughly speaking, we equate the plausibility of a model with the\n",
            "likeliness that a user accepts it as an explanation for a prediction. In\n",
            "particular, we argue that, all other things being equal, longer explanations\n",
            "may be more convincing than shorter ones, and that the predominant bias for\n",
            "shorter models, which is typically necessary for learning powerful\n",
            "discriminative models, may not be suitable when it comes to user acceptance of\n",
            "the learned models. To that end, we first recapitulate evidence for and against\n",
            "this postulate, and then report the results of an evaluation in a\n",
            "crowd-sourcing study based on about 3.000 judgments. The results do not reveal\n",
            "a strong preference for simple rules, whereas we can observe a weak preference\n",
            "for longer rules in some domains. We then relate these results to well-known\n",
            "cognitive biases such as the conjunction fallacy, the representative heuristic,\n",
            "or the recogition heuristic, and investigate their relation to rule length and\n",
            "plausibility.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1802.03774\n",
            "Title: On Kernel Method-Based Connectionist Models and Supervised Deep Learning\n",
            "  Without Backpropagation\n",
            "  We propose a novel family of connectionist models based on kernel machines\n",
            "and consider the problem of learning layer-by-layer a compositional hypothesis\n",
            "class, i.e., a feedforward, multilayer architecture, in a supervised setting.\n",
            "In terms of the models, we present a principled method to \"kernelize\" (partly\n",
            "or completely) any neural network (NN). With this method, we obtain a\n",
            "counterpart of any given NN that is powered by kernel machines instead of\n",
            "neurons. In terms of learning, when learning a feedforward deep architecture in\n",
            "a supervised setting, one needs to train all the components simultaneously\n",
            "using backpropagation (BP) since there are no explicit targets for the hidden\n",
            "layers (Rumelhart86). We consider without loss of generality the two-layer case\n",
            "and present a general framework that explicitly characterizes a target for the\n",
            "hidden layer that is optimal for minimizing the objective function of the\n",
            "network. This characterization then makes possible a purely greedy training\n",
            "scheme that learns one layer at a time, starting from the input layer. We\n",
            "provide realizations of the abstract framework under certain architectures and\n",
            "objective functions. Based on these realizations, we present a layer-wise\n",
            "training algorithm for an l-layer feedforward network for classification, where\n",
            "l>=2 can be arbitrary. This algorithm can be given an intuitive geometric\n",
            "interpretation that makes the learning dynamics transparent. Empirical results\n",
            "are provided to complement our theory. We show that the kernelized networks,\n",
            "trained layer-wise, compare favorably with classical kernel machines as well as\n",
            "other connectionist models trained by BP. We also visualize the inner workings\n",
            "of the greedy kernelized models to validate our claim on the transparency of\n",
            "the layer-wise algorithm.\n",
            "\n",
            "**Paper Id: 2002.08247\n",
            "Title: Learning Global Transparent Models Consistent with Local Contrastive\n",
            "  Explanations\n",
            "  There is a rich and growing literature on producing local\n",
            "contrastive/counterfactual explanations for black-box models (e.g. neural\n",
            "networks).\n",
            "  In these methods, for an input, an explanation is in the form of a contrast\n",
            "point differing in very few features from the original input and lying in a\n",
            "different class. Other works try to build globally interpretable models like\n",
            "decision trees and rule lists based on the data using actual labels or based on\n",
            "the black-box models predictions. Although these interpretable global models\n",
            "can be useful, they may not be consistent with local explanations from a\n",
            "specific black-box of choice. In this work, we explore the question: Can we\n",
            "produce a transparent global model that is simultaneously accurate and\n",
            "consistent with the local (contrastive) explanations of the black-box model? We\n",
            "introduce a natural local consistency metric that quantifies if the local\n",
            "explanations and predictions of the black-box model are also consistent with\n",
            "the proxy global transparent model. Based on a key insight we propose a novel\n",
            "method where we create custom boolean features from sparse local contrastive\n",
            "explanations of the black-box model and then train a globally transparent model\n",
            "on just these, and showcase empirically that such models have higher local\n",
            "consistency compared with other known strategies, while still being close in\n",
            "performance to models that are trained with access to the original data.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1802.05101\n",
            "Title: Democratizing AI: Non-expert design of prediction tasks\n",
            "  Non-experts have long made important contributions to machine learning (ML)\n",
            "by contributing training data, and recent work has shown that non-experts can\n",
            "also help with feature engineering by suggesting novel predictive features.\n",
            "However, non-experts have only contributed features to prediction tasks already\n",
            "posed by experienced ML practitioners. Here we study how non-experts can design\n",
            "prediction tasks themselves, what types of tasks non-experts will design, and\n",
            "whether predictive models can be automatically trained on data sourced for\n",
            "their tasks. We use a crowdsourcing platform where non-experts design\n",
            "predictive tasks that are then categorized and ranked by the crowd.\n",
            "Crowdsourced data are collected for top-ranked tasks and predictive models are\n",
            "then trained and evaluated automatically using those data. We show that\n",
            "individuals without ML experience can collectively construct useful datasets\n",
            "and that predictive models can be learned on these datasets, but challenges\n",
            "remain. The prediction tasks designed by non-experts covered a broad range of\n",
            "domains, from politics and current events to health behavior, demographics, and\n",
            "more. Proper instructions are crucial for non-experts, so we also conducted a\n",
            "randomized trial to understand how different instructions may influence the\n",
            "types of prediction tasks being proposed. In general, understanding better how\n",
            "non-experts can contribute to ML can further leverage advances in Automatic ML\n",
            "and has important implications as ML continues to drive workplace automation.\n",
            "\n",
            "**Paper Id: 1905.04232\n",
            "Title: Automatic Programming of Cellular Automata and Artificial Neural\n",
            "  Networks Guided by Philosophy\n",
            "  Many computer models such as cellular automata and artificial neural networks\n",
            "have been developed and successfully applied. However, in some cases, these\n",
            "models might be restrictive on the possible solutions or their solutions might\n",
            "be difficult to interpret. To overcome this problem, we outline a new approach,\n",
            "the so-called allagmatic method, that automatically programs and executes\n",
            "models with as little limitations as possible while maintaining human\n",
            "interpretability. Earlier we described a metamodel and its building blocks\n",
            "according to the philosophical concepts of structure (spatial dimension) and\n",
            "operation (temporal dimension). They are entity, milieu, and update function\n",
            "that together abstractly describe cellular automata, artificial neural\n",
            "networks, and possibly any kind of computer model. By automatically combining\n",
            "these building blocks in an evolutionary computation, interpretability might be\n",
            "increased by the relationship to the metamodel, and models might be translated\n",
            "into more interpretable models via the metamodel. We propose generic and\n",
            "object-oriented programming to implement the entities and their milieus as\n",
            "dynamic and generic arrays and the update function as a method. We show two\n",
            "experiments where a simple cellular automaton and an artificial neural network\n",
            "are automatically programmed, compiled, and executed. A target state is\n",
            "successfully evolved and learned in the cellular automaton and artificial\n",
            "neural network, respectively. We conclude that the allagmatic method can create\n",
            "and execute cellular automaton and artificial neural network models in an\n",
            "automated manner with the guidance of philosophy.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1803.01316\n",
            "Title: On Cognitive Preferences and the Plausibility of Rule-based Models\n",
            "  It is conventional wisdom in machine learning and data mining that logical\n",
            "models such as rule sets are more interpretable than other models, and that\n",
            "among such rule-based models, simpler models are more interpretable than more\n",
            "complex ones. In this position paper, we question this latter assumption by\n",
            "focusing on one particular aspect of interpretability, namely the plausibility\n",
            "of models. Roughly speaking, we equate the plausibility of a model with the\n",
            "likeliness that a user accepts it as an explanation for a prediction. In\n",
            "particular, we argue that, all other things being equal, longer explanations\n",
            "may be more convincing than shorter ones, and that the predominant bias for\n",
            "shorter models, which is typically necessary for learning powerful\n",
            "discriminative models, may not be suitable when it comes to user acceptance of\n",
            "the learned models. To that end, we first recapitulate evidence for and against\n",
            "this postulate, and then report the results of an evaluation in a\n",
            "crowd-sourcing study based on about 3.000 judgments. The results do not reveal\n",
            "a strong preference for simple rules, whereas we can observe a weak preference\n",
            "for longer rules in some domains. We then relate these results to well-known\n",
            "cognitive biases such as the conjunction fallacy, the representative heuristic,\n",
            "or the recogition heuristic, and investigate their relation to rule length and\n",
            "plausibility.\n",
            "\n",
            "**Paper Id: 1910.09358\n",
            "Title: A Decision-Theoretic Approach for Model Interpretability in Bayesian\n",
            "  Framework\n",
            "  A salient approach to interpretable machine learning is to restrict modeling\n",
            "to simple models. In the Bayesian framework, this can be pursued by restricting\n",
            "the model structure and prior to favor interpretable models. Fundamentally,\n",
            "however, interpretability is about users' preferences, not the data generation\n",
            "mechanism; it is more natural to formulate interpretability as a utility\n",
            "function. In this work, we propose an interpretability utility, which\n",
            "explicates the trade-off between explanation fidelity and interpretability in\n",
            "the Bayesian framework. The method consists of two steps. First, a reference\n",
            "model, possibly a black-box Bayesian predictive model which does not compromise\n",
            "accuracy, is fitted to the training data. Second, a proxy model from an\n",
            "interpretable model family that best mimics the predictive behaviour of the\n",
            "reference model is found by optimizing the interpretability utility function.\n",
            "The approach is model agnostic -- neither the interpretable model nor the\n",
            "reference model are restricted to a certain class of models -- and the\n",
            "optimization problem can be solved using standard tools. Through experiments on\n",
            "real-word data sets, using decision trees as interpretable models and Bayesian\n",
            "additive regression models as reference models, we show that for the same level\n",
            "of interpretability, our approach generates more accurate models than the\n",
            "alternative of restricting the prior. We also propose a systematic way to\n",
            "measure stability of interpretabile models constructed by different\n",
            "interpretability approaches and show that our proposed approach generates more\n",
            "stable models.\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "**Paper Id: 1803.02965\n",
            "Title: A Multi-Objective Deep Reinforcement Learning Framework\n",
            "  This paper introduces a new scalable multi-objective deep reinforcement\n",
            "learning (MODRL) framework based on deep Q-networks. We develop a\n",
            "high-performance MODRL framework that supports both single-policy and\n",
            "multi-policy strategies, as well as both linear and non-linear approaches to\n",
            "action selection. The experimental results on two benchmark problems\n",
            "(two-objective deep sea treasure environment and three-objective Mountain Car\n",
            "problem) indicate that the proposed framework is able to find the\n",
            "Pareto-optimal solutions effectively. The proposed framework is generic and\n",
            "highly modularized, which allows the integration of different deep\n",
            "reinforcement learning algorithms in different complex problem domains. This\n",
            "therefore overcomes many disadvantages involved with standard multi-objective\n",
            "reinforcement learning methods in the current literature. The proposed\n",
            "framework acts as a testbed platform that accelerates the development of MODRL\n",
            "for solving increasingly complicated multi-objective problems.\n",
            "\n",
            "**Paper Id: 1807.07024\n",
            "Title: Optimal design of experiments to identify latent behavioral types\n",
            "  Bayesian optimal experiments that maximize the information gained from\n",
            "collected data are critical to efficiently identify behavioral models. We\n",
            "extend a seminal method for designing Bayesian optimal experiments by\n",
            "introducing two computational improvements that make the procedure tractable:\n",
            "(1) a search algorithm from artificial intelligence that efficiently explores\n",
            "the space of possible design parameters, and (2) a sampling procedure which\n",
            "evaluates each design parameter combination more efficiently. We apply our\n",
            "procedure to a game of imperfect information to evaluate and quantify the\n",
            "computational improvements. We then collect data across five different\n",
            "experimental designs to compare the ability of the optimal experimental design\n",
            "to discriminate among competing behavioral models against the experimental\n",
            "designs chosen by a \"wisdom of experts\" prediction experiment. We find that\n",
            "data from the experiment suggested by the optimal design approach requires\n",
            "significantly less data to distinguish behavioral models (i.e., test\n",
            "hypotheses) than data from the experiment suggested by experts. Substantively,\n",
            "we find that reinforcement learning best explains human decision-making in the\n",
            "imperfect information game and that behavior is not adequately described by the\n",
            "Bayesian Nash equilibrium. Our procedure is general and computationally\n",
            "efficient and can be applied to dynamically optimize online experiments.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}