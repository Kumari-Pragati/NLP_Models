{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatting with TeachableAgent\n"
      ],
      "metadata": {
        "id": "oWYNzlm9m6LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lHWSSSDJFtYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec2f49d-3e4e-4982-b3d7-52620adbc892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyautogen[teachable] in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (5.6.3)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (2.1.1)\n",
            "Requirement already satisfied: openai~=1.2 in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (1.3.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (1.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (2.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (0.5.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (from pyautogen[teachable]) (0.4.18)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen[teachable]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.2->pyautogen[teachable]) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen[teachable]) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen[teachable]) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen[teachable]) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai~=1.2->pyautogen[teachable]) (4.8.0)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (2.31.0)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.104.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.24.0.post1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (3.0.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.16.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.42b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.21.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.15.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.59.2)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (4.1.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (28.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb->pyautogen[teachable]) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen[teachable]) (2023.6.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen[teachable]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen[teachable]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.2->pyautogen[teachable]) (1.1.3)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb->pyautogen[teachable]) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen[teachable]) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.2->pyautogen[teachable]) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.2->pyautogen[teachable]) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (3.2.2)\n",
            "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (1.26.18)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->pyautogen[teachable]) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->pyautogen[teachable]) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable]) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable]) (1.61.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable]) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.21.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->pyautogen[teachable]) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (0.42b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (0.42b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (0.42b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.42b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (0.42b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->pyautogen[teachable]) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->pyautogen[teachable]) (1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb->pyautogen[teachable]) (3.3.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->pyautogen[teachable]) (8.1.7)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable]) (0.6.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable]) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable]) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->pyautogen[teachable]) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->pyautogen[teachable]) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->pyautogen[teachable]) (3.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->pyautogen[teachable]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->pyautogen[teachable]) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"pyautogen[teachable]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set your API Endpoint\n"
      ],
      "metadata": {
        "id": "StfioxixnEZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    env_or_file=\"/content/llm.json\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4\"],\n",
        "    },\n",
        ")\n",
        "print(config_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr6jnzqJl2tV",
        "outputId": "27121041-0eb1-48c4-c81b-965fd816a072"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'model': 'gpt-4', 'api_key': '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'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Agents\n"
      ],
      "metadata": {
        "id": "7SWewR9jnKUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen.agentchat.contrib.teachable_agent import TeachableAgent\n",
        "from autogen import UserProxyAgent\n",
        "\n",
        "llm_config = {\n",
        "    \"config_list\": config_list,\n",
        "    \"timeout\": 60,\n",
        "    \"cache_seed\": None,  # Use an int to seed the response cache. Use None to disable caching.\n",
        "}\n",
        "\n",
        "teach_config={\n",
        "    \"verbosity\": 0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
        "    \"reset_db\": True,  # Set to True to start over with an empty database.\n",
        "    \"path_to_db_dir\": \"./tmp/notebook/teachable_agent_db\",  # Path to the directory where the database will be stored.\n",
        "    \"recall_threshold\": 1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
        "}\n",
        "\n",
        "try:\n",
        "    from termcolor import colored\n",
        "except ImportError:\n",
        "\n",
        "    def colored(x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "teachable_agent = TeachableAgent(\n",
        "    name=\"teachableagent\",\n",
        "    llm_config=llm_config,\n",
        "    teach_config=teach_config)\n",
        "\n",
        "user = UserProxyAgent(\n",
        "    name=\"user\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
        "    max_consecutive_auto_reply=0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdqVXhFtmqs5",
        "outputId": "25688f24-0262-4827-b5bc-27a072a7b01d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CLEARING MEMORY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning new facts"
      ],
      "metadata": {
        "id": "Q-lUt21BnWi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the Vicuna model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlo1micFrs2Y",
        "outputId": "fd907029-e860-4c42-fb97-5c823a637d61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "What is the Vicuna model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "I'm sorry, but I currently don't have information on a model specifically referred to as the \"Vicuna model\". It's possible that the term has been misspelled, misconstrued, or it's a specific concept tied to a certain context. The term \"vicuna\" typically refers to a kind of camelid native to South America. If you provide more context, I'll do my best to provide a more accurate and detailed answer.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Vicuna is a 13B-parameter language model released by Meta.\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4C8Oa7yr1mF",
        "outputId": "4e29a2f6-8b98-4dde-f895-30a147774a19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Vicuna is a 13B-parameter language model released by Meta.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "My apologies for the confusion. As of the time of our last update, I didn't have information regarding Vicuna, which is indeed a 13-billion parameter language model developed by Meta. It certainly seems to be an impressive progression in the field of AI language models. However, I don’t have any detailed specifications or performances. I would recommend checking the most up-to-date references or Meta's official communications for the most accurate information. I'll make sure to remember this information for future reference.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the Orca model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1hkNXIzsAm0",
        "outputId": "2658db85-8d41-4ac5-aa3a-4864e1973357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "What is the Orca model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "I'm sorry for any confusion, but as of our last update, I didn't have information regarding an \"Orca\" model. If you're referring to a specific model in a certain field like AI, physics, or biology, could you please provide more details? To ensure I provide accurate and precise information, it would be very helpful if I could understand the context of your question better. Orca is a common term and could refer to the marine mammal (also known as a Killer Whale), a software package for quantum chemistry, and other things in different contexts.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsehuF7ksZsn",
        "outputId": "2063edd8-b7bc-48e6-9e35-e092d79b0795"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "I appreciate your update on the Orca language model developed by Microsoft. As of the last knowledge update, detailed info regarding Orca wasn't available, but I'll remember this going forward. This 13-billion parameter language model reportedly outperforms Vicuna on multiple tasks, which indicates advancements in AI language modeling technology. It's always exciting to hear about new developments in this field. For complete information and specifics on Orca's functionalities and performance, I recommend checking Microsoft's official resources or other recent publications.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teachable_agent.learn_from_user_feedback()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeW2BkUOsn-y",
        "outputId": "9c192c19-e437-4f0a-9f67-ca5f38a3daea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 37.9MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's start a new chat by clearing the previous chat's history. At this point, common LLM-based assistants would forget everything from the last chat. But `TeachableAgent` can retrieve memories from its vector DB as needed, allowing it to recall and reason over facts that the user taught it in earlier conversations."
      ],
      "metadata": {
        "id": "xI6lSj5etEhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does the Vicuna model compare to the Orca model?\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnang9CtE-2",
        "outputId": "e5cd2f5a-7d31-44f6-af08-818d8c1f9978"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "How does the Vicuna model compare to the Orca model?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Both Vicuna and Orca are advanced language models with 13 billion parameters, released by Meta and Microsoft respectively.\n",
            "\n",
            "Orca outperforms Vicuna in most tasks. While details may vary based on specific applications and use-cases, generally speaking, Orca demonstrates superior performance across a wide variety of natural language processing tasks. This includes but is not limited to language understanding, generating human-like text, and answering complex queries.\n",
            "\n",
            "However, without specific information about the exact features, use cases or parameters for comparison, it's hard to give a detailed comparison. It will also be beneficial to stay updated with the newest releases and improvements from both companies, as AI model performances are rapidly evolving.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning user preferences\n"
      ],
      "metadata": {
        "id": "M_o35155tw0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Please summarize this abstract.\n",
        "\n",
        "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
        "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
        "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu8OC5rxtx3E",
        "outputId": "fbc362d0-a614-4e55-88c8-9e605d7d2d4e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract.\n",
            "\n",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
            "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "AutoGen is an open-source platform that aids in the development of Language Learning Model (LLM) applications by allowing various agents to interact and accomplish tasks. These agents are adaptable and can function in different modes that incorporate LLMs, human inputs, and tools. Developers can specify agent interaction behaviors and use both natural language and code to create diverse conversation patterns. AutoGen is versatile, allowing for the creation of a wide range of applications of varying complexities and LLM capabilities. Its effectiveness in areas like mathematics, coding, question answering, operations research, online decision-making, entertainment, etc. has been shown in empirical studies.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But that's unstructured. So let's teach the agent our preference."
      ],
      "metadata": {
        "id": "FXlURkjqt79z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Please summarize this abstract.\n",
        "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
        "\n",
        "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
        "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
        "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjOsuIgut8y8",
        "outputId": "c23d59bf-a667-42ca-bef0-964285994091"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract. \n",
            "When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
            "\n",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
            "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "- Title: \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\"\n",
            "- Innovation: AutoGen is a unique open-source framework designed for developers to create applications using multiple agents, which can interact with each other to complete tasks. It also allows flexible programming of conversation patterns using both natural language and computer code, and can be used to build applications with diverse complexities and capacities regarding LLM (Language Model).\n",
            "- Key Empirical Results: Empirical studies found AutoGen to be effective in many applications across a wide range of domains, such as mathematics, coding, question-answering, operations research, online decision-making, and entertainment.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's much better, but will the teachable agent remember these preferences in the future, for a different paper? Let's start a new chat to find out!"
      ],
      "metadata": {
        "id": "q4FmsbLEuKmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teachable_agent.learn_from_user_feedback()\n",
        "\n",
        "text = \"\"\"Please summarize this abstract.\n",
        "\n",
        "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
        "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
        "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8VJN0gGuLhG",
        "outputId": "a2670d79-233b-4f27-d914-8daf1f8cdac2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n",
            "user (to teachableagent):\n",
            "\n",
            "Please summarize this abstract.\n",
            "\n",
            "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
            "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
            "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "- Title: \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\"\n",
            "- Innovation: The researchers outline the capabilities of an early version of GPT-4, a large language model developed by OpenAI, which displays skills not only in language but also in areas like mathematics, coding, vision, medicine, law, and psychology. GPT-4's performance in these areas is near human-level and often exceeds other AI models such as ChatGPT.\n",
            "- Key Findings: GPT-4 potentially represents an early version of an artificial general intelligence system, capable of surpassing prior models in various tasks. However, the researchers emphasize the need for further investigation into its limitations and pursuit of more comprehensive AGI versions, possibly requiring a paradigm shift away from next-word prediction. The team also reflects on the societal impacts and future research directions following these AI advancements.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning new skills\n"
      ],
      "metadata": {
        "id": "UP8Xrx_nuxh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqJQykYOuyaL",
        "outputId": "52d6b1f9-a582-40a2-c7f6-78200ffed62c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Your memory about AutoGen is interesting but doesn't seem to directly relate to solving this particular mathematical problem. \n",
            "\n",
            "# Plan\n",
            "1. Identify the relationship between the numbers in the left-hand side of the equation and the total on the right-hand side. \n",
            "2. Try adjusting one number at a time by either increasing or decreasing it and seeing how these changes affect the total.\n",
            "3. Repeat step 2 until the total becomes 99 when only one integer is modified.\n",
            "\n",
            "# Solution\n",
            "\n",
            "9 * 4 is 36 and 6 * 6 is 36. The sum is 72.\n",
            "If we increase one of the integers by 27, 72 becomes 99. \n",
            "This could be done by modifying 9 to 36. Doing so results in 36 * 4 which is equal to 144. \n",
            "We can then rewrite 144 as 144 - 45 and substitute this back into our original equation.\n",
            "The result is: 144 - 45 * 4 + 6 * 6. \n",
            "After performing the math, we get 99.\n",
            "We have therefore modified exactly one integer to get the desired result of 99.\n",
            " \n",
            "The solution is: 36 * 4 + 6 * 6 (with an additional operation \"+/-\" within the multiplication)\n",
            " \n",
            "However, if modifications inside of an operation are not allowed, it is not possible to adjust one integral value in the given equation without modifying others to achieve the result of 99.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, lets teach the agent a reliable strategy for solving such tasks."
      ],
      "metadata": {
        "id": "hqMfPw-tvCU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\n",
        "Here's some advice that may help:\n",
        "1. Let E denote the original number on the right.\n",
        "2. Let F denote the final number on the right.\n",
        "3. Calculate the difference between the two, G = F - E.\n",
        "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
        "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
        "6. Verify that J is an integer, and that H * J = G.\n",
        "7. Find the number on the left which is multiplied by H, and call it K.\n",
        "8. Change K to K + J.\n",
        "9. Recompute the value on the left, and verify that it equals F.\n",
        "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjKq1RlivDDN",
        "outputId": "c688465a-b7ed-474a-b1d0-5b17b8450b25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "Here's some advice that may help:\n",
            "1. Let E denote the original number on the right.\n",
            "2. Let F denote the final number on the right.\n",
            "3. Calculate the difference between the two, G = F - E.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
            "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
            "6. Verify that J is an integer, and that H * J = G.\n",
            "7. Find the number on the left which is multiplied by H, and call it K.\n",
            "8. Change K to K + J.\n",
            "9. Recompute the value on the left, and verify that it equals F.\n",
            "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "Let's follow the advice given:\n",
            "\n",
            "1. The original number on the right, E, is 72.\n",
            "2. The final number on the right, F, is 99.\n",
            "3. The difference between F and E, G, is 27.\n",
            "4. Looking at the numbers on the left, 4 divides evenly into 27.\n",
            "5. The number of times that 4 divides into 27, J, is 6.75, which is not an integer. However, 6 divides evenly into 27.\n",
            "6. The number of times that 6 divides into 27, J, is 4.5, which is not an integer. Therefore, we can't derive an integer solution using these steps. \n",
            "   \n",
            "However, if we consider negative integers as allowable adjustments, then we can look at this problem differently.\n",
            "\n",
            "# Steps for alternate solution:\n",
            "\n",
            "1. Find the absolute difference between the desired result and current result. This will yield an amount to adjust by: Abs(72-99) = 27.\n",
            "2. Now the goal is to analyze the multipliers: 4, 6 and 9 to see if we can use one of them to adjust the whole equation by 27 (Change the sign of the results and we modify negative 27 instead of plus 27)\n",
            "3. The multiplier 9 with an ajustment of -3 will give -27 (as 9*-3=-27 is fitting). We can therefore conclude that replacing 9 by 6 (as 9-3=6) will solve the issue.\n",
            "\n",
            "Solution: \n",
            "\n",
            "The solution is 6 * 4 + 6 * 6. After performing the math, we obtain 24 + 36 = 60, which, when you add the -27 results in 33. However, it's possible that I made a mistake in my calculations:\n",
            "\n",
            "1. Adjust by changing 9 to 6.\n",
            "2. The new equation would be 6 * 4 + 6 * 6.\n",
            "3. After performing the math, we obtain 24 + 36 = 60.\n",
            "4. We aimed to compensate for a difference of -27, not to result in 60. I apologize for this mistake.\n",
            "\n",
            "A correct solution eludes me at the moment, as it appears no single integer modification on the left-side of this equation can result in a total of 99 on the right-side.\n",
            "\n",
            "I apologize as the solution won't be given as \"A * B + C * D\". I appreciate your understanding.\n",
            "\n",
            "This seems a difficult problem as we can only modify one integer and all integers on the left side are positive in the beginning. If negative integers or fractions were accepted, then we could devise a solution by modification of one integer. Or if we could modify two integers then also a solution might be possible. \n",
            "\n",
            "It should be noted that this is under the assumption that multiplication and addition operations are commutative and associative and we can't change the sequence of operations to arrive at 99.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When given this advice, GPT-4 can solve such problems over 95% of the time. But can the teachable agent remember the strategy so the user doesn't have to explain it over and over? As before, let's start a new chat to find out."
      ],
      "metadata": {
        "id": "HHoG5BAgvP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teachable_agent.learn_from_user_feedback()\n",
        "\n",
        "text = \"\"\"Consider the identity:\n",
        "9 * 4 + 6 * 6 = 72\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF54gc8kvQ5L",
        "outputId": "eef79e53-4bd8-498c-feb5-bf59e80f7534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\n",
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "9 * 4 + 6 * 6 = 72\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "# Plan\n",
            "\n",
            "1. The original number on the right, E, is 72.\n",
            "2. The final number on the right, F, is 99.\n",
            "3. Calculate the difference between the two, G = F - E = 99 - 72 = 27.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G. Here, 9 divides 27, so let's pick 9 as H.\n",
            "5. Calculate J = G / H = 27 / 9 = 3. This is the number of times that H divides into G.\n",
            "6. Verify that J is an integer, and that H * J = G. In this case, it's true because 9 * 3 = 27.\n",
            "7. Find the number on the left which is multiplied by H, and call it K. Here, K is 4.\n",
            "8. Change K to K + J, which gives us 4 + 3 = 7.\n",
            "9. Recompute the value on the left, and verify that it equals F. Here, 9 * 7 + 6 * 6 = 99.\n",
            "\n",
            "# Solution\n",
            "The solution is: 9 * 7 + 6 * 6.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final check, let's test the teachable agent's newly learned skill on a separate instance of the task."
      ],
      "metadata": {
        "id": "1MDSAGmWwCyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Consider the identity:\n",
        "8 * 3 + 7 * 9 = 87\n",
        "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
        "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
        "\"\"\"\n",
        "user.initiate_chat(teachable_agent, message=text, clear_history=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJaLOoypwEHE",
        "outputId": "bf2440b0-e96c-4c98-ceee-ef637084faac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user (to teachableagent):\n",
            "\n",
            "Consider the identity:  \n",
            "8 * 3 + 7 * 9 = 87\n",
            "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
            "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "teachableagent (to user):\n",
            "\n",
            "# Plan\n",
            "\n",
            "1. The original number on the right, E, is 87.\n",
            "2. The final number on the right, F, is 59.\n",
            "3. Calculate the difference between the two, G = F - E = 59 - 87 = -28.\n",
            "4. Examine the numbers on the left one by one until finding one that divides evenly into G. Here, 7 divides -28, so let's pick 7 as H.\n",
            "5. Calculate J = G / H = -28 / 7 = -4. This is the number of times that H divides into G.\n",
            "6. Verify that J is an integer, and that H * J = G. In this case, it's true because 7 * -4 = -28.\n",
            "7. Find the number on the left which is multiplied by H, and call it K. Here, K is 9.\n",
            "8. Change K to K + J, which gives us 9 + -4 = 5.\n",
            "9. Recompute the value on the left, and verify that it equals F. Here, 8 * 3 + 7 * 5 = 59.\n",
            "\n",
            "# Solution\n",
            "The solution is: 8 * 3 + 7 * 5.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}